{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pedro - Aula 9 - Exercício",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Pedro_Aula_9_Exerc%C3%ADcio_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "\n",
        "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "\n",
        "Importante: \n",
        "- Deve-se implementar o próprio laço de treinamento.\n",
        "- Implementar o acumulo de gradiente.\n",
        "\n",
        "Dicas:\n",
        "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "\n",
        "- Solução para erro de memória:\n",
        "  - Usar bfloat16 permite quase dobrar o batch size\n",
        "\n",
        "Opcional:\n",
        "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio.\n",
        "\n",
        "- Usar pytorch lightning. Para entender como o pytorch lightning funciona, veja uma implementação simplificada no notebook `06_01_Treino_Validação_MNIST_Lightning_Lite.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "92543fcc-fea6-497b-d84d-5097e3214fdf"
      },
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdf4dce5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdea04e-4378-4860-d8b6-012b8a45d3dc"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487ee4ad-8211-48b8-acda-63f2601f948b"
      },
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False Stan Laurel and Oliver Hardy are the most famous comedy duo in history, and deservedly so, so I am h\n",
            "False Okay, I was bored and decided to see this movie. But I think the main thing that brought this movie \n",
            "True For anyone who may not know what a one-actor movie was like, this is the best example. This plot is \n",
            "3 últimas amostras treino:\n",
            "False The movie started off strong, LL Cool J (Deed) as an undercover police officer, with partner Sgt. La\n",
            "True Though the pieces are uneven this collection of 11 short films is truly a moving and human experienc\n",
            "True Since my third or fourth viewing some time ago, I've abstained from La Maman et la putain while I wa\n",
            "3 primeiras amostras validação:\n",
            "True I had never heard of Robert Roy MacGregor before \"Rob Roy\" came out, but the movie is definitely wor\n",
            "True OK, here is my personal list of top Nicktoons shows as in today:<br /><br />1. All Grown Up/SpongeBo\n",
            "True First of all, I apologize for my English. <br /><br />Everybody from ex-Yugoslavia who isn't some ex\n",
            "3 últimas amostras validação:\n",
            "True The first time I saw this \"film\" I loved it. When I was 11, I was more interested in the music and d\n",
            "True I probably saw this movie first in about 1995. Since then I've returned to it many times. It's great\n",
            "True I'm stunt, I must admit I never saw a movie with such good story and none stop high special effect m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CuqNJEZpkCk"
      },
      "source": [
        "# Experimentado a biblioteca Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOhY2ynRpnyR",
        "outputId": "83cd2e92-8046-43d3-da7e-a0137b149d5c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUNSd1snppU-"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPleoCdwrAFc"
      },
      "source": [
        "checkpoint = 'bert-base-uncased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TLNQW_iq8V2",
        "outputId": "4bb5c437-1c4b-428f-c990-a18a2c804d1f"
      },
      "source": [
        "# Utilizando o tokenizador que foi utilizado para treinar o checkpoint\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 9761, 11893, 1998, 6291, 9532, 2024, 1996, 2087, 3297, 4038, 6829, 1999, 2381, 1010, 1998, 10849, 2135, 2061, 1010, 2061, 1045, 2572, 3407, 2000, 2156, 2151, 1997, 2037, 3152, 1012, 10468, 1037, 2158, 2012, 1037, 7109, 4713, 2003, 1996, 2959, 2000, 8579, 1010, 1998, 2574, 2438, 25208, 15288, 2007, 2035, 1996, 7109, 14950, 1012, 2002, 2003, 8345, 2012, 2188, 2007, 9761, 2011, 2010, 2217, 1010, 11303, 4251, 1010, 1998, 1996, 3460, 1006, 2508, 10346, 8485, 3385, 1007, 11640, 2000, 2360, 2002, 2003, 2746, 2058, 2000, 4638, 2006, 25208, 1012, 2044, 27504, 27902, 1998, 6451, 2003, 8494, 20043, 2039, 2011, 1037, 2892, 1011, 7168, 7192, 2386, 1010, 1996, 3460, 3310, 1999, 2005, 1037, 4638, 1011, 2039, 1010, 1998, 2044, 2070, 5852, 1010, 2002, 26021, 5948, 13555, 1005, 1055, 6501, 1998, 2893, 2070, 2712, 2250, 2006, 1996, 4153, 1012, 2044, 9761, 10975, 18908, 13087, 2070, 9368, 2652, 1010, 5689, 2041, 1996, 3332, 2011, 1996, 3042, 11601, 1998, 1037, 2482, 5823, 1010, 2002, 1998, 25208, 2000, 1037, 8946, 2000, 9278, 1037, 4049, 1012, 2027, 2562, 1996, 4049, 2006, 1996, 8946, 2667, 2000, 6501, 1037, 13555, 1010, 1998, 9761, 2038, 2716, 2010, 9368, 999, 5564, 1010, 1996, 3780, 1005, 1055, 2392, 3931, 9631, 2008, 6359, 4172, 8982, 4590, 1011, 6376, 20462, 1006, 2957, 29433, 1007, 2038, 6376, 1010, 1998, 2002, 13583, 2015, 3031, 1996, 3337, 1005, 4049, 2096, 2027, 2024, 5777, 1010, 1998, 1996, 13555, 21271, 2015, 2083, 1996, 4049, 8164, 1010, 15013, 2041, 2000, 2712, 1012, 1999, 1996, 2851, 1010, 1996, 2156, 2037, 3295, 1010, 1998, 1996, 6359, 3310, 2041, 9694, 2242, 2000, 4521, 1010, 1998, 2002, 7516, 9761, 1998, 25208, 2437, 8275, 2833, 1010, 1041, 1012, 1043, 1012, 5164, 2005, 26666, 1010, 7815, 2005, 8808, 1010, 5583, 2005, 11611, 1010, 25742, 2005, 6240, 18510, 1010, 1998, 2002, 2749, 2068, 2000, 4521, 2009, 1012, 2043, 25208, 4627, 18329, 2006, 2242, 1010, 9761, 13783, 2010, 9368, 2000, 2393, 1010, 1998, 25208, 1005, 1055, 7385, 4152, 2032, 19477, 1996, 6359, 1010, 1998, 2009, 7906, 2183, 6229, 1996, 2610, 7180, 1010, 2069, 2000, 2031, 25208, 1005, 1055, 7385, 2131, 2068, 2404, 1999, 3827, 2205, 1012, 2045, 2020, 1996, 9543, 10458, 5312, 1997, 4038, 1010, 2021, 2009, 3475, 1005, 1056, 1037, 2307, 2304, 1998, 2317, 2143, 1012, 1000, 2092, 1010, 2182, 1005, 1055, 2178, 3835, 6752, 2017, 1005, 2310, 5407, 2033, 2046, 999, 1000, 2001, 2193, 3438, 2006, 2531, 2086, 1010, 2531, 16614, 1010, 1998, 9761, 11893, 1998, 6291, 9532, 2020, 2193, 1021, 2006, 1996, 25119, 1005, 9971, 1012, 3100, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yII4brx_qlGz",
        "outputId": "6ee1b078-cee9-4827-901b-cb0e467f8652"
      },
      "source": [
        "# Build do modelo com base no checkpoint\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tfrK_q0rcEz",
        "outputId": "ec6880cf-245c-4540-c0a9-d0c0b26970d1"
      },
      "source": [
        "tokens = tokenizer(x_train[0], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits',\n",
              "                           tensor([[ 0.4554, -0.5477]], grad_fn=<AddmmBackward>))])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9Et9P-ryb3"
      },
      "source": [
        "# Classes Dataset e Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnpN0XrisK8u"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DataCollatorWithPadding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RCKSvQOs9SE"
      },
      "source": [
        "class IMDBDataset():\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], int(self.y[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-MQ8KD5uPOT"
      },
      "source": [
        "# Aplicando a tokenizacao diretamente nos batches e nao em cada exemplo, para fazer uso do codigo otimizado para batches do HF\n",
        "def create_dataloader(x, y, tokenizer, batch_size, shuffle=False, max_length=250):\n",
        "  def data_collator(batch):\n",
        "    x, y = zip(*batch)\n",
        "    tokenized_x = tokenizer(x, padding='longest', truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    return tokenized_x['input_ids'], torch.LongTensor(y), tokenized_x['attention_mask']\n",
        "  dataset = IMDBDataset(x, y)\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=data_collator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPf0em6q2MVg"
      },
      "source": [
        "# Loop de treino, validação e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzYEEnX42PkM",
        "outputId": "f5070b84-9d0a-4b38-8249-7cdb5a66c4f2"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-enX8Dg62e9e"
      },
      "source": [
        "def train(model, train, valid, optimizer, batch_count, filename_save, n_epochs=5, run=None, params=None):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "  count = 0\n",
        "  train_losses, valid_losses = [], []\n",
        "  optimizer.zero_grad()\n",
        "  if run:\n",
        "    run['parameters'] = params\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train, attention_mask in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "      outputs = model(input_ids=x_train, attention_mask=attention_mask, labels=y_train)\n",
        "      batch_loss = outputs.loss\n",
        "      batch_loss.backward()\n",
        "      if count == batch_count-1:\n",
        "        count = -1\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "      count += 1\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid, attention_mask in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(input_ids=x_valid, attention_mask=attention_mask, labels=y_valid)\n",
        "            # calcula a perda\n",
        "            batch_loss = outputs.loss\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    if run:\n",
        "      run[f\"{filename_save}_valid/loss\"].log(valid_loss)\n",
        "      run[f\"{filename_save}_valid/acc\"].log(valid_acc)\n",
        "      run[f\"{filename_save}_train/loss\"].log(train_loss)\n",
        "\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        model.save_pretrained(filename_save)\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkQxUViF4yJ-"
      },
      "source": [
        "# Validando o overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeVTJAZi5Hw1"
      },
      "source": [
        "from transformers import AdamW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRBa0Ljh41Y9",
        "outputId": "d8bb14af-1036-470c-a1f8-ed5a7cbce219"
      },
      "source": [
        "checkpoint = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3NGjOM-5ejB"
      },
      "source": [
        "dataloader_train = create_dataloader(x_train[:20], y_train[:20], tokenizer, 20, shuffle=True)\n",
        "dataloader_valid = create_dataloader(x_valid[:20], y_valid[:20], tokenizer, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMc6xlnA5Uph",
        "outputId": "57bf3bff-5eef-4c39-d45e-2723bb14deea"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(model, dataloader_train, dataloader_valid, optimizer, 1, \"bert-imdb\", n_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/19 Train Loss: 0.035609 Valid Loss: 0.034723 Valid Acc: 0.500\n",
            "best model\n",
            "Época: 1/19 Train Loss: 0.032153 Valid Loss: 0.035063 Valid Acc: 0.450\n",
            "Época: 2/19 Train Loss: 0.029061 Valid Loss: 0.037210 Valid Acc: 0.600\n",
            "Época: 3/19 Train Loss: 0.026236 Valid Loss: 0.037679 Valid Acc: 0.500\n",
            "Época: 4/19 Train Loss: 0.021259 Valid Loss: 0.033887 Valid Acc: 0.550\n",
            "best model\n",
            "Época: 5/19 Train Loss: 0.017975 Valid Loss: 0.033585 Valid Acc: 0.600\n",
            "best model\n",
            "Época: 6/19 Train Loss: 0.015056 Valid Loss: 0.034098 Valid Acc: 0.550\n",
            "Época: 7/19 Train Loss: 0.011919 Valid Loss: 0.034701 Valid Acc: 0.600\n",
            "Época: 8/19 Train Loss: 0.011060 Valid Loss: 0.034657 Valid Acc: 0.550\n",
            "Época: 9/19 Train Loss: 0.008896 Valid Loss: 0.035670 Valid Acc: 0.550\n",
            "Época: 10/19 Train Loss: 0.007037 Valid Loss: 0.037786 Valid Acc: 0.600\n",
            "Época: 11/19 Train Loss: 0.005926 Valid Loss: 0.039901 Valid Acc: 0.600\n",
            "Época: 12/19 Train Loss: 0.004680 Valid Loss: 0.042317 Valid Acc: 0.600\n",
            "Época: 13/19 Train Loss: 0.004097 Valid Loss: 0.045191 Valid Acc: 0.550\n",
            "Época: 14/19 Train Loss: 0.003057 Valid Loss: 0.047127 Valid Acc: 0.550\n",
            "Época: 15/19 Train Loss: 0.002845 Valid Loss: 0.049095 Valid Acc: 0.550\n",
            "Época: 16/19 Train Loss: 0.002526 Valid Loss: 0.052359 Valid Acc: 0.550\n",
            "Época: 17/19 Train Loss: 0.001845 Valid Loss: 0.056199 Valid Acc: 0.550\n",
            "Época: 18/19 Train Loss: 0.001383 Valid Loss: 0.059482 Valid Acc: 0.600\n",
            "Época: 19/19 Train Loss: 0.001380 Valid Loss: 0.061172 Valid Acc: 0.650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULkZNIf_9bJf",
        "outputId": "92250e3c-d5f7-46df-9834-d94ca5d7c1e7"
      },
      "source": [
        "accumulated_accuracy = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x_test_, y_test_, attention_mask in dataloader_train:\n",
        "        x_test_ = x_test_.to(device)\n",
        "        y_test_ = y_test_.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # predict da rede\n",
        "        outputs = model(input_ids=x_test_, attention_mask=attention_mask, labels=y_test_)\n",
        "        # calcula a perda\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y_test_).sum()\n",
        "        accumulated_accuracy += batch_accuracy\n",
        "\n",
        "test_acc = accumulated_accuracy / len(dataloader_train.dataset)\n",
        "test_acc *= 100\n",
        "print('*' * 40)\n",
        "print(f'Acurácia de {test_acc:.3f} %')\n",
        "print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 100.000 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHq7e1Tr5y9V"
      },
      "source": [
        "# Análise de sentimento utilizando BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4RivVx29YjM",
        "outputId": "55db3259-7778-427f-8867-218d0a092803"
      },
      "source": [
        "checkpoint = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoKv8Ouv9vEA"
      },
      "source": [
        "dataloader_train = create_dataloader(x_train, y_train, tokenizer, 20, shuffle=True)\n",
        "dataloader_valid = create_dataloader(x_valid, y_valid, tokenizer, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yDWCZIL95CH",
        "outputId": "48a37d57-4da7-4f8a-d38a-1d540635bcd5"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(model, dataloader_train, dataloader_valid, optimizer, 4, \"bert-imdb\", n_epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/2 Train Loss: 0.014313 Valid Loss: 0.014166 Valid Acc: 0.879\n",
            "best model\n",
            "Época: 1/2 Train Loss: 0.007381 Valid Loss: 0.012810 Valid Acc: 0.912\n",
            "best model\n",
            "Época: 2/2 Train Loss: 0.003596 Valid Loss: 0.014741 Valid Acc: 0.914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we0a4qJlA81-"
      },
      "source": [
        "dataloader_test = create_dataloader(x_test, y_test, tokenizer, 20)\n",
        "accumulated_accuracy = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x_test_, y_test_, attention_mask in dataloader_test:\n",
        "        x_test_ = x_test_.to(device)\n",
        "        y_test_ = y_test_.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # predict da rede\n",
        "        outputs = model(input_ids=x_test_, attention_mask=attention_mask, labels=y_test_)\n",
        "        # calcula a perda\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y_test_).sum()\n",
        "        accumulated_accuracy += batch_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLH9ns1Ha07f",
        "outputId": "b733ba64-9fab-4508-fb18-9878135883c6"
      },
      "source": [
        "test_acc = accumulated_accuracy / len(dataloader_test.dataset)\n",
        "test_acc *= 100\n",
        "print('*' * 40)\n",
        "print(f'Acurácia de {test_acc:.3f} %')\n",
        "print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 91.564 %\n",
            "****************************************\n"
          ]
        }
      ]
    }
  ]
}