{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pedro Gengo - Aula 8 - Exercício",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Pedro_Gengo_Aula_8_Exerc%C3%ADcio_MultiHeadSelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-GKnbP4zVlY",
        "outputId": "5365b38d-bf88-4804-febd-012059305bb3"
      },
      "source": [
        "!pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.12.0.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 13.1 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.2.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.18.56-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Requirement already satisfied: jsonschema<4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.56\n",
            "  Downloading botocore-1.21.56-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 52.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.56->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.17.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 48.3 MB/s \n",
            "\u001b[?25hCollecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.12.0-py2.py3-none-any.whl size=478334 sha256=fe7683969f614af2b82c70fe2a5370f3154fa3ec8cf3470ef756211c9f96e6ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/2f/0a/f16d84ff3a68368a6b7d39d199c37454d2cefaf3b99e6eb435\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=51d01d7e65a1a427c4f51e012d473822ba7eb072f77fe5dcb511d4c13059fd11\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=37625d5bfd5782f43722089e829533f2b8bbff57e22220325ebf8922b41ed45c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, urllib3, strict-rfc3339, rfc3987, jmespath, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 PyJWT-2.2.0 boto3-1.18.56 botocore-1.21.56 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.7 jmespath-0.10.0 jsonref-0.2 monotonic-1.6 neptune-client-0.12.0 rfc3987-1.3.8 s3transfer-0.5.0 simplejson-3.17.5 smmap-4.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "Treinar e medir a acurácia de um modelo de classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "O modelo deverá ter uma camada de auto-atenção completa igual à do artigo do \"Attention is All You Need\".\n",
        "\n",
        "Implementar a Análise de Sentimento do IMDB, igual ao da semana passada (IMDB), mas agora usando a atenção \"completa\":\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Scaled Dot-product\n",
        "- Multi-head\n",
        "- Layer Normalization\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Deverá ser entregue apenas a implementação matricial, ou seja, não precisa implementar a forma em laço.\n",
        "\n",
        "Devemos usar embeddings pretreinados do Glove como entrada para a camada de auto-atenção. Lembrar de congelá-los pois, caso contrário,  pode ocorrer overfit.\n",
        "\n",
        "Ao corrigir o exercicio, iremos também nos atentar na eficiencia/velocidade das implementações.\n",
        "\n",
        "Dicas:\n",
        "- A dificuldade deste exercício será implementar a auto-atenção de forma matricial usando minibatches. Para lidar com exemplos de tamanho variável, deve-se truncá-los e aplicar padding.\n",
        "\n",
        "- Evitar usar qualquer laço na implementação matricial, pois isso a deixará muito ineficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojyVOubollcH"
      },
      "source": [
        "## Definindo os parametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-YHxi_AllQZ"
      },
      "source": [
        "params = {\n",
        "    'vocabulary_size': 400000,\n",
        "    'padding_idx': 400001,\n",
        "    'max_length': 200,\n",
        "    'dim': 300,\n",
        "    'n_heads': 6,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import neptune.new as neptune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "8d30a988-6d5c-445e-b703-794b1c2c9950"
      },
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efbdc977c10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3017d354-b8b1-471a-e0f1-549da33f9e27"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 12:00:58--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 104.26.3.19, 172.67.69.159, 104.26.2.19, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/aclImdb.tgz [following]\n",
            "--2021-10-07 12:00:59--  https://files.fast.ai/data/aclImdb.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.3.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [application/x-gtar-compressed]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  12.4MB/s    in 12s     \n",
            "\n",
            "2021-10-07 12:01:11 (11.6 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d82773-a276-44f7-d74a-b8a059fe585c"
      },
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False It's nothing more than a weird coincidence that I decided to watch STARLIFT on the 59th anniversary \n",
            "False I am wanting to make a \"Holmes with Doors\" pun but I can't quite string it all together. Suitably gr\n",
            "True All the folks who sit here and say that this movie's weak link is the Ramones would probably say tha\n",
            "3 últimas amostras treino:\n",
            "False It's somewhat telling that most of the great reviews for the film on IMDb all come from people who h\n",
            "True This is a bit long (2 hours, 20 minutes) but it had a a lot of the famous Pearl Buck novel in it. In\n",
            "True Surprisingly good. The acting was fun, the screenplay was fun, the music was cheesie fun, the plot w\n",
            "3 primeiras amostras validação:\n",
            "True Of all the kung-fu films made through the 70's and 80's this is one that has developed a real cult f\n",
            "True Excellent film dealing with the life of an old man as he looks back over the years. Starting around \n",
            "True Good movies are original, some leave a message or touch you in a certain way, but sometimes you're n\n",
            "3 últimas amostras validação:\n",
            "True This movie is why I found this website. I couldn't find this movie anywhere else! I am so glad we fo\n",
            "True I loved this movie. I knew it would be chocked full of camp and silliness like the original series. \n",
            "True I was all ready to pan this episode, seeing that this 'Master' really doesn't have any horror films \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c_-4MRtsOL_"
      },
      "source": [
        "# Carregando os embeddings do Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5h_a9nvs5FJ",
        "outputId": "90fc9629-3b7b-4c1d-f7a4-38f1843f178d"
      },
      "source": [
        "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip -d glove_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 12:01:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-10-07 12:01:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-10-07 12:01:29--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 41s  \n",
            "\n",
            "2021-10-07 12:04:11 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove_dir/glove.6B.50d.txt  \n",
            "  inflating: glove_dir/glove.6B.100d.txt  \n",
            "  inflating: glove_dir/glove.6B.200d.txt  \n",
            "  inflating: glove_dir/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9NSvROYvnWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbcffb3-1d54-4b09-e07c-c3be35c71116"
      },
      "source": [
        "from torchtext.vocab import GloVe\n",
        "glove_vectors = GloVe(name='6B', dim=300, cache='./glove_dir')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 399999/400000 [00:50<00:00, 7995.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz4RNqJ3wNzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b610a51-154e-49d5-b657-899a042f6899"
      },
      "source": [
        "print(glove_vectors.vectors.shape)\n",
        "print('Primeiras 20 palavras e seus índices:', list(glove_vectors.stoi.items())[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([400000, 300])\n",
            "Primeiras 20 palavras e seus índices: [('the', 0), (',', 1), ('.', 2), ('of', 3), ('to', 4), ('and', 5), ('in', 6), ('a', 7), ('\"', 8), (\"'s\", 9), ('for', 10), ('-', 11), ('that', 12), ('on', 13), ('is', 14), ('was', 15), ('said', 16), ('with', 17), ('he', 18), ('as', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ySaZBHxTLFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ee43f2-4cc1-4fdb-d619-1904119d8d80"
      },
      "source": [
        "vocab = glove_vectors.stoi\n",
        "vocab['<UNK>'] = params['vocabulary_size'] # The last row is for the unknown token.\n",
        "\n",
        "# We create a random vector for the unknown token\n",
        "unk_vector = torch.FloatTensor(1, glove_vectors.vectors.shape[1]).uniform_(-0.5, 0.5)\n",
        "\n",
        "# We create a vector of zeros for the pad token\n",
        "pad_vector = torch.zeros(1, glove_vectors.vectors.shape[1])\n",
        "\n",
        "# And add them to the embeddings matrix.\n",
        "embeddings = torch.cat((glove_vectors.vectors, unk_vector, pad_vector), dim=0)\n",
        "\n",
        "print(f'Total de palavras: {len(vocab)}')\n",
        "print(f'embeddings.shape: {embeddings.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras: 400001\n",
            "embeddings.shape: torch.Size([400002, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlaPgP0Z_D4"
      },
      "source": [
        "# Definindo o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIpp1C_qZ-QX"
      },
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return [token.lower() for token in re.compile('\\w+').findall(text)]\n",
        "\n",
        "\n",
        "def to_token_ids(text, vocab, max_length, padding_idx):\n",
        "    tokens = tokenize(text)[:max_length]  # Truncating.\n",
        "    token_ids = []\n",
        "    for token in tokens:\n",
        "        # We use the id of the \"<UNK>\" token if we don't find it in the vocabulary.\n",
        "        token_id = vocab.get(token, vocab['<UNK>'])\n",
        "        token_ids.append(token_id)\n",
        "\n",
        "    # Adding PAD tokens, if necessary.\n",
        "    token_ids += [padding_idx] * max(0, max_length - len(token_ids))\n",
        "    return token_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_6pddDHEM_r"
      },
      "source": [
        "## Definindo o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLReRSuDEPLL"
      },
      "source": [
        "# É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "# garantimos que os pesos vao ser sempre os mesmos.\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "class SelfAttentionLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, padding_idx, n_heads, dim, max_length):\n",
        "        super().__init__()\n",
        "        # Escreva o codigo aqui.\n",
        "        # É importante que as camadas seja criadas na ordem abaixo, para\n",
        "        # garantimos que terão os mesmos pesos usados para criar o vetor target\n",
        "        # usado nos asserts:\n",
        "        embedding_dim = embeddings.shape[-1]\n",
        "        self.n_heads = n_heads\n",
        "        self.max_length = max_length\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        # Rede\n",
        "        self.embedding_layer = torch.nn.Embedding.from_pretrained(embeddings, padding_idx = padding_idx)\n",
        "        self.positional_embeddings = torch.nn.Linear(dim, max_length, bias=False)\n",
        "        self.W_q = torch.nn.Linear(dim, dim, bias=False)\n",
        "        self.W_k = torch.nn.Linear(dim, dim, bias=False)\n",
        "        self.W_v = torch.nn.Linear(dim, dim, bias=False)\n",
        "        self.W_o = torch.nn.Linear(dim, dim, bias=False)\n",
        "        self.layer_norm1 = torch.nn.LayerNorm(dim, eps=1e-06)\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "                                                  torch.nn.Linear(dim, dim),\n",
        "                                                  torch.nn.ReLU(),\n",
        "                                                  torch.nn.Linear(dim, dim),\n",
        "                                                ) \n",
        "        self.layer_norm2 = torch.nn.LayerNorm(dim, eps=1e-06)\n",
        "    \n",
        "    def attention(self, q, k, v, mask):\n",
        "        scores = torch.matmul(q, k.transpose(-1, -2))# B, H, L, L\n",
        "        # B, L -> B, 1, 1, L -> B, H, L, L -> Como cada linha de scores eh o score para os outros tokens, queremos replicar a mascara para todos as linhas, ja que os tokens sao os mesmos\n",
        "        mask_expanded = mask[:, None, None, :].expand_as(scores)\n",
        "        scores.masked_fill_(~mask_expanded, float('-inf')) # B, H, L, L -> Preenchemos onde temos PAD com -inf\n",
        "        scores = scores / np.sqrt(self.dim // self.n_heads)\n",
        "        probs = F.softmax(scores, dim=-1) # shape = B, H, L, L -> Tenho as probs para cada token de cada linha\n",
        "        E = torch.matmul(probs, v) # B, H, L, L x B, H, L, D/H = B, H, L, D/H\n",
        "        return E\n",
        "\n",
        "    def forward(self, batch_token_ids):\n",
        "        batch_size = batch_token_ids.shape[0]\n",
        "\n",
        "        embs = self.embedding_layer(batch_token_ids)\n",
        "        x = embs + self.positional_embeddings.weight[None, :, :] # B, L, D\n",
        "        dim_heads = self.dim // self.n_heads\n",
        "\n",
        "        q = self.W_q(x).reshape(batch_size, self.max_length, self.n_heads, dim_heads)\n",
        "        k = self.W_k(x).reshape(batch_size, self.max_length, self.n_heads, dim_heads)\n",
        "        v = self.W_v(x).reshape(batch_size, self.max_length, self.n_heads, dim_heads)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) # H, L, D/H\n",
        "        mask = batch_token_ids != self.padding_idx # B, L -> Estamos pegando os token que nao sao PAD\n",
        "\n",
        "        e = self.attention(q, k, v, mask) # B, H, L, D/H\n",
        "        e = e.transpose(2, 1).contiguous() # B, L, H, D/H\n",
        "        e = e.reshape(batch_size, self.max_length, -1) # B, L, D\n",
        "        e = self.W_o(e) # B, L, D\n",
        "\n",
        "        residual1 = x + e\n",
        "        norm1 = self.layer_norm1(residual1)\n",
        "\n",
        "        feed_forward_x = self.feed_forward(norm1)\n",
        "        residual2 = norm1 + feed_forward_x\n",
        "        norm2 = self.layer_norm2(residual2)\n",
        "        e_out = norm2 * mask[:, :, None]\n",
        "        mean_embeddings = e_out.sum(1) / torch.clamp(mask.sum(1)[:, None], min=1)\n",
        "        return mean_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSbNNLzjya7z"
      },
      "source": [
        "## Testando a implementação com embeddings \"falsos\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K_LJm2lygau",
        "outputId": "7fb47d52-ba1a-475d-8e7d-f3fef5088eb4"
      },
      "source": [
        "fake_vocab = {\n",
        "    'a': 0,\n",
        "    'b': 1,\n",
        "    'c': 2,\n",
        "    '<UNK>': 3 \n",
        "}\n",
        "\n",
        "fake_embeddings = torch.arange(0, 2 * len(fake_vocab)).reshape(len(fake_vocab), 2).float()\n",
        "pad_vector = torch.zeros(1, 2)\n",
        "fake_embeddings = torch.cat((fake_embeddings, pad_vector), dim=0)\n",
        "\n",
        "fake_examples = [\n",
        "    'a', # Testing PAD\n",
        "    'a b',\n",
        "    'a c b', # Testing truncation\n",
        "    'a z', # Testing <UNK>\n",
        "    ]\n",
        "\n",
        "print(f'Total de palavras: {len(fake_vocab)}')\n",
        "print(f'embeddings.shape: {fake_embeddings.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras: 4\n",
            "embeddings.shape: torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5lhQ2YY4rz6",
        "outputId": "340ece68-ed74-4750-943f-296377169a24"
      },
      "source": [
        "fake_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.],\n",
              "        [6., 7.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IHiV6nRzg4q"
      },
      "source": [
        "self_attention_layer = SelfAttentionLayer(\n",
        "    embeddings=fake_embeddings,\n",
        "    padding_idx=4,\n",
        "    dim=2,\n",
        "    n_heads=2,\n",
        "    max_length=2)\n",
        "\n",
        "batch_token_ids = []\n",
        "for example in fake_examples:\n",
        "    token_ids = to_token_ids(\n",
        "        text=example,\n",
        "        vocab=fake_vocab,\n",
        "        max_length=2,\n",
        "        padding_idx=4)\n",
        "    batch_token_ids.append(token_ids)\n",
        "\n",
        "batch_token_ids = torch.LongTensor(batch_token_ids)\n",
        "my_output = self_attention_layer(batch_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFF0AkR74Drr"
      },
      "source": [
        "torch.set_printoptions(precision=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RMkzafh1nkk",
        "outputId": "d0801e8c-d4a4-42a1-a164-48800253e42b"
      },
      "source": [
        "my_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9999975562,  0.9999975562],\n",
              "        [-0.9999975562,  0.9999976158],\n",
              "        [-0.9999975562,  0.9999974966],\n",
              "        [-0.9999974966,  0.9999974966]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJsMGZ911l67"
      },
      "source": [
        "target_output = torch.FloatTensor([\n",
        "    [-0.9999975562,  0.9999975562],\n",
        "    [-0.9999975562,  0.9999976158],\n",
        "    [-0.9999975562,  0.9999974966],\n",
        "    [-0.9999974966,  0.9999974966]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR_sjMsm1kK1"
      },
      "source": [
        "assert torch.allclose(my_output, target_output, atol=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS6p45y4It01"
      },
      "source": [
        "## Testando a implementação com 8 exemplos do dataset do IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm_WeJi-EciW"
      },
      "source": [
        "examples = [\n",
        "    \"THE TEMP (1993) didn't do much theatrical business, but here's the direct-to-video rip-off you didn't want, anyway! Ellen Bradford (Mel Harris) is the new woman at Millennium Investments, a high scale brokerage firm, who starts getting helpful hints from wide-eyed secretary Deidre (Sheila Kelley). Deidre turns out to be an ambitious daddy's girl who will stop at nothing to move up the corporate ladder, including screwing a top broker she can't stand and murdering anyone who gets on her bad side. She digs up skeletons in Ellen's closet, tries to cause problems with her husband (Barry Bostwick), kills while making it look like she is responsible, kidnaps her daughter and tries to get her to embezzle money from the company.<br /><br />Harris and Kelley deliver competent performances, the supporting cast is alright and it's reasonably well put-together, but that doesn't fully compensate for a script that travels down a well-worn path and offers few surprises.\",\n",
        "    \"Sondra Locke stinks in this film, but then she was an awful 'actress' anyway. Unfortunately, she drags everyone else (including then =real life boyfriend Clint Eastwood down the drain with her. But what was Clint Eastwood thinking when he agreed to star in this one? One read of the script should have told him that this one was going to be a real snorer. It's an exceptionally weak story, basically no story or plot at all. Add in bored, poor acting, even from the normally good Eastwood. There's absolutely no action except a couple arguments and as far as I was concerned, this film ranks up at the top of the heap of natural sleep enhancers. Wow! Could a film BE any more boring? I think watching paint dry or the grass grow might be more fun. A real stinker. Don't bother with this one.\",\n",
        "    \"Judy Davis shows us here why she is one of Australia's most respected and loved actors - her portrayal of a lonely, directionless nomad is first-rate. A teenaged Claudia Karvan also gives us a glimpse of what would make her one of this country's most popular actors in years to come, with future roles in THE BIG STEAL, THE HEARTBREAK KID, DATING THE ENEMY, RISK and the acclaimed TV series THE SECRET LIFE OF US. (Incidentally, Karvan, as a child, was a young girl whose toy Panda was stolen outside a chemist's shop in the 1983 drama GOING DOWN with Tracey Mann.) If this films comes your way, make sure you see it!! Rating: 79/100. See also: HOTEL SORRENTO, RADIANCE, VACANT POSSESSION, LANTANA.\",\n",
        "    'New York playwright Michael Caine (as Sidney Bruhl) is 46-years-old and fading fast; as the film opens, Mr. Caine\\'s latest play flops on Broadway. TV reviewers poke fun at Caine, and he gets drunk. Passing out on the Long Island Railroad lands Caine in Montauk, instead of his residence in East Hampton. Finally arriving home, Caine is comforted by tightly-attired wife Dyan Cannon (as Myra), an unfortunately high-strung heart patient. There, Caine and Ms. Cannon discuss a new play called \"Deathtrap\", written by hunky young Christopher Reeve (as Clifford \"Cliff\" Anderson), one of Caine\\'s former students. The couple believe Mr. Reeve\\'s \"Deathtrap\" is the hit needed to revive Caine\\'s career.<br /><br />\"The Trap Is Set\\x85 For A Wickedly Funny Who\\'ll-Do-It.\" <br /><br />Directed by Sidney Lumet, Ira Levin\\'s long-running Broadway hit doesn\\'t stray too far from its stage origin. The cast is enjoyable and the story\\'s twists are still engrossing. One thing that did not work (for me) was the curtain call ending; surely, it played better on stage. \"Deathtrap\" is a fun film to watch again; the performances are dead on - but, in hindsight, the greeting Reeve gives Caine at the East Hampton train station should have been simplified to a smiling \"Hello.\" The location isn\\'t really East Hampton, but the windmill and pond look similar. And, the much ballyhooed love scene is shockingly tepid. But, the play was so good, \"even a gifted director couldn\\'t ruin it.\" And, Mr. Lumet doesn\\'t disappoint.<br /><br />******** Deathtrap (3/19/82) Sidney Lumet ~ Michael Caine, Christopher Reeve, Dyan Cannon, Irene Worth',\n",
        "    'Students often ask me why I choose this version of Othello. Shakespeare\\'s text is strongly truncated and the film contains material which earned it an \"R\" rating.<br /><br />I have several reasons for using this production: First, I had not seen a depiction of the Moor that actually made me sympathetic to Othello until I saw Fishburne play him. I saw James Earl Jones and Christopher Plummer play Othello and Iago on Broadway, and it was wonderful. Plummer\\'s energy was especially noticeable. But in spite of Jone\\'s incredible presence both physically and vocally, the character he played just seemed too passive to illicit from me a complete emotional purgation in the Aristotelian sense. Jones, in fact, affirmed what I felt when in an interview he noted that he had played Othello as passive--seeing Iago as basically doing him over. Unfortunately this sapped my grief for the character destruction. Thus, I felt sympathy for Jone\\'s Moor but not the horror over his corruption by an evil man. In contrast, Fishburne\\'s Othello is a strong and vigorous figure familiar with taking action. Thus, Iago\\'s temptation to actively deal with what is presented to Othello as his wife\\'s unfaithfulness is a perversion of the general\\'s positive quality to be active not passive.1 The horror of the story is that this good quality in Othello becomes perverted. Fishburne\\'s depiction is therefore classically tragic.<br /><br />Second, Fishburne is the first black actor to play Othello in a film. Both Orsen Wells and Anthony Hopkins did fine film versions, but they were white men in black face.2 Why is this important? Why should a Black actor be the Black man on the stage?3 Certainly in Shakespeare\\'s day they used black face just as they used boys to make girls. Perhaps then, the reason is the same. Female actors bring a special quality to female roles on the Shakespearian stage because they understand best what Shakespeare\\'s genius was trying to present. A gifted black actor should play the moor because his experience in a white dominated culture is vital to understanding what Shakespeare\\'s genius recognized: the pain of being marginalized because of race. An important theme in Othello is isolation caused by racism. Although it is a mistake to insert American racism into a Shakespearian play, there can be little doubt that racism is still working among the characters. Many, including Desdimona\\'s father, think that a union between a Venetian white Christian woman and a North African black Christian man is UNNATURAL.<br /><br />Third, Shakespeare was never G rated. He never has been. His stage productions were always typified by violence and strong language. But Shakespeare\\'s genius uses these elements not as sensationialism but for artistic honesty.',\n",
        "    'Roeg has done some great movies, but this a turkey. It has a feel of a play written by an untalented high-school student for his class assignment. The set decoration is appealing in a somewhat surrealistic way, but the actual story is insufferable hokum.',\n",
        "    \"<br /><br />What is left of Planet Earth is populated by a few poor and starving rag-tag survivors. They must eat bugs and insects, or whatever, after a poison war, or something, has nearly wiped out all human civilization. In these dark times, one of the few people on Earth still able to live in comfort, we will call him the All Knowing Big Boss, has a great quest to prevent some secret spore seeds from being released into the air. It seems that the All Knowing Big Boss is the last person on Earth that knows that these spores even exist. The spores are located far away from any living soul, and they are highly protected by many layers of deadly defense systems. <br /><br />The All Knowing Big Boss wants the secret spores to remain in their secret protected containers. So, he makes a plan to send in a macho action team to remove the spore containers from all of the protective systems and secret location. Sending people to the location of secret spores makes them no longer a secret. Sending people to disable all of the protective systems makes it possible for the spores to be easily released into the air. How about letting sleeping dogs lie?! <br /><br />The one pleasant feature of ENCRYPT is the radiant and elegant Vivian Wu. As the unremarkable macho action team members drop off with mechanically paced predictable timing, engaging Vivian Wu's charm makes acceptable the plot idea of her old employer wanting her so much. She is an object of love, an object of desire -- a very believable concept!<br /><br />Fans of Vivian Wu may want to check out an outstanding B-movie she is in from a couple years back called DINNER RUSH. DINNER RUSH is highly recommended. ENCRYPT is not.\",\n",
        "    \"So the other night I decided to watch Tales from the Hollywood Hills: Natica Jackson. Or Power, Passion, Murder as it is called in Holland. When I bought the film I noticed that Michelle Pfeiffer was starring in it and I thought that had to say something about the quality. Unfortunately, it didn't.<br /><br />1) The plot of the film is really confusing. There are two story lines running simultaneously during the film. Only they have nothing in common. Throughout the entire movie I was waiting for the moment these two story lines would come together so the plot would be clear to me. But it still hasn't.<br /><br />2) The title of the film says the film will be about Natica Jackson. Well it is, sometimes. Like said the film covers two different stories and the part about Natica Jackson is the shortest. So another title for this movie would not be a wrong choice.<br /><br />To conclude my story, I really recommend that you leave this movie where it belongs, on the shelf in the store on a place nobody can see it. By doing this you won't waste 90 minutes of your life, as I did.\"         \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glpuB87bI1_X"
      },
      "source": [
        "self_attention_layer = SelfAttentionLayer(\n",
        "    embeddings=embeddings,\n",
        "    padding_idx=params['padding_idx'],\n",
        "    dim=params['dim'],\n",
        "    n_heads=params['n_heads'],\n",
        "    max_length=params['max_length'])\n",
        "\n",
        "batch_token_ids = []\n",
        "for example in examples:\n",
        "    token_ids = to_token_ids(\n",
        "        text=example,\n",
        "        vocab=vocab,\n",
        "        max_length=params['max_length'],\n",
        "        padding_idx=params['padding_idx'])\n",
        "    batch_token_ids.append(token_ids)\n",
        "\n",
        "batch_token_ids = torch.LongTensor(batch_token_ids)\n",
        "my_output = self_attention_layer(batch_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0VUv1A_r5oO"
      },
      "source": [
        "Fazemos o download do tensor esperado e o comparamos com nossa saída"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THva0R6h6A_e",
        "outputId": "db60e1ec-5127-4d33-d562-6faa9545cba1"
      },
      "source": [
        "!wget https://storage.googleapis.com/neuralresearcher_data/unicamp/ia376e_2021s2/aula8/target_tensor.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 12:05:29--  https://storage.googleapis.com/neuralresearcher_data/unicamp/ia376e_2021s2/aula8/target_tensor.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.15.128, 173.194.76.128, 66.102.1.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.15.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10347 (10K) [application/octet-stream]\n",
            "Saving to: ‘target_tensor.pt’\n",
            "\n",
            "\rtarget_tensor.pt      0%[                    ]       0  --.-KB/s               \rtarget_tensor.pt    100%[===================>]  10.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-07 12:05:29 (78.8 MB/s) - ‘target_tensor.pt’ saved [10347/10347]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2C1ZvxoAkCN"
      },
      "source": [
        "target_output = torch.load('target_tensor.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNPTGnCLGBl"
      },
      "source": [
        "assert torch.allclose(my_output, target_output, atol=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh7NNucM38wm"
      },
      "source": [
        "# Classificador\n",
        "\n",
        "Diferentemente dos outros exercícios, aqui iremos usar a camada de _self attention_ implementada acima com o intuito realizar uma classificação binária das avaliações de filmes entre positiva e negativa. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfJUUfCF4AJf"
      },
      "source": [
        "### Criação da classe Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHfqut36gAX"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, x, y, tokenizer, vocab, max_length, padding_idx):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab = vocab\n",
        "    self.max_length = max_length\n",
        "    self.padding_idx = padding_idx\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.tokenizer(self.x[idx], self.vocab, self.max_length, self.padding_idx)\n",
        "    return torch.tensor(x).long(), torch.tensor(self.y[idx]).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirnjDaI4ssk"
      },
      "source": [
        "## Loops de treino, validação e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4dsfZpsQ0Y_",
        "outputId": "738055c1-042d-46aa-b84f-d0c365dbcaef"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN1sSxVzQ5N7"
      },
      "source": [
        "def train(model, train, valid, criterion, optimizer, filename_save, n_epochs=10, run=None, params=None):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "  train_losses, valid_losses = [], []\n",
        "  if run:\n",
        "    run['parameters'] = params\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device).reshape(-1, 1)\n",
        "      outputs = model(x_train)\n",
        "      batch_loss = criterion(outputs, y_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device).reshape(-1, 1)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(x_valid)\n",
        "\n",
        "            # calcula a perda\n",
        "            batch_loss = criterion(outputs, y_valid)\n",
        "            preds = outputs > 0.5\n",
        "            # preds = outputs.argmax(dim=1)\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    if run:\n",
        "      run[f\"{filename_save}_valid/loss\"].log(valid_loss)\n",
        "      run[f\"{filename_save}_valid/acc\"].log(valid_acc)\n",
        "      run[f\"{filename_save}_train/loss\"].log(train_loss)\n",
        "\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), filename_save + '.pt')\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I0eLbvERClx"
      },
      "source": [
        "def predict(model, state_dict, test, run=None):\n",
        "  accumulated_accuracy = 0\n",
        "  model.load_state_dict(torch.load(state_dict + '.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for x_test, y_test in test:\n",
        "          x_test = x_test.to(device)\n",
        "          y_test = y_test.to(device).reshape(-1,1)\n",
        "\n",
        "          # predict da rede\n",
        "          outputs = model(x_test)\n",
        "  \n",
        "          # calcula a perda\n",
        "          batch_loss = criterion(outputs, y_test)\n",
        "          preds = outputs > 0.5\n",
        "          # preds = outputs.argmax(dim=1)\n",
        "\n",
        "          # calcula a acurácia\n",
        "          batch_accuracy = (preds == y_test).sum()\n",
        "          accumulated_accuracy += batch_accuracy\n",
        "\n",
        "  test_acc = accumulated_accuracy / len(test.dataset)\n",
        "  test_acc *= 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {test_acc:.3f} %')\n",
        "  print('*' * 40)\n",
        "\n",
        "  if run:\n",
        "    run['results'] = test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk6YEgox4C8r"
      },
      "source": [
        "## Definição da rede\n",
        "\n",
        "Usarei uma rede onde temos uma camada de atenção seguida por duas camadas lineares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Igy1xNN-XA"
      },
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embeddings, padding_idx, n_heads, dim, max_length, size_lin1):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.self_attention = SelfAttentionLayer(embeddings, padding_idx, n_heads, dim, max_length)\n",
        "    self.lin1 = torch.nn.Linear(dim, size_lin1)\n",
        "    self.lin2 = torch.nn.Linear(size_lin1, 1)\n",
        "    self.output = torch.nn.Sigmoid()\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.lin1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    x = self.output(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSyY1VQFQE5U"
      },
      "source": [
        "## Experimento teste\n",
        "\n",
        "Utilizando poucos dados, irei rodar um experimento para validar a execução do fluxo de treino e validação e verificar se está ocorrendo overfit com poucos dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJAXBUBm_FWK"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 300\n",
        "batch_size = 50\n",
        "hidden_size = 150\n",
        "filename = \"self_attention\"\n",
        "\n",
        "hprams = {\"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"hidden_size\": hidden_size\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuWqZpmY_Je-"
      },
      "source": [
        "dataset_train = Dataset(x_train[:50], y_train[:50], to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_valid = Dataset(x_valid[:50], y_valid[:50], to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_test = Dataset(x_test, y_test, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=hprams[\"batch_size\"], shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=hprams[\"batch_size\"], shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=hprams[\"batch_size\"], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDZ9ebSa_LPz",
        "outputId": "0e3d0bfc-746e-4b5c-e1df-43e25b984643"
      },
      "source": [
        "cls = Classifier(embeddings, params['padding_idx'], params['n_heads'], params['dim'], params['max_length'], hprams[\"hidden_size\"])\n",
        "cls.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (self_attention): SelfAttentionLayer(\n",
              "    (embedding_layer): Embedding(400002, 300, padding_idx=400001)\n",
              "    (positional_embeddings): Linear(in_features=300, out_features=200, bias=False)\n",
              "    (W_q): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_k): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_v): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_o): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (layer_norm1): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
              "    (feed_forward): Sequential(\n",
              "      (0): Linear(in_features=300, out_features=300, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=300, out_features=300, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (lin1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
              "  (output): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpohmcmb_Lwh"
      },
      "source": [
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(cls.parameters(), lr=hprams[\"learning_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhuhAjF7_O3b",
        "outputId": "2d8409e6-53bf-462f-fcf4-800c4ad5bebb"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(cls, dataloader_train, dataloader_valid, criterion,\n",
        "          optimizer, filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/299 Train Loss: 0.013815 Valid Loss: 0.036262 Valid Acc: 0.400\n",
            "best model\n",
            "Época: 1/299 Train Loss: 0.028075 Valid Loss: 0.024280 Valid Acc: 0.600\n",
            "best model\n",
            "Época: 2/299 Train Loss: 0.032322 Valid Loss: 0.015749 Valid Acc: 0.400\n",
            "best model\n",
            "Época: 3/299 Train Loss: 0.014194 Valid Loss: 0.014523 Valid Acc: 0.400\n",
            "best model\n",
            "Época: 4/299 Train Loss: 0.013814 Valid Loss: 0.013507 Valid Acc: 0.600\n",
            "best model\n",
            "Época: 5/299 Train Loss: 0.015013 Valid Loss: 0.019556 Valid Acc: 0.400\n",
            "Época: 6/299 Train Loss: 0.016226 Valid Loss: 0.015194 Valid Acc: 0.400\n",
            "Época: 7/299 Train Loss: 0.013928 Valid Loss: 0.013575 Valid Acc: 0.600\n",
            "Época: 8/299 Train Loss: 0.015141 Valid Loss: 0.013542 Valid Acc: 0.600\n",
            "Época: 9/299 Train Loss: 0.013912 Valid Loss: 0.016211 Valid Acc: 0.400\n",
            "Época: 10/299 Train Loss: 0.014030 Valid Loss: 0.016556 Valid Acc: 0.400\n",
            "Época: 11/299 Train Loss: 0.013891 Valid Loss: 0.014410 Valid Acc: 0.400\n",
            "Época: 12/299 Train Loss: 0.012674 Valid Loss: 0.013885 Valid Acc: 0.520\n",
            "Época: 13/299 Train Loss: 0.011523 Valid Loss: 0.018738 Valid Acc: 0.540\n",
            "Época: 14/299 Train Loss: 0.008844 Valid Loss: 0.022468 Valid Acc: 0.520\n",
            "Época: 15/299 Train Loss: 0.006855 Valid Loss: 0.018267 Valid Acc: 0.520\n",
            "Época: 16/299 Train Loss: 0.005259 Valid Loss: 0.020740 Valid Acc: 0.520\n",
            "Época: 17/299 Train Loss: 0.003584 Valid Loss: 0.026293 Valid Acc: 0.480\n",
            "Época: 18/299 Train Loss: 0.002568 Valid Loss: 0.029718 Valid Acc: 0.480\n",
            "Época: 19/299 Train Loss: 0.001419 Valid Loss: 0.033935 Valid Acc: 0.480\n",
            "Época: 20/299 Train Loss: 0.000777 Valid Loss: 0.042852 Valid Acc: 0.460\n",
            "Época: 21/299 Train Loss: 0.000265 Valid Loss: 0.050665 Valid Acc: 0.480\n",
            "Época: 22/299 Train Loss: 0.000152 Valid Loss: 0.057520 Valid Acc: 0.460\n",
            "Época: 23/299 Train Loss: 0.000078 Valid Loss: 0.064306 Valid Acc: 0.460\n",
            "Época: 24/299 Train Loss: 0.000037 Valid Loss: 0.071285 Valid Acc: 0.480\n",
            "Época: 25/299 Train Loss: 0.000017 Valid Loss: 0.078031 Valid Acc: 0.480\n",
            "Época: 26/299 Train Loss: 0.000008 Valid Loss: 0.084493 Valid Acc: 0.460\n",
            "Época: 27/299 Train Loss: 0.000003 Valid Loss: 0.090857 Valid Acc: 0.480\n",
            "Época: 28/299 Train Loss: 0.000001 Valid Loss: 0.096980 Valid Acc: 0.480\n",
            "Época: 29/299 Train Loss: 0.000001 Valid Loss: 0.102351 Valid Acc: 0.480\n",
            "Época: 30/299 Train Loss: 0.000000 Valid Loss: 0.106763 Valid Acc: 0.500\n",
            "Época: 31/299 Train Loss: 0.000000 Valid Loss: 0.110574 Valid Acc: 0.520\n",
            "Época: 32/299 Train Loss: 0.000000 Valid Loss: 0.114980 Valid Acc: 0.520\n",
            "Época: 33/299 Train Loss: 0.000000 Valid Loss: 0.119016 Valid Acc: 0.520\n",
            "Época: 34/299 Train Loss: 0.000000 Valid Loss: 0.122098 Valid Acc: 0.520\n",
            "Época: 35/299 Train Loss: 0.000000 Valid Loss: 0.159067 Valid Acc: 0.520\n",
            "Época: 36/299 Train Loss: 0.000000 Valid Loss: 0.195329 Valid Acc: 0.520\n",
            "Época: 37/299 Train Loss: 0.000000 Valid Loss: 0.197118 Valid Acc: 0.520\n",
            "Época: 38/299 Train Loss: 0.000000 Valid Loss: 0.266333 Valid Acc: 0.520\n",
            "Época: 39/299 Train Loss: 0.000000 Valid Loss: 0.301473 Valid Acc: 0.520\n",
            "Época: 40/299 Train Loss: 0.000000 Valid Loss: 0.369714 Valid Acc: 0.520\n",
            "Época: 41/299 Train Loss: 0.000000 Valid Loss: 0.370924 Valid Acc: 0.520\n",
            "Época: 42/299 Train Loss: 0.000000 Valid Loss: 0.439026 Valid Acc: 0.520\n",
            "Época: 43/299 Train Loss: 0.000000 Valid Loss: 0.473381 Valid Acc: 0.520\n",
            "Época: 44/299 Train Loss: 0.000000 Valid Loss: 0.474159 Valid Acc: 0.520\n",
            "Época: 45/299 Train Loss: 0.000000 Valid Loss: 0.474810 Valid Acc: 0.520\n",
            "Época: 46/299 Train Loss: 0.000000 Valid Loss: 0.475368 Valid Acc: 0.520\n",
            "Época: 47/299 Train Loss: 0.000000 Valid Loss: 0.476068 Valid Acc: 0.520\n",
            "Época: 48/299 Train Loss: 0.000000 Valid Loss: 0.476383 Valid Acc: 0.520\n",
            "Época: 49/299 Train Loss: 0.000000 Valid Loss: 0.510372 Valid Acc: 0.520\n",
            "Época: 50/299 Train Loss: 0.000000 Valid Loss: 0.510976 Valid Acc: 0.520\n",
            "Época: 51/299 Train Loss: 0.000000 Valid Loss: 0.511674 Valid Acc: 0.520\n",
            "Época: 52/299 Train Loss: 0.000000 Valid Loss: 0.512364 Valid Acc: 0.500\n",
            "Época: 53/299 Train Loss: 0.000000 Valid Loss: 0.512845 Valid Acc: 0.500\n",
            "Época: 54/299 Train Loss: 0.000000 Valid Loss: 0.513294 Valid Acc: 0.500\n",
            "Época: 55/299 Train Loss: 0.000000 Valid Loss: 0.513854 Valid Acc: 0.500\n",
            "Época: 56/299 Train Loss: 0.000000 Valid Loss: 0.514406 Valid Acc: 0.500\n",
            "Época: 57/299 Train Loss: 0.000000 Valid Loss: 0.514937 Valid Acc: 0.500\n",
            "Época: 58/299 Train Loss: 0.000000 Valid Loss: 0.515433 Valid Acc: 0.500\n",
            "Época: 59/299 Train Loss: 0.000000 Valid Loss: 0.515892 Valid Acc: 0.500\n",
            "Época: 60/299 Train Loss: 0.000000 Valid Loss: 0.516316 Valid Acc: 0.500\n",
            "Época: 61/299 Train Loss: 0.000000 Valid Loss: 0.516796 Valid Acc: 0.500\n",
            "Época: 62/299 Train Loss: 0.000000 Valid Loss: 0.517155 Valid Acc: 0.500\n",
            "Época: 63/299 Train Loss: 0.000000 Valid Loss: 0.517471 Valid Acc: 0.500\n",
            "Época: 64/299 Train Loss: 0.000000 Valid Loss: 0.517759 Valid Acc: 0.500\n",
            "Época: 65/299 Train Loss: 0.000000 Valid Loss: 0.518021 Valid Acc: 0.500\n",
            "Época: 66/299 Train Loss: 0.000000 Valid Loss: 0.518260 Valid Acc: 0.500\n",
            "Época: 67/299 Train Loss: 0.000000 Valid Loss: 0.518477 Valid Acc: 0.500\n",
            "Época: 68/299 Train Loss: 0.000000 Valid Loss: 0.518674 Valid Acc: 0.500\n",
            "Época: 69/299 Train Loss: 0.000000 Valid Loss: 0.518852 Valid Acc: 0.500\n",
            "Época: 70/299 Train Loss: 0.000000 Valid Loss: 0.519011 Valid Acc: 0.500\n",
            "Época: 71/299 Train Loss: 0.000000 Valid Loss: 0.519155 Valid Acc: 0.500\n",
            "Época: 72/299 Train Loss: 0.000000 Valid Loss: 0.519285 Valid Acc: 0.500\n",
            "Época: 73/299 Train Loss: 0.000000 Valid Loss: 0.519404 Valid Acc: 0.500\n",
            "Época: 74/299 Train Loss: 0.000000 Valid Loss: 0.519512 Valid Acc: 0.500\n",
            "Época: 75/299 Train Loss: 0.000000 Valid Loss: 0.519610 Valid Acc: 0.500\n",
            "Época: 76/299 Train Loss: 0.000000 Valid Loss: 0.553322 Valid Acc: 0.500\n",
            "Época: 77/299 Train Loss: 0.000000 Valid Loss: 0.553402 Valid Acc: 0.500\n",
            "Época: 78/299 Train Loss: 0.000000 Valid Loss: 0.553475 Valid Acc: 0.500\n",
            "Época: 79/299 Train Loss: 0.000000 Valid Loss: 0.553541 Valid Acc: 0.500\n",
            "Época: 80/299 Train Loss: 0.000000 Valid Loss: 0.553600 Valid Acc: 0.500\n",
            "Época: 81/299 Train Loss: 0.000000 Valid Loss: 0.553654 Valid Acc: 0.500\n",
            "Época: 82/299 Train Loss: 0.000000 Valid Loss: 0.553703 Valid Acc: 0.500\n",
            "Época: 83/299 Train Loss: 0.000000 Valid Loss: 0.553747 Valid Acc: 0.500\n",
            "Época: 84/299 Train Loss: 0.000000 Valid Loss: 0.553787 Valid Acc: 0.500\n",
            "Época: 85/299 Train Loss: 0.000000 Valid Loss: 0.553823 Valid Acc: 0.500\n",
            "Época: 86/299 Train Loss: 0.000000 Valid Loss: 0.553856 Valid Acc: 0.500\n",
            "Época: 87/299 Train Loss: 0.000000 Valid Loss: 0.553886 Valid Acc: 0.500\n",
            "Época: 88/299 Train Loss: 0.000000 Valid Loss: 0.553913 Valid Acc: 0.500\n",
            "Época: 89/299 Train Loss: 0.000000 Valid Loss: 0.553938 Valid Acc: 0.500\n",
            "Época: 90/299 Train Loss: 0.000000 Valid Loss: 0.553960 Valid Acc: 0.500\n",
            "Época: 91/299 Train Loss: 0.000000 Valid Loss: 0.553981 Valid Acc: 0.500\n",
            "Época: 92/299 Train Loss: 0.000000 Valid Loss: 0.553999 Valid Acc: 0.500\n",
            "Época: 93/299 Train Loss: 0.000000 Valid Loss: 0.554016 Valid Acc: 0.500\n",
            "Época: 94/299 Train Loss: 0.000000 Valid Loss: 0.554031 Valid Acc: 0.500\n",
            "Época: 95/299 Train Loss: 0.000000 Valid Loss: 0.554045 Valid Acc: 0.500\n",
            "Época: 96/299 Train Loss: 0.000000 Valid Loss: 0.554057 Valid Acc: 0.500\n",
            "Época: 97/299 Train Loss: 0.000000 Valid Loss: 0.554068 Valid Acc: 0.500\n",
            "Época: 98/299 Train Loss: 0.000000 Valid Loss: 0.554079 Valid Acc: 0.500\n",
            "Época: 99/299 Train Loss: 0.000000 Valid Loss: 0.554088 Valid Acc: 0.500\n",
            "Época: 100/299 Train Loss: 0.000000 Valid Loss: 0.554097 Valid Acc: 0.500\n",
            "Época: 101/299 Train Loss: 0.000000 Valid Loss: 0.554104 Valid Acc: 0.500\n",
            "Época: 102/299 Train Loss: 0.000000 Valid Loss: 0.554111 Valid Acc: 0.500\n",
            "Época: 103/299 Train Loss: 0.000000 Valid Loss: 0.554118 Valid Acc: 0.500\n",
            "Época: 104/299 Train Loss: 0.000000 Valid Loss: 0.554124 Valid Acc: 0.500\n",
            "Época: 105/299 Train Loss: 0.000000 Valid Loss: 0.554129 Valid Acc: 0.500\n",
            "Época: 106/299 Train Loss: 0.000000 Valid Loss: 0.554134 Valid Acc: 0.500\n",
            "Época: 107/299 Train Loss: 0.000000 Valid Loss: 0.554139 Valid Acc: 0.500\n",
            "Época: 108/299 Train Loss: 0.000000 Valid Loss: 0.554144 Valid Acc: 0.500\n",
            "Época: 109/299 Train Loss: 0.000000 Valid Loss: 0.554148 Valid Acc: 0.500\n",
            "Época: 110/299 Train Loss: 0.000000 Valid Loss: 0.554152 Valid Acc: 0.500\n",
            "Época: 111/299 Train Loss: 0.000000 Valid Loss: 0.554155 Valid Acc: 0.500\n",
            "Época: 112/299 Train Loss: 0.000000 Valid Loss: 0.554158 Valid Acc: 0.500\n",
            "Época: 113/299 Train Loss: 0.000000 Valid Loss: 0.554161 Valid Acc: 0.500\n",
            "Época: 114/299 Train Loss: 0.000000 Valid Loss: 0.554164 Valid Acc: 0.500\n",
            "Época: 115/299 Train Loss: 0.000000 Valid Loss: 0.554167 Valid Acc: 0.500\n",
            "Época: 116/299 Train Loss: 0.000000 Valid Loss: 0.554170 Valid Acc: 0.500\n",
            "Época: 117/299 Train Loss: 0.000000 Valid Loss: 0.554172 Valid Acc: 0.500\n",
            "Época: 118/299 Train Loss: 0.000000 Valid Loss: 0.554174 Valid Acc: 0.500\n",
            "Época: 119/299 Train Loss: 0.000000 Valid Loss: 0.554177 Valid Acc: 0.500\n",
            "Época: 120/299 Train Loss: 0.000000 Valid Loss: 0.554179 Valid Acc: 0.500\n",
            "Época: 121/299 Train Loss: 0.000000 Valid Loss: 0.554181 Valid Acc: 0.500\n",
            "Época: 122/299 Train Loss: 0.000000 Valid Loss: 0.554183 Valid Acc: 0.500\n",
            "Época: 123/299 Train Loss: 0.000000 Valid Loss: 0.554184 Valid Acc: 0.500\n",
            "Época: 124/299 Train Loss: 0.000000 Valid Loss: 0.554186 Valid Acc: 0.500\n",
            "Época: 125/299 Train Loss: 0.000000 Valid Loss: 0.554188 Valid Acc: 0.500\n",
            "Época: 126/299 Train Loss: 0.000000 Valid Loss: 0.554190 Valid Acc: 0.500\n",
            "Época: 127/299 Train Loss: 0.000000 Valid Loss: 0.554191 Valid Acc: 0.500\n",
            "Época: 128/299 Train Loss: 0.000000 Valid Loss: 0.554193 Valid Acc: 0.500\n",
            "Época: 129/299 Train Loss: 0.000000 Valid Loss: 0.554195 Valid Acc: 0.500\n",
            "Época: 130/299 Train Loss: 0.000000 Valid Loss: 0.554196 Valid Acc: 0.500\n",
            "Época: 131/299 Train Loss: 0.000000 Valid Loss: 0.554198 Valid Acc: 0.500\n",
            "Época: 132/299 Train Loss: 0.000000 Valid Loss: 0.554199 Valid Acc: 0.500\n",
            "Época: 133/299 Train Loss: 0.000000 Valid Loss: 0.554201 Valid Acc: 0.500\n",
            "Época: 134/299 Train Loss: 0.000000 Valid Loss: 0.554202 Valid Acc: 0.500\n",
            "Época: 135/299 Train Loss: 0.000000 Valid Loss: 0.554204 Valid Acc: 0.500\n",
            "Época: 136/299 Train Loss: 0.000000 Valid Loss: 0.554206 Valid Acc: 0.500\n",
            "Época: 137/299 Train Loss: 0.000000 Valid Loss: 0.554207 Valid Acc: 0.500\n",
            "Época: 138/299 Train Loss: 0.000000 Valid Loss: 0.554209 Valid Acc: 0.500\n",
            "Época: 139/299 Train Loss: 0.000000 Valid Loss: 0.554210 Valid Acc: 0.500\n",
            "Época: 140/299 Train Loss: 0.000000 Valid Loss: 0.554212 Valid Acc: 0.500\n",
            "Época: 141/299 Train Loss: 0.000000 Valid Loss: 0.554213 Valid Acc: 0.500\n",
            "Época: 142/299 Train Loss: 0.000000 Valid Loss: 0.554215 Valid Acc: 0.500\n",
            "Época: 143/299 Train Loss: 0.000000 Valid Loss: 0.554217 Valid Acc: 0.500\n",
            "Época: 144/299 Train Loss: 0.000000 Valid Loss: 0.554218 Valid Acc: 0.500\n",
            "Época: 145/299 Train Loss: 0.000000 Valid Loss: 0.554220 Valid Acc: 0.500\n",
            "Época: 146/299 Train Loss: 0.000000 Valid Loss: 0.554222 Valid Acc: 0.500\n",
            "Época: 147/299 Train Loss: 0.000000 Valid Loss: 0.554223 Valid Acc: 0.500\n",
            "Época: 148/299 Train Loss: 0.000000 Valid Loss: 0.554225 Valid Acc: 0.500\n",
            "Época: 149/299 Train Loss: 0.000000 Valid Loss: 0.554227 Valid Acc: 0.500\n",
            "Época: 150/299 Train Loss: 0.000000 Valid Loss: 0.554229 Valid Acc: 0.500\n",
            "Época: 151/299 Train Loss: 0.000000 Valid Loss: 0.554231 Valid Acc: 0.500\n",
            "Época: 152/299 Train Loss: 0.000000 Valid Loss: 0.554233 Valid Acc: 0.500\n",
            "Época: 153/299 Train Loss: 0.000000 Valid Loss: 0.554235 Valid Acc: 0.500\n",
            "Época: 154/299 Train Loss: 0.000000 Valid Loss: 0.554237 Valid Acc: 0.500\n",
            "Época: 155/299 Train Loss: 0.000000 Valid Loss: 0.554239 Valid Acc: 0.500\n",
            "Época: 156/299 Train Loss: 0.000000 Valid Loss: 0.554241 Valid Acc: 0.500\n",
            "Época: 157/299 Train Loss: 0.000000 Valid Loss: 0.554243 Valid Acc: 0.500\n",
            "Época: 158/299 Train Loss: 0.000000 Valid Loss: 0.554245 Valid Acc: 0.500\n",
            "Época: 159/299 Train Loss: 0.000000 Valid Loss: 0.554247 Valid Acc: 0.500\n",
            "Época: 160/299 Train Loss: 0.000000 Valid Loss: 0.554249 Valid Acc: 0.500\n",
            "Época: 161/299 Train Loss: 0.000000 Valid Loss: 0.554252 Valid Acc: 0.500\n",
            "Época: 162/299 Train Loss: 0.000000 Valid Loss: 0.554254 Valid Acc: 0.500\n",
            "Época: 163/299 Train Loss: 0.000000 Valid Loss: 0.554256 Valid Acc: 0.500\n",
            "Época: 164/299 Train Loss: 0.000000 Valid Loss: 0.554259 Valid Acc: 0.500\n",
            "Época: 165/299 Train Loss: 0.000000 Valid Loss: 0.554261 Valid Acc: 0.500\n",
            "Época: 166/299 Train Loss: 0.000000 Valid Loss: 0.554264 Valid Acc: 0.500\n",
            "Época: 167/299 Train Loss: 0.000000 Valid Loss: 0.554266 Valid Acc: 0.500\n",
            "Época: 168/299 Train Loss: 0.000000 Valid Loss: 0.554269 Valid Acc: 0.500\n",
            "Época: 169/299 Train Loss: 0.000000 Valid Loss: 0.554271 Valid Acc: 0.500\n",
            "Época: 170/299 Train Loss: 0.000000 Valid Loss: 0.554274 Valid Acc: 0.500\n",
            "Época: 171/299 Train Loss: 0.000000 Valid Loss: 0.554277 Valid Acc: 0.500\n",
            "Época: 172/299 Train Loss: 0.000000 Valid Loss: 0.554280 Valid Acc: 0.500\n",
            "Época: 173/299 Train Loss: 0.000000 Valid Loss: 0.554283 Valid Acc: 0.500\n",
            "Época: 174/299 Train Loss: 0.000000 Valid Loss: 0.554287 Valid Acc: 0.500\n",
            "Época: 175/299 Train Loss: 0.000000 Valid Loss: 0.554290 Valid Acc: 0.500\n",
            "Época: 176/299 Train Loss: 0.000000 Valid Loss: 0.554294 Valid Acc: 0.500\n",
            "Época: 177/299 Train Loss: 0.000000 Valid Loss: 0.554298 Valid Acc: 0.500\n",
            "Época: 178/299 Train Loss: 0.000000 Valid Loss: 0.554302 Valid Acc: 0.500\n",
            "Época: 179/299 Train Loss: 0.000000 Valid Loss: 0.554307 Valid Acc: 0.500\n",
            "Época: 180/299 Train Loss: 0.000000 Valid Loss: 0.554312 Valid Acc: 0.500\n",
            "Época: 181/299 Train Loss: 0.000000 Valid Loss: 0.554316 Valid Acc: 0.500\n",
            "Época: 182/299 Train Loss: 0.000000 Valid Loss: 0.554321 Valid Acc: 0.500\n",
            "Época: 183/299 Train Loss: 0.000000 Valid Loss: 0.554326 Valid Acc: 0.500\n",
            "Época: 184/299 Train Loss: 0.000000 Valid Loss: 0.554331 Valid Acc: 0.500\n",
            "Época: 185/299 Train Loss: 0.000000 Valid Loss: 0.554336 Valid Acc: 0.500\n",
            "Época: 186/299 Train Loss: 0.000000 Valid Loss: 0.554341 Valid Acc: 0.500\n",
            "Época: 187/299 Train Loss: 0.000000 Valid Loss: 0.554346 Valid Acc: 0.500\n",
            "Época: 188/299 Train Loss: 0.000000 Valid Loss: 0.554352 Valid Acc: 0.500\n",
            "Época: 189/299 Train Loss: 0.000000 Valid Loss: 0.554357 Valid Acc: 0.500\n",
            "Época: 190/299 Train Loss: 0.000000 Valid Loss: 0.554362 Valid Acc: 0.500\n",
            "Época: 191/299 Train Loss: 0.000000 Valid Loss: 0.554368 Valid Acc: 0.500\n",
            "Época: 192/299 Train Loss: 0.000000 Valid Loss: 0.554374 Valid Acc: 0.500\n",
            "Época: 193/299 Train Loss: 0.000000 Valid Loss: 0.554379 Valid Acc: 0.500\n",
            "Época: 194/299 Train Loss: 0.000000 Valid Loss: 0.554385 Valid Acc: 0.500\n",
            "Época: 195/299 Train Loss: 0.000000 Valid Loss: 0.554391 Valid Acc: 0.500\n",
            "Época: 196/299 Train Loss: 0.000000 Valid Loss: 0.554397 Valid Acc: 0.500\n",
            "Época: 197/299 Train Loss: 0.000000 Valid Loss: 0.554404 Valid Acc: 0.500\n",
            "Época: 198/299 Train Loss: 0.000000 Valid Loss: 0.554410 Valid Acc: 0.500\n",
            "Época: 199/299 Train Loss: 0.000000 Valid Loss: 0.554416 Valid Acc: 0.500\n",
            "Época: 200/299 Train Loss: 0.000000 Valid Loss: 0.554423 Valid Acc: 0.500\n",
            "Época: 201/299 Train Loss: 0.000000 Valid Loss: 0.554429 Valid Acc: 0.500\n",
            "Época: 202/299 Train Loss: 0.000000 Valid Loss: 0.554436 Valid Acc: 0.500\n",
            "Época: 203/299 Train Loss: 0.000000 Valid Loss: 0.554443 Valid Acc: 0.500\n",
            "Época: 204/299 Train Loss: 0.000000 Valid Loss: 0.554450 Valid Acc: 0.500\n",
            "Época: 205/299 Train Loss: 0.000000 Valid Loss: 0.554457 Valid Acc: 0.500\n",
            "Época: 206/299 Train Loss: 0.000000 Valid Loss: 0.554464 Valid Acc: 0.500\n",
            "Época: 207/299 Train Loss: 0.000000 Valid Loss: 0.554471 Valid Acc: 0.500\n",
            "Época: 208/299 Train Loss: 0.000000 Valid Loss: 0.554478 Valid Acc: 0.500\n",
            "Época: 209/299 Train Loss: 0.000000 Valid Loss: 0.554486 Valid Acc: 0.500\n",
            "Época: 210/299 Train Loss: 0.000000 Valid Loss: 0.554493 Valid Acc: 0.500\n",
            "Época: 211/299 Train Loss: 0.000000 Valid Loss: 0.554501 Valid Acc: 0.500\n",
            "Época: 212/299 Train Loss: 0.000000 Valid Loss: 0.554509 Valid Acc: 0.500\n",
            "Época: 213/299 Train Loss: 0.000000 Valid Loss: 0.554517 Valid Acc: 0.500\n",
            "Época: 214/299 Train Loss: 0.000000 Valid Loss: 0.554525 Valid Acc: 0.500\n",
            "Época: 215/299 Train Loss: 0.000000 Valid Loss: 0.554533 Valid Acc: 0.500\n",
            "Época: 216/299 Train Loss: 0.000000 Valid Loss: 0.554541 Valid Acc: 0.500\n",
            "Época: 217/299 Train Loss: 0.000000 Valid Loss: 0.554550 Valid Acc: 0.500\n",
            "Época: 218/299 Train Loss: 0.000000 Valid Loss: 0.554558 Valid Acc: 0.500\n",
            "Época: 219/299 Train Loss: 0.000000 Valid Loss: 0.554567 Valid Acc: 0.500\n",
            "Época: 220/299 Train Loss: 0.000000 Valid Loss: 0.554575 Valid Acc: 0.500\n",
            "Época: 221/299 Train Loss: 0.000000 Valid Loss: 0.554584 Valid Acc: 0.500\n",
            "Época: 222/299 Train Loss: 0.000000 Valid Loss: 0.554593 Valid Acc: 0.500\n",
            "Época: 223/299 Train Loss: 0.000000 Valid Loss: 0.554602 Valid Acc: 0.500\n",
            "Época: 224/299 Train Loss: 0.000000 Valid Loss: 0.554612 Valid Acc: 0.500\n",
            "Época: 225/299 Train Loss: 0.000000 Valid Loss: 0.554621 Valid Acc: 0.500\n",
            "Época: 226/299 Train Loss: 0.000000 Valid Loss: 0.554630 Valid Acc: 0.500\n",
            "Época: 227/299 Train Loss: 0.000000 Valid Loss: 0.554640 Valid Acc: 0.500\n",
            "Época: 228/299 Train Loss: 0.000000 Valid Loss: 0.554650 Valid Acc: 0.500\n",
            "Época: 229/299 Train Loss: 0.000000 Valid Loss: 0.554659 Valid Acc: 0.500\n",
            "Época: 230/299 Train Loss: 0.000000 Valid Loss: 0.554669 Valid Acc: 0.500\n",
            "Época: 231/299 Train Loss: 0.000000 Valid Loss: 0.554679 Valid Acc: 0.500\n",
            "Época: 232/299 Train Loss: 0.000000 Valid Loss: 0.554690 Valid Acc: 0.500\n",
            "Época: 233/299 Train Loss: 0.000000 Valid Loss: 0.554700 Valid Acc: 0.500\n",
            "Época: 234/299 Train Loss: 0.000000 Valid Loss: 0.554710 Valid Acc: 0.500\n",
            "Época: 235/299 Train Loss: 0.000000 Valid Loss: 0.554721 Valid Acc: 0.500\n",
            "Época: 236/299 Train Loss: 0.000000 Valid Loss: 0.554731 Valid Acc: 0.500\n",
            "Época: 237/299 Train Loss: 0.000000 Valid Loss: 0.554742 Valid Acc: 0.500\n",
            "Época: 238/299 Train Loss: 0.000000 Valid Loss: 0.554753 Valid Acc: 0.500\n",
            "Época: 239/299 Train Loss: 0.000000 Valid Loss: 0.554764 Valid Acc: 0.500\n",
            "Época: 240/299 Train Loss: 0.000000 Valid Loss: 0.554775 Valid Acc: 0.500\n",
            "Época: 241/299 Train Loss: 0.000000 Valid Loss: 0.554787 Valid Acc: 0.500\n",
            "Época: 242/299 Train Loss: 0.000000 Valid Loss: 0.554798 Valid Acc: 0.500\n",
            "Época: 243/299 Train Loss: 0.000000 Valid Loss: 0.554809 Valid Acc: 0.500\n",
            "Época: 244/299 Train Loss: 0.000000 Valid Loss: 0.554821 Valid Acc: 0.500\n",
            "Época: 245/299 Train Loss: 0.000000 Valid Loss: 0.554833 Valid Acc: 0.500\n",
            "Época: 246/299 Train Loss: 0.000000 Valid Loss: 0.554845 Valid Acc: 0.500\n",
            "Época: 247/299 Train Loss: 0.000000 Valid Loss: 0.554857 Valid Acc: 0.500\n",
            "Época: 248/299 Train Loss: 0.000000 Valid Loss: 0.554869 Valid Acc: 0.500\n",
            "Época: 249/299 Train Loss: 0.000000 Valid Loss: 0.554881 Valid Acc: 0.500\n",
            "Época: 250/299 Train Loss: 0.000000 Valid Loss: 0.554893 Valid Acc: 0.500\n",
            "Época: 251/299 Train Loss: 0.000000 Valid Loss: 0.554906 Valid Acc: 0.500\n",
            "Época: 252/299 Train Loss: 0.000000 Valid Loss: 0.554918 Valid Acc: 0.500\n",
            "Época: 253/299 Train Loss: 0.000000 Valid Loss: 0.554931 Valid Acc: 0.500\n",
            "Época: 254/299 Train Loss: 0.000000 Valid Loss: 0.554944 Valid Acc: 0.500\n",
            "Época: 255/299 Train Loss: 0.000000 Valid Loss: 0.554956 Valid Acc: 0.500\n",
            "Época: 256/299 Train Loss: 0.000000 Valid Loss: 0.554969 Valid Acc: 0.500\n",
            "Época: 257/299 Train Loss: 0.000000 Valid Loss: 0.554982 Valid Acc: 0.500\n",
            "Época: 258/299 Train Loss: 0.000000 Valid Loss: 0.554996 Valid Acc: 0.500\n",
            "Época: 259/299 Train Loss: 0.000000 Valid Loss: 0.555009 Valid Acc: 0.500\n",
            "Época: 260/299 Train Loss: 0.000000 Valid Loss: 0.555022 Valid Acc: 0.500\n",
            "Época: 261/299 Train Loss: 0.000000 Valid Loss: 0.555036 Valid Acc: 0.500\n",
            "Época: 262/299 Train Loss: 0.000000 Valid Loss: 0.555049 Valid Acc: 0.500\n",
            "Época: 263/299 Train Loss: 0.000000 Valid Loss: 0.555063 Valid Acc: 0.500\n",
            "Época: 264/299 Train Loss: 0.000000 Valid Loss: 0.555077 Valid Acc: 0.500\n",
            "Época: 265/299 Train Loss: 0.000000 Valid Loss: 0.555091 Valid Acc: 0.500\n",
            "Época: 266/299 Train Loss: 0.000000 Valid Loss: 0.555105 Valid Acc: 0.500\n",
            "Época: 267/299 Train Loss: 0.000000 Valid Loss: 0.555119 Valid Acc: 0.500\n",
            "Época: 268/299 Train Loss: 0.000000 Valid Loss: 0.555133 Valid Acc: 0.500\n",
            "Época: 269/299 Train Loss: 0.000000 Valid Loss: 0.555147 Valid Acc: 0.500\n",
            "Época: 270/299 Train Loss: 0.000000 Valid Loss: 0.555161 Valid Acc: 0.500\n",
            "Época: 271/299 Train Loss: 0.000000 Valid Loss: 0.555176 Valid Acc: 0.500\n",
            "Época: 272/299 Train Loss: 0.000000 Valid Loss: 0.555190 Valid Acc: 0.500\n",
            "Época: 273/299 Train Loss: 0.000000 Valid Loss: 0.555205 Valid Acc: 0.500\n",
            "Época: 274/299 Train Loss: 0.000000 Valid Loss: 0.555220 Valid Acc: 0.500\n",
            "Época: 275/299 Train Loss: 0.000000 Valid Loss: 0.555234 Valid Acc: 0.500\n",
            "Época: 276/299 Train Loss: 0.000000 Valid Loss: 0.555249 Valid Acc: 0.500\n",
            "Época: 277/299 Train Loss: 0.000000 Valid Loss: 0.555264 Valid Acc: 0.500\n",
            "Época: 278/299 Train Loss: 0.000000 Valid Loss: 0.555279 Valid Acc: 0.500\n",
            "Época: 279/299 Train Loss: 0.000000 Valid Loss: 0.555294 Valid Acc: 0.500\n",
            "Época: 280/299 Train Loss: 0.000000 Valid Loss: 0.555309 Valid Acc: 0.500\n",
            "Época: 281/299 Train Loss: 0.000000 Valid Loss: 0.555324 Valid Acc: 0.500\n",
            "Época: 282/299 Train Loss: 0.000000 Valid Loss: 0.555340 Valid Acc: 0.500\n",
            "Época: 283/299 Train Loss: 0.000000 Valid Loss: 0.555355 Valid Acc: 0.500\n",
            "Época: 284/299 Train Loss: 0.000000 Valid Loss: 0.555370 Valid Acc: 0.500\n",
            "Época: 285/299 Train Loss: 0.000000 Valid Loss: 0.555386 Valid Acc: 0.500\n",
            "Época: 286/299 Train Loss: 0.000000 Valid Loss: 0.555401 Valid Acc: 0.500\n",
            "Época: 287/299 Train Loss: 0.000000 Valid Loss: 0.555417 Valid Acc: 0.500\n",
            "Época: 288/299 Train Loss: 0.000000 Valid Loss: 0.555433 Valid Acc: 0.500\n",
            "Época: 289/299 Train Loss: 0.000000 Valid Loss: 0.555448 Valid Acc: 0.500\n",
            "Época: 290/299 Train Loss: 0.000000 Valid Loss: 0.555464 Valid Acc: 0.500\n",
            "Época: 291/299 Train Loss: 0.000000 Valid Loss: 0.555480 Valid Acc: 0.500\n",
            "Época: 292/299 Train Loss: 0.000000 Valid Loss: 0.555496 Valid Acc: 0.500\n",
            "Época: 293/299 Train Loss: 0.000000 Valid Loss: 0.555512 Valid Acc: 0.500\n",
            "Época: 294/299 Train Loss: 0.000000 Valid Loss: 0.555528 Valid Acc: 0.500\n",
            "Época: 295/299 Train Loss: 0.000000 Valid Loss: 0.555544 Valid Acc: 0.500\n",
            "Época: 296/299 Train Loss: 0.000000 Valid Loss: 0.555560 Valid Acc: 0.500\n",
            "Época: 297/299 Train Loss: 0.000000 Valid Loss: 0.555576 Valid Acc: 0.500\n",
            "Época: 298/299 Train Loss: 0.000000 Valid Loss: 0.555592 Valid Acc: 0.500\n",
            "Época: 299/299 Train Loss: 0.000000 Valid Loss: 0.555608 Valid Acc: 0.500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BglT68Vpdwj-",
        "outputId": "0c482125-4166-4347-c713-dea8738746cb"
      },
      "source": [
        "accumulated_accuracy = 0\n",
        "cls.eval()\n",
        "with torch.no_grad():\n",
        "    for x_test_, y_test_ in dataloader_train:\n",
        "        x_test_ = x_test_.to(device)\n",
        "        y_test_ = y_test_.to(device).reshape(-1,1)\n",
        "\n",
        "        # predict da rede\n",
        "        outputs = cls(x_test_)\n",
        "\n",
        "        # calcula a perda\n",
        "        batch_loss = criterion(outputs, y_test_)\n",
        "        preds = outputs > 0.5\n",
        "        # preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y_test_).sum()\n",
        "        accumulated_accuracy += batch_accuracy\n",
        "\n",
        "test_acc = accumulated_accuracy / len(dataloader_train.dataset)\n",
        "test_acc *= 100\n",
        "print('*' * 40)\n",
        "print(f'Acurácia de {test_acc:.3f} %')\n",
        "print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 100.000 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrlY7L-W7etN"
      },
      "source": [
        "## Experimento final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb2XDD7z7h_4",
        "outputId": "3c1f1374-7ab6-4e5d-cadc-17ffc0e2fb16"
      },
      "source": [
        "run = neptune.init(\n",
        "    project=\"pedro.gengo/IA-376\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZjYyNDA1MS1hZDJlLTRiZDctYjIxNy0xMTNhY2FmNzZhYmIifQ==\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/pedro.gengo/IA-376/e/IA-23\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWNK9LjrQbi3"
      },
      "source": [
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "hidden_size = 150\n",
        "filename = \"self_attention\"\n",
        "\n",
        "hprams = {\"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"hidden_size\": hidden_size\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0h1668xPl-k"
      },
      "source": [
        "dataset_train = Dataset(x_train, y_train, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_valid = Dataset(x_valid, y_valid, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_test = Dataset(x_test, y_test, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=hprams[\"batch_size\"], shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=hprams[\"batch_size\"], shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=hprams[\"batch_size\"], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH1ZBV4dQlm-",
        "outputId": "6eff0319-3f17-4698-904b-cc083ef2a86d"
      },
      "source": [
        "cls = Classifier(embeddings, params['padding_idx'], params['n_heads'], params['dim'], params['max_length'], hprams[\"hidden_size\"])\n",
        "cls.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (self_attention): SelfAttentionLayer(\n",
              "    (embedding_layer): Embedding(400002, 300, padding_idx=400001)\n",
              "    (positional_embeddings): Linear(in_features=300, out_features=200, bias=False)\n",
              "    (W_q): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_k): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_v): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (W_o): Linear(in_features=300, out_features=300, bias=False)\n",
              "    (layer_norm1): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
              "    (feed_forward): Sequential(\n",
              "      (0): Linear(in_features=300, out_features=300, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=300, out_features=300, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((300,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (lin1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
              "  (output): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFBBrB_ZREp4"
      },
      "source": [
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(cls.parameters(), lr=hprams[\"learning_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Ie61iKRgOG",
        "outputId": "be7efb8b-9a5e-4d03-f91d-af57ca478d60"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(cls, dataloader_train, dataloader_valid, criterion,\n",
        "          optimizer, filename, n_epochs=n_epochs, run=run, params=hprams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/19 Train Loss: 0.008201 Valid Loss: 0.007379 Valid Acc: 0.838\n",
            "best model\n",
            "Época: 1/19 Train Loss: 0.007012 Valid Loss: 0.007278 Valid Acc: 0.837\n",
            "best model\n",
            "Época: 2/19 Train Loss: 0.006454 Valid Loss: 0.007979 Valid Acc: 0.824\n",
            "Época: 3/19 Train Loss: 0.006120 Valid Loss: 0.008190 Valid Acc: 0.821\n",
            "Época: 4/19 Train Loss: 0.005526 Valid Loss: 0.007172 Valid Acc: 0.843\n",
            "best model\n",
            "Época: 5/19 Train Loss: 0.004933 Valid Loss: 0.007573 Valid Acc: 0.838\n",
            "Época: 6/19 Train Loss: 0.004274 Valid Loss: 0.008480 Valid Acc: 0.833\n",
            "Época: 7/19 Train Loss: 0.003635 Valid Loss: 0.008671 Valid Acc: 0.847\n",
            "Época: 8/19 Train Loss: 0.003008 Valid Loss: 0.009078 Valid Acc: 0.843\n",
            "Época: 9/19 Train Loss: 0.002343 Valid Loss: 0.009892 Valid Acc: 0.842\n",
            "Época: 10/19 Train Loss: 0.001968 Valid Loss: 0.011372 Valid Acc: 0.844\n",
            "Época: 11/19 Train Loss: 0.001671 Valid Loss: 0.011905 Valid Acc: 0.833\n",
            "Época: 12/19 Train Loss: 0.001233 Valid Loss: 0.012317 Valid Acc: 0.835\n",
            "Época: 13/19 Train Loss: 0.001123 Valid Loss: 0.014000 Valid Acc: 0.837\n",
            "Época: 14/19 Train Loss: 0.001079 Valid Loss: 0.014297 Valid Acc: 0.834\n",
            "Época: 15/19 Train Loss: 0.000917 Valid Loss: 0.016055 Valid Acc: 0.838\n",
            "Época: 16/19 Train Loss: 0.000631 Valid Loss: 0.019718 Valid Acc: 0.834\n",
            "Época: 17/19 Train Loss: 0.000898 Valid Loss: 0.015481 Valid Acc: 0.836\n",
            "Época: 18/19 Train Loss: 0.000669 Valid Loss: 0.016134 Valid Acc: 0.832\n",
            "Época: 19/19 Train Loss: 0.000747 Valid Loss: 0.017485 Valid Acc: 0.835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWgeAfa08ZeX",
        "outputId": "d9c1cb9c-c2c2-4f4c-d731-96d7f07780ab"
      },
      "source": [
        "predict(cls, filename, dataloader_test, run=run)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 84.760 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojByP_nCplE6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}