{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 5 - Exercício - Pedro Gengo",
      "provenance": [],
      "collapsed_sections": [
        "1OG5DT_dm6mk",
        "uhpAkifICdJo",
        "qQ5xJZ6vT4BJ",
        "jT73F9s-VMtQ",
        "E_sFzr58XktL",
        "iZUx0BwYYe1h",
        "NgaquKdFcCZu"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Aula_5_Exerc%C3%ADcio_Pedro_Gengo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNjTRrR5TwSi",
        "outputId": "cb461ae0-9622-43d9-8f29-17b769b7a8a5"
      },
      "source": [
        "!pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.10.10.tar.gz (268 kB)\n",
            "\u001b[K     |████████████████████████████████| 268 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 46.6 MB/s \n",
            "\u001b[?25hCollecting boto3==1.18.14\n",
            "  Downloading boto3-1.18.14-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.14\n",
            "  Downloading botocore-1.21.42-py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 66.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.14->boto3==1.18.14->neptune-client) (2.8.2)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 51.1 MB/s \n",
            "\u001b[?25hCollecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.10.10-py2.py3-none-any.whl size=464917 sha256=dc683de15272de2216de759ae42ff6d579dc3571dca2695245a8f841f050835c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/41/d8/7b2abc93762861e492f7fd68b7769b2b130501b46e7775b5b6\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=6ada0e2b966bb787f4f67ccdc20a5d8915418fd473e698170531413ea3c905b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=b42c6404e3378de247368b0551fc0ad89ce77448fe0e4544e116bf4ccdeaa74e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, urllib3, strict-rfc3339, rfc3987, jmespath, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.18 PyJWT-2.1.0 boto3-1.18.14 botocore-1.21.42 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.7 jmespath-0.10.0 jsonref-0.2 monotonic-1.6 neptune-client-0.10.10 rfc3987-1.3.8 s3transfer-0.5.0 simplejson-3.17.5 smmap-4.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import OrderedDict\n",
        "\n",
        "import neptune.new as neptune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHeZ9nAOEB0U"
      },
      "source": [
        "def set_seed():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "  torch.cuda.manual_seed(123)\n",
        "set_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea99853-679a-43fb-f6c5-e202c8ac5332"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-15 14:47:35--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 172.67.69.159, 104.26.2.19, 104.26.3.19, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|172.67.69.159|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/aclImdb.tgz [following]\n",
            "--2021-09-15 14:47:35--  https://files.fast.ai/data/aclImdb.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|172.67.69.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [application/x-gtar-compressed]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  63.5MB/s    in 2.2s    \n",
            "\n",
            "2021-09-15 14:47:37 (63.5 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a626218-8450-462e-8b05-749d93177c47"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "y_valid = y_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "y_train = y_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False This film is terrible. I was really looking forward to it, as I thought \"Lantana\" was great.<br /><b\n",
            "False I live in Ottawa where this film was made and I really wish it hadn't been. This is one God-awful fl\n",
            "True This is an excellent, fast paced thriller by Wes Craven (Nightmare on Elm Street), who for 85 minute\n",
            "3 últimas amostras treino:\n",
            "False The Good:<br /><br />Effective color scheme. Good costumes. Top notch set production. Well detailed \n",
            "True Gregory Peck gives a brilliant performance in this film. The last 15 minutes (or thereabouts) are gr\n",
            "True Michael Caton-Jones's Scottish period piece bears little connection to the Sir Walter Scott novel of\n",
            "3 primeiras amostras validação:\n",
            "True Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't t\n",
            "True To get in touch with the beauty of this film pay close attention to the sound track, not only the mu\n",
            "True Vijay Krishna Acharya's 'Tashan' is a over-hyped, stylized, product. Sure its a one of the most styl\n",
            "3 últimas amostras validação:\n",
            "True I will always have a soft spot for this Disney flick, another of their part live action/part animati\n",
            "True This couldn't have been better. The strong restraints on Mike Sullivan's expressions couldn't have b\n",
            "True \"The sweet is never as sweet without the sour.\" This quote was essentially the theme for the movie i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiLnZh8fKXvm",
        "outputId": "eeed9ed9-77a2-43e2-a551-efaac9739fd3"
      },
      "source": [
        "sum([len(item.split()) for item in x_train])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4674365"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ5xJZ6vT4BJ"
      },
      "source": [
        "# Tokenização\n",
        "\n",
        "Nessa etapa, o objetivo é pegar os textos de treino, validação e teste e tokenizá-los, ou seja, receber um _string_ e transformá-la em uma lista de tokens/palavras.\n",
        "\n",
        "Para garantir o uso de sequências de mesmo tamanho, também aplicaremos um filtro para termos no máximo 200 tokens em cada texto tokenizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rbclhFdh5SL"
      },
      "source": [
        "from re import findall\n",
        "\n",
        "def tokenizer(texts, size):\n",
        "  tokenized_texts = []\n",
        "  for text in texts:\n",
        "    tokens = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n",
        "    tokenized = [token.lower() for token in tokens][:size]\n",
        "    tokenized_texts.append(tokenized)\n",
        "  return tokenized_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLkG_Qx1ia33"
      },
      "source": [
        "SENTENCE_SIZE = 200\n",
        "tokenized_x_train = tokenizer(x_train, SENTENCE_SIZE)\n",
        "tokenized_x_valid = tokenizer(x_valid, SENTENCE_SIZE)\n",
        "tokenized_x_test = tokenizer(x_test, SENTENCE_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7t8PjLlUhos"
      },
      "source": [
        "# Criação do vocabulário\n",
        "\n",
        "Para garantir que tenhamos o mesmo vocabulário tanto para o experimento com Bag of Words quanto para o experimento com Embeddings, e com o intuito de evitar o reprocessamento, criamos um dataset que será utilizado em todos os experimentos.\n",
        "\n",
        "Para isso, recebemos todos os textos tokenizados de treino e aplicamos o Counter em cima deles. Ao final, percorremos todos os elementos desse vocabulário associando cada token com sua posição baseada na quantidade de vezes que apareceu, gerando assim o índice que cada token terá."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwm7NZiWilso"
      },
      "source": [
        "from collections import Counter\n",
        "def create_vocab(tokenized_texts, max_size=None):\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(text)\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(max_size))}\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x-4iQr2i6tE"
      },
      "source": [
        "VOCAB = create_vocab(tokenized_x_train)\n",
        "VOCAB['__UNK__'] = len(VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT73F9s-VMtQ"
      },
      "source": [
        "# Vetorizadores\n",
        "\n",
        "Abaixo temos a definição da classe BagOfWords e EmbeddingVectorizer. Ambas serão utilizadas para aplicar a transformação de um dado, seja de treino ou não, em tempo de execução do treino.\n",
        "\n",
        "No método _.transform()_ da classe BagOfWords, recebemos uma lista de textos tokenizados, percorremos cada um deles criando um novo vetor, do tamanho do vocabulário, preenchido por zeros. Aplicamos então um Counter nesse texto tokenizado e substituimos no vetor de zeros, nos índices correspondentes a cada uma das palavras encontrar no texto, pela contagem do token.\n",
        "\n",
        "Já o método _.transform()_ , passamos também um conjunto de textos tokenizados e, para cada texto, convertemos cada token existente em um índice, utilizando o dicionário. Após isso, verificamos se o tamanho do vetor gerado é menor que 200 e, caso seja, inserimos tokens PAD (tamanho do vocab) até completar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kyakbZ0jaWP"
      },
      "source": [
        "class BagOfWords():\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def fit(self):\n",
        "    pass\n",
        "  \n",
        "  def transform(self, texts):\n",
        "    transformed_texts = []\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = torch.zeros(len(self.vocab))\n",
        "      unk = self.vocab['__UNK__']\n",
        "      counter = Counter(text)\n",
        "      for key, value in counter.items():\n",
        "        # if key in self.vocab:\n",
        "        bow_text[self.vocab.get(key, unk)] += value\n",
        "      transformed_texts.append(bow_text)\n",
        "    return torch.vstack(transformed_texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44RQ2OADki1R"
      },
      "source": [
        "class EmbeddingVectorizer():\n",
        "\n",
        "  def __init__(self, vocab, sequence_size=200):\n",
        "    self.vocab = vocab\n",
        "    self.sequence_size = sequence_size\n",
        "  \n",
        "  def fit(self):\n",
        "    pass\n",
        "  \n",
        "  def transform(self, texts):\n",
        "    transformed_texts = []\n",
        "    for i, text in enumerate(texts):\n",
        "      unk = self.vocab['__UNK__']\n",
        "      emb_text = []\n",
        "      for key in text:\n",
        "        # if key in self.vocab:\n",
        "        emb_text.append(self.vocab.get(key, unk))\n",
        "      if len(emb_text) < 200:\n",
        "        emb_text = emb_text + [len(self.vocab)] * (self.sequence_size - len(emb_text))\n",
        "      transformed_texts.append(torch.LongTensor(emb_text))\n",
        "    return torch.vstack(transformed_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_sFzr58XktL"
      },
      "source": [
        "# Dataset e Dataloader\n",
        "\n",
        "Na classe Dataset, a diferença é a aplicação do método _.transform()_ dentro do método *.\\_\\_getitem\\_\\_*. Com isso, evitamos sobrecarregar a memória com a base de dados inteira vetorizada e fazemos isso em tempo de execução do treinamento.\n",
        "\n",
        "Já no DataLoader, englobei ele em uma função que gera as entradas da rede neural. Essa função monta cada um dos Datasets e depois monta os DataLoaders e os retorna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUc1rr5nlfnC"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, x, y, vectorizer=None):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.vectorizer = vectorizer\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.vectorizer.transform([self.x[idx]])[0], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1BdMwbVm8iX"
      },
      "source": [
        "def create_nn_input(tokenized_text_train, y_train,\n",
        "                    tokenized_text_valid, y_valid,\n",
        "                    tokenized_text_test, y_test,\n",
        "                    vectorizer, batch_size):\n",
        "  \n",
        "\n",
        "  train_dataset = Dataset(tokenized_text_train, torch.Tensor(y_train).reshape(-1,1), vectorizer)\n",
        "  valid_dataset = Dataset(tokenized_text_valid, torch.Tensor(y_valid).reshape(-1,1), vectorizer)\n",
        "  test_dataset = Dataset(tokenized_text_test, torch.Tensor(y_test).reshape(-1,1), vectorizer)\n",
        "\n",
        "  vectorized_texts_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "  vectorized_texts_valid = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "  vectorized_texts_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return vectorized_texts_train, vectorized_texts_valid, vectorized_texts_test, vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZUx0BwYYe1h"
      },
      "source": [
        "# Loop de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKoc3wsJak1L",
        "outputId": "31b22bad-5a7c-4a28-a477-579dd72d45d5"
      },
      "source": [
        "run = neptune.init(\n",
        "    project=\"pedro.gengo/IA-376\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZjYyNDA1MS1hZDJlLTRiZDctYjIxNy0xMTNhY2FmNzZhYmIifQ==\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Info (NVML): Driver Not Loaded. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/pedro.gengo/IA-376/e/IA-4\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClD4tNBxnZbI",
        "outputId": "51381153-78d8-40cf-8217-8109259bc6c2"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZkrBql1nfue"
      },
      "source": [
        "def train(model, train, valid, criterion, optimizer, filename_save, n_epochs=10):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "  train_losses, valid_losses = [], []\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device)\n",
        "      outputs = model(x_train)\n",
        "      batch_loss = criterion(outputs, y_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    run[f\"{filename_save}_train/loss\"].log(train_loss)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(x_valid)\n",
        "\n",
        "            # calcula a perda\n",
        "            batch_loss = criterion(outputs, y_valid)\n",
        "            preds = outputs > 0.5\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    run[f\"{filename_save}_valid/loss\"].log(valid_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "    run[f\"{filename_save}_valid/acc\"].log(valid_acc)\n",
        "\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), filename_save + '.pt')\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qt5RCvG67ia"
      },
      "source": [
        "def predict(model, state_dict, test):\n",
        "  accumulated_accuracy = 0\n",
        "  model.load_state_dict(torch.load(state_dict + '.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for x_test, y_test in test:\n",
        "          x_test = x_test.to(device)\n",
        "          y_test = y_test.to(device)\n",
        "\n",
        "          # predict da rede\n",
        "          outputs = model(x_test)\n",
        "\n",
        "          # calcula a perda\n",
        "          batch_loss = criterion(outputs, y_test)\n",
        "          preds = outputs > 0.5\n",
        "\n",
        "          # calcula a acurácia\n",
        "          batch_accuracy = (preds == y_test).sum()\n",
        "          accumulated_accuracy += batch_accuracy\n",
        "\n",
        "  test_acc = accumulated_accuracy / len(test.dataset)\n",
        "  test_acc *= 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {test_acc:.3f} %')\n",
        "  print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgaquKdFcCZu"
      },
      "source": [
        "# Modelos\n",
        "\n",
        "Aqui, definos as duas classes de modelos: Modelo baseado em camadas lineares, que usará as representações Bag of Words e o modelo baseado na camada Embedding que usará a representação como uma sequência de índices. Um detalhe muito importante é que, para funcionar de maneira igual, é necessário realizar uma soma após a camada de embedding e, além disso, iniciar as camadas com os mesmos pesos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJCIIwqEnjGr"
      },
      "source": [
        "class LinearModel(torch.nn.Module):\n",
        "  def __init__(self, input_size, emb_size):\n",
        "    super(LinearModel, self).__init__()\n",
        "    self.dense = torch.nn.Sequential(\n",
        "        torch.nn.Linear(input_size, emb_size, bias = False),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(50,1, bias = False),\n",
        "        torch.nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.dense(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkzR1AhbntZU"
      },
      "source": [
        "class EmbModel(torch.nn.Module):\n",
        "  def __init__(self, n_classes, emb_size, padding_idx):\n",
        "    super(EmbModel, self).__init__()\n",
        "    self.emb_layer = torch.nn.Embedding(n_classes, emb_size, padding_idx = padding_idx)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.linear = torch.nn.Linear(emb_size, 1, bias=False)\n",
        "    self.out = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.emb_layer(x)\n",
        "    x = torch.sum(x,dim = 1)\n",
        "    x = self.relu(x)\n",
        "    x = self.linear(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yippJ2HwcpKo"
      },
      "source": [
        "# Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDi60lW3crSJ"
      },
      "source": [
        "## Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv_uGBeaoBpv"
      },
      "source": [
        "max_size = None\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.01\n",
        "emb_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydahiP9wcv5i"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsefg0gfoF3N",
        "outputId": "bd81e6cc-06a4-4b0a-94ac-7e1ec59d6bb8"
      },
      "source": [
        "set_seed()\n",
        "save_filename = 'bow'\n",
        "vectorizer = BagOfWords(vocab=VOCAB)\n",
        "\n",
        "vectorized_train_bow, vectorized_valid_bow, vectorized_test_bow, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size)\n",
        "bow_model = LinearModel(len(VOCAB), emb_size)\n",
        "initial_weights = [bow_model.dense[0].weight.clone(),\n",
        "                   bow_model.dense[2].weight.clone()]\n",
        "\n",
        "bow_model.to(device)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(bow_model.parameters(), lr=learningRate)\n",
        "set_seed()\n",
        "_, train_losses_bow, valid_losses_bow = train(bow_model, vectorized_train_bow, vectorized_valid_bow, criterion,\n",
        "          optimizer, save_filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/9 Train Loss: 0.010182 Valid Loss: 0.009688 Valid Acc: 0.695\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.008973 Valid Loss: 0.008618 Valid Acc: 0.746\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.007986 Valid Loss: 0.007795 Valid Acc: 0.786\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.007250 Valid Loss: 0.007226 Valid Acc: 0.806\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.006721 Valid Loss: 0.006831 Valid Acc: 0.814\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.006325 Valid Loss: 0.006543 Valid Acc: 0.822\n",
            "best model\n",
            "Época: 6/9 Train Loss: 0.006011 Valid Loss: 0.006318 Valid Acc: 0.831\n",
            "best model\n",
            "Época: 7/9 Train Loss: 0.005749 Valid Loss: 0.006137 Valid Acc: 0.839\n",
            "best model\n",
            "Época: 8/9 Train Loss: 0.005526 Valid Loss: 0.005987 Valid Acc: 0.843\n",
            "best model\n",
            "Época: 9/9 Train Loss: 0.005331 Valid Loss: 0.005861 Valid Acc: 0.848\n",
            "best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgldUgM_7CTG",
        "outputId": "aee2cb93-4c3d-45f2-e64c-a99f1633e335"
      },
      "source": [
        "predict(bow_model, save_filename, vectorized_test_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 83.384 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAOzaG5-cy_J"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu29sxxRnufj",
        "outputId": "f96825bb-1f5f-4b47-e456-de541a536d07"
      },
      "source": [
        "set_seed()\n",
        "save_filename = 'emb'\n",
        "vectorizer = EmbeddingVectorizer(vocab=VOCAB)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size)\n",
        "\n",
        "emb_init_weights = OrderedDict([\n",
        "                                ('emb_layer.weight', torch.cat((initial_weights[0].T, torch.zeros(1, emb_size)))),\n",
        "                                ('linear.weight', initial_weights[1])\n",
        "                                ])\n",
        "emb_model = EmbModel(len(VOCAB)+1, emb_size, padding_idx = len(VOCAB))\n",
        "emb_model.load_state_dict(emb_init_weights)\n",
        "emb_model.to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(emb_model.parameters(), lr=learningRate)\n",
        "set_seed()\n",
        "_, train_losses_emb, valid_losses_emb = train(emb_model, vectorized_train, vectorized_valid, criterion,\n",
        "          optimizer, save_filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/9 Train Loss: 0.010182 Valid Loss: 0.009688 Valid Acc: 0.695\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.008973 Valid Loss: 0.008618 Valid Acc: 0.746\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.007986 Valid Loss: 0.007795 Valid Acc: 0.786\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.007250 Valid Loss: 0.007226 Valid Acc: 0.806\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.006721 Valid Loss: 0.006831 Valid Acc: 0.814\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.006325 Valid Loss: 0.006543 Valid Acc: 0.822\n",
            "best model\n",
            "Época: 6/9 Train Loss: 0.006011 Valid Loss: 0.006318 Valid Acc: 0.831\n",
            "best model\n",
            "Época: 7/9 Train Loss: 0.005749 Valid Loss: 0.006137 Valid Acc: 0.839\n",
            "best model\n",
            "Época: 8/9 Train Loss: 0.005526 Valid Loss: 0.005987 Valid Acc: 0.843\n",
            "best model\n",
            "Época: 9/9 Train Loss: 0.005331 Valid Loss: 0.005861 Valid Acc: 0.848\n",
            "best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_SAholp7JVl",
        "outputId": "a37519e4-305c-41ed-9a76-dd744776b9c0"
      },
      "source": [
        "predict(emb_model, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 83.384 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0jDynY_PN5S",
        "outputId": "5618dad9-4da2-420e-98e7-139895510660"
      },
      "source": [
        "run.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Waiting for the remaining 7 operations to synchronize with Neptune. Do not kill this process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 7 operations synced, thanks for waiting!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9GeTUzkg835"
      },
      "source": [
        "## Comparação dos resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l6u9wSRhA2s"
      },
      "source": [
        "assert torch.allclose(torch.tensor(train_losses_emb), torch.tensor(train_losses_bow))\n",
        "assert torch.allclose(torch.tensor(valid_losses_emb), torch.tensor(valid_losses_bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VP71Q4jhSxF"
      },
      "source": [
        "# Breve explicação do motivo dos resultados terem sido iguais\n",
        "\n",
        "Para explicar, mostrarei que utilizar um sequência de índices, que são transformados cada um em um vetor e depois somados, é o mesmo que utilizar uma representação Bag of Words baseada em contagem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KncaDlCqh9ne",
        "outputId": "2d4f2742-e65b-476b-fb9c-359796fc771c"
      },
      "source": [
        "vectorized_valid_bow.dataset[0][0], vectorized_valid_bow.dataset[0][0].shape #Primeiro texto, ja tokenizado (bow), do conjunto de validacao"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([8., 7., 2.,  ..., 0., 0., 0.]), torch.Size([57472]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ambYruLAiJUL",
        "outputId": "11e784e4-1871-4424-ae00-9b2ec943c35d"
      },
      "source": [
        "vectorized_valid.dataset[0][0] #Primeiro texto, ja tokenizado (embedding), do conjunto de validacao"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  976,     1,   959,    25,    38,    72,   386,   291,   649,     6,\n",
              "           98,    25,    10,    14,   267,    23,   101,   352,    17,     7,\n",
              "          267,    23,    11,    77,   128,    79,   747,     0,   277,    57,\n",
              "         1873,    16,   556, 10718,  2206,   145,    27,  2533,    56,    24,\n",
              "            0,  4603,   864,     2,   138,    10,    13,   145,    27,    63,\n",
              "         2064,    91,     7,   327,     3, 16215,     0,  2731,    16,   608,\n",
              "           82,  1409,   654,    75,    53,    23,    50,   509,   145,    27,\n",
              "           80,  4022,  4689,    44, 19399,     4,    27,    56,  4090,     4,\n",
              "            0,  5781,     3,     0,   114,   277,   107,   370,   200,     6,\n",
              "           13,  1437,    47,     0,   512,   158,    13,     1,   303,     3,\n",
              "          685,  3638,   339,   295,  9951,    25,    11,     7,    12,   342,\n",
              "            4,    30,     1,   303,     3,    11,    14,    17,     4,    95,\n",
              "          129,    14,    33,   200,   176,    39,    11,     6,   384,   220,\n",
              "           24,   210,   179,     3,    10,    13,    11, 23126, 20091,    65,\n",
              "          140,     1,   388,   179,     6,   384,  1397,   220,    24,    61,\n",
              "           13,     0,    66,    28,     9,    28,    44,   114,   395,    37,\n",
              "          161,   764,     1,  2569,  1234,    20,   206,     1,    51,  1868,\n",
              "            7,    13,    41,    62,   162,    48,   118,    13,    40,  2368,\n",
              "        35211,    21,   296,    47,    61,    13,     0,   322,   420,    96,\n",
              "           27,  1509,     2,    75,    64,    27,    65,     1,    56, 22733])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbs9QZQ4iNcB",
        "outputId": "ce247add-0cc5-4676-a83c-f041f61408e5"
      },
      "source": [
        "from torch.nn import functional\n",
        "\n",
        "onehot = functional.one_hot(vectorized_valid.dataset[0][0], num_classes=len(VOCAB) + 1) # Matriz one-hot\n",
        "onehot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 1, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 1, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjKLD46IiZSO",
        "outputId": "0fc5172a-7b18-4417-96e5-ee7468d8c164"
      },
      "source": [
        "onehot.sum(dim=0), onehot.sum(dim=0).shape # temos um elemento a mais, que eh o padding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([8, 7, 2,  ..., 0, 0, 0]), torch.Size([57473]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZMvn5UPihGA"
      },
      "source": [
        "assert torch.all(onehot.sum(dim=0)[:-1] == vectorized_valid_bow.dataset[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "097DOEuYiwS4"
      },
      "source": [
        "Assim, vemos que ao passar pela camada de embedding e garantindo que temos os mesmos pesos, ao somar a matriz resultando, teremos o mesmo resultado da saída da primeira camada linear do modelo que utilizamos Bag of Words, pois ao somar dois tokens iguais na camada linear, é o mesmo que multiplicar os pesos da camada linear por dois, e somar todos os vetores dos tokens após a camada de embedding, é o mesmo que somar todos os _pesos * contagens_ na camada linear."
      ]
    }
  ]
}