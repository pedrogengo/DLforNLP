{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 4 - Exercício - Pedro Gengo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Aula_4_Exerc%C3%ADcio_Pedro_Gengo_NN_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "## Instruções\n",
        "\n",
        "- Treinar uma rede neural como classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n",
        "\n",
        "- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n",
        "    1) Bag-of-words booleano\n",
        "    2) Bag-of-words com contagem das palavras (histograma das palavras)\n",
        "    3) TF-IDF\n",
        "\n",
        "Deve-se implementar o laço de treinamento e validação da rede neural.\n",
        "\n",
        "Neste exercício usaremos o IMDB com 20l exemplos para treino, 5k para desenvolvimento e 25k para teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e48101-5230-4482-81ca-c329e8988053"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca672284-c2e2-4394-dc40-3ded383ea9f0"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "y_valid = y_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "y_train = y_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False I go to UCSB and take some classes with the executive producer, Alison Anders. She's a superb teache\n",
            "True The barbarians maybe´s not the best film that anybody of us have seen, but really????........It´s so\n",
            "True \"Der Todesking\" is not exactly the type of film that makes you merry Jörg Buttgereit's second cult \n",
            "3 últimas amostras treino:\n",
            "False This is a really dumb movie. It could be fun with the cool looking aliens and the country setting, b\n",
            "True Alex Winter and Keanu Reeves return as the two dopes from San Dimas who get sent on another trip of \n",
            "False In an attempt to bring back the teen slasher genre that was taken away by spoofs like Scary Movie an\n",
            "3 primeiras amostras validação:\n",
            "True Spoilers.<br /><br />First off, nothing really happened in this movie, other than a woman bleeding i\n",
            "True Just a regular Jason lee movie, There were some parts of the movie that were funny. The undertone of\n",
            "True My name is Domino Harvery. {EDIT *dizzying* CHOP} My--my--my name is Domino Harvey. {CUT, CHOP} My n\n",
            "3 últimas amostras validação:\n",
            "True Midnight Cowboy opens with a run down Drive In theater with the voice-over of the main character Joe\n",
            "True \"A Classic is something that everybody wants to have read but nobody wants to read. A classic is als\n",
            "False In 1993, \"the visitors\" was an enormous hit in France. So, the sequence was inevitable and unfortuna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQJ1E_cKkXpT"
      },
      "source": [
        "## Processamento dos textos\n",
        "\n",
        "Nessa etapa, como descrito no enunciado do exercício, iremos realizar três tipos de vetorização:\n",
        "\n",
        "1. BoW booleano\n",
        "2. BoW com base na frequência\n",
        "3. TF-IDF\n",
        "\n",
        "É muito importante ressaltar a importância de aplicar o \"fit\" apenas no treino, ou seja, utilizar apenas o vocabulário do treino, e utilizar o que foi encontrado para o teste/validação.\n",
        "\n",
        "Para isso, usarei a estrutura de fit/transform bastante conhecida da biblioteca sklearn. Começarei, então, criando uma classe abstrata que será herdada na criação das outras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzlNA3zKmWBE"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        " \n",
        "class Transformer(ABC):\n",
        " \n",
        "  @abstractmethod\n",
        "  def fit(self):\n",
        "      pass\n",
        "    \n",
        "  @abstractmethod\n",
        "  def transform(self):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdDJMVUnSK0"
      },
      "source": [
        "## BoW (booleano e com frequência)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjnyDSRsnUW7"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class BagOfWords(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros.\n",
        "\n",
        "  Attrs:\n",
        "    boolean(bool): Flag que define se o vetor gerado sera com base na\n",
        "      frequencia (contagem) ou com base na ocorrencia ou nao (bool) de\n",
        "      uma palavra do vocabulario.\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "    use_unknown(bool): Flag que define o uso ou nao de um elemento para\n",
        "      palavras que nao existem no vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, boolean=False, max_size=None, stopwords = [], use_unknown=False):\n",
        "    self.max_size = max_size\n",
        "    self.boolean = boolean\n",
        "    self.stopwords = stopwords\n",
        "    self.use_unknown = use_unknown\n",
        "\n",
        "  def _create_vocab(self, tokenized_texts):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(text)\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    if self.use_unknown:\n",
        "      vocab['unknown'] = len(vocab)\n",
        "    return vocab\n",
        "  \n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    vocab = self._create_vocab(texts)\n",
        "    self.vocabulary = vocab\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      bow_texts(torch.tensor): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "    if self.use_unknown:\n",
        "      unknown = self.vocabulary.get('unknown')\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = torch.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      if self.use_unknown:\n",
        "        index = [self.vocabulary.get(key, unknown) for key in counter.keys() if key not in self.stopwords]\n",
        "      else:\n",
        "        index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n",
        "\n",
        "      if self.boolean:\n",
        "        bow_text[index] = 1\n",
        "      else:\n",
        "        values = [value for key, value in counter.items() if (self.use_unknown and key not in self.stopwords)\n",
        "                    or key in self.vocabulary.keys()]\n",
        "        bow_text[index] = torch.Tensor(values)\n",
        "\n",
        "      transformed_texts.append(bow_text)\n",
        "\n",
        "    return torch.vstack(transformed_texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTNcy1YhyylV"
      },
      "source": [
        "## Vocabulario deve ser: {'a': 0, 'texttinho': 1, 'testando': 2, 'b': 3, 'c': 4, 'unknown': 5}\n",
        "texts_test = [['text', 'texttinho', 'texttinho', 'testando'], ['a', 'b', 'a', 'a', 'c']]\n",
        "\n",
        "## Testando BoW com frequencia\n",
        "bow = BagOfWords(boolean=False, stopwords=['text'], use_unknown=True)\n",
        "bow.fit(texts_test)\n",
        "assert torch.all(bow.transform(texts_test) == torch.Tensor([[0., 2., 1., 0., 0., 0.], [3., 0., 0., 1., 1., 0.]]))\n",
        "\n",
        "## Testando BoW booleano\n",
        "bow = BagOfWords(boolean=True, stopwords=['text'], use_unknown=False)\n",
        "bow.fit(texts_test)\n",
        "assert torch.all(bow.transform(texts_test) == torch.Tensor([[0., 1., 1., 0., 0.], [1., 0., 0., 1., 1.]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyp2AzOx1UQD"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "$$\\text{TF-IDF}(t, d, C) = tf(t, d) * idf(t, C)$$\n",
        "\n",
        "Abrindo as funções definidas na equação principal:\n",
        "- $tf(t, d) = \\text{numero de vezes que o termo t aparece no documento d}$\n",
        "- $idf(t, C) = \\log{\\frac{C}{n_t}}$ \n",
        "\n",
        "Onde: \n",
        "\n",
        "- $\\text{t: token ou termo;}$\n",
        "- $\\text{d: documento(frase, enunciado, etc);}$\n",
        "- $\\text{C: Corpus (conjunto de documentos).}$\n",
        "- $n_t\\text{: numero de documentos onde o token t aparece.}$\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Zv_fUG3mkb"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class TfIdf(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros utilizando TFIDF.\n",
        "\n",
        "  Attrs:\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_size=None, stopwords = []):\n",
        "    self.max_size = max_size\n",
        "    self.stopwords = stopwords\n",
        "\n",
        "  def _count_tokens_in_doc(self, tokenized_texts):\n",
        "    '''\n",
        "    Realiza a contagem de em quantos documentos uma mesma\n",
        "    palavra aparece, desconsiderando as stopwords.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(set(text))\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    return counter\n",
        "  \n",
        "  def _create_idf(self, counter):\n",
        "    '''\n",
        "    Cria o vetor de idf para cada um dos tokens do conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      idf(np.array): Array contendo o valor de idf para cada um dos tokens\n",
        "        do conjunto de treino.\n",
        "    '''\n",
        "    idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n",
        "    return np.log(idf)\n",
        "\n",
        "  def _create_vocab(self, counter):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    return vocab\n",
        "\n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    self.len_corpus = len(texts)\n",
        "    counter = self._count_tokens_in_doc(texts)\n",
        "\n",
        "    self.vocabulary = self._create_vocab(counter)\n",
        "    self.idf = self._create_idf(counter)\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      tfidf_texts(torch.tensor): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = torch.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      index = []\n",
        "      values = []\n",
        "      for key, value in counter.items():\n",
        "        if key in self.vocabulary.keys():\n",
        "          index.append(self.vocabulary[key])\n",
        "          values.append(value)\n",
        "\n",
        "      bow_text[index] = torch.Tensor(values)\n",
        "\n",
        "      transformed_texts.append(bow_text * self.idf)\n",
        "\n",
        "    return torch.vstack(transformed_texts).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKFMFu77hEM"
      },
      "source": [
        "## Vocabulario: {'t1': 0, 't2': 1, 't3': 2, 't4': 3}\n",
        "\n",
        "## Validando o TFIDF\n",
        "texts = [['t1', 't2', 't3', 't2', 't1'], ['t2', 't1'], ['t4', 't1']]\n",
        "tfidf = TfIdf()\n",
        "tfidf.fit(texts)\n",
        "assert torch.all(tfidf.transform(texts) - torch.Tensor([[0., 2 * np.log(3/2), np.log(3), 0.], [0., np.log(3/2), 0., 0.] , [0., 0., 0., np.log(3)]]) < 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOdOREM_g1I"
      },
      "source": [
        "## Tokenização\n",
        "\n",
        "Irei aplicar uma tokenização simples, onde irei remover a pontuação do texto e irei dividí-lo por palavras, ou seja, meus tokens serão as palavras que compõe a avaliação do filme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUKtOiSXo-z1"
      },
      "source": [
        "from re import findall\n",
        "\n",
        "def tokenizer(texts):\n",
        "  tokenized_texts = []\n",
        "  for text in texts:\n",
        "    tokens = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n",
        "    tokenized = [token.lower() for token in tokens]\n",
        "    tokenized_texts.append(tokenized)\n",
        "  return tokenized_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbGyXLWpr8J"
      },
      "source": [
        "tokenized_x_train = tokenizer(x_train)\n",
        "tokenized_x_valid = tokenizer(x_valid)\n",
        "tokenized_x_test = tokenizer(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqydRLn5Vl2"
      },
      "source": [
        "## Input para a rede neural\n",
        "\n",
        "No Pytorch, para treinarmos uma rede neural, é de extrema importância criarmos uma estrutura chamada Dataloader, a qual retorna, em cada iteração, um batch do tamanho definido. Para criarmos essa estrutura podemos definir uma outra classe, chamada Dataset, a qual temos que sobrescrever dois métodos: \\_\\_len\\_\\_ e \\_\\_getitem\\_\\_. Abaixo, podemos ver a definição do Dataset e a criação de uma função que cria os Dataloaders para utilizarmos no treino e teste. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP9s66aRW7qM"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VectorizedDataset(Dataset):\n",
        "    def __init__(self, x, y, vectorizer):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.vectorizer.transform([self.x[idx]])[0]\n",
        "        y = self.y[idx]\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoaOpkHSKQtZ"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_nn_input(tokenized_text_train, y_train,\n",
        "                    tokenized_text_valid, y_valid,\n",
        "                    tokenized_text_test, y_test,\n",
        "                    vectorizer, batch_size, shuffle):\n",
        "  \n",
        "  vectorizer.fit(tokenized_text_train)\n",
        "\n",
        "  train_dataset = VectorizedDataset(tokenized_text_train, torch.Tensor(y_train).reshape(-1,1), vectorizer)\n",
        "  valid_dataset = VectorizedDataset(tokenized_text_valid, torch.Tensor(y_valid).reshape(-1,1), vectorizer)\n",
        "  test_dataset = VectorizedDataset(tokenized_text_test, torch.Tensor(y_test).reshape(-1,1), vectorizer)\n",
        "\n",
        "  vectorized_texts_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  vectorized_texts_valid = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "  vectorized_texts_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return vectorized_texts_train, vectorized_texts_valid, vectorized_texts_test, vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn-p8WIK6Igl"
      },
      "source": [
        "## Definição da Rede Neural\n",
        "\n",
        "Na célula abaixo defino minha rede neural. Utilizarei 3 camadas escondidas com 128, 64 e 32 neurônios e, na camada de saída, teremos apenas uma saída e a aplicação de uma função sigmoide, que trará a probabilidade de ser positiva a avaliação (se > 0.5, é positiva, caso contrário, negativa)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g4INOL1J63S"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_units=128):\n",
        "        super().__init__()\n",
        "        self.dense = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPbiUIrHZlun",
        "outputId": "e0be44c2-a7d4-4ee9-da90-935b76b225df"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla P4\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4pcdOkoPF_w",
        "outputId": "3a7ee9b4-3eab-4f0e-ef33-d8556a58e68a"
      },
      "source": [
        "mlp = MLP(10000)\n",
        "mlp.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=10000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMqzN6a63Oh"
      },
      "source": [
        "### Treino da rede neural\n",
        "\n",
        "Abaixo, temos a definição do loop de treino:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWzhTJlZRyk"
      },
      "source": [
        "def train(model, train, valid, criterion, optimizer, filename_save, n_epochs=10):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device)\n",
        "      outputs = model(x_train)\n",
        "      batch_loss = criterion(outputs, y_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    \n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(x_valid)\n",
        "\n",
        "            # calcula a perda\n",
        "            batch_loss = criterion(outputs, y_valid)\n",
        "            preds = outputs > 0.5\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), filename_save + '.pt')\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayXsGnyW8lmo"
      },
      "source": [
        "## Loss e otimizador:\n",
        "\n",
        "Na célula a seguir estou definindo a Loss a ser utilizada (binary cross entropy, já que temos apenas duas classes) e Stochastic gradient descent como otimizador. Manterei os parâmetros para todos os experimentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ynox3OWO0lu"
      },
      "source": [
        "learningRate = 0.01\n",
        "\n",
        "# Utilizaremos CrossEntropyLoss como função de perda\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "# Gradiente descendente\n",
        "optimizer = torch.optim.SGD(mlp.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvPTgVXv_6xv"
      },
      "source": [
        "## Avaliação no conjunto de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ8xI9Df-qxi"
      },
      "source": [
        "def predict(model, state_dict, test):\n",
        "  accumulated_accuracy = 0\n",
        "  model.load_state_dict(torch.load(state_dict + '.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for x_test, y_test in test:\n",
        "          x_test = x_test.to(device)\n",
        "          y_test = y_test.to(device)\n",
        "\n",
        "          # predict da rede\n",
        "          outputs = model(x_test)\n",
        "\n",
        "          # calcula a perda\n",
        "          batch_loss = criterion(outputs, y_test)\n",
        "          preds = outputs > 0.5\n",
        "\n",
        "          # calcula a acurácia\n",
        "          batch_accuracy = (preds == y_test).sum()\n",
        "          accumulated_accuracy += batch_accuracy\n",
        "\n",
        "  test_acc = accumulated_accuracy / len(test.dataset)\n",
        "  test_acc *= 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {test_acc:.3f} %')\n",
        "  print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCd7irmn9N9r"
      },
      "source": [
        "## Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY2s0vY99RMV"
      },
      "source": [
        "### Bow Booleano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-C_3NNh9_Pq"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.1\n",
        "save_filename = 'bow_bool'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckg6CA2L9oCt",
        "outputId": "997a1d33-17c2-4498-a7a3-6c6cfa57bbdb"
      },
      "source": [
        "mlp_bow_bool = MLP(max_size)\n",
        "mlp_bow_bool.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSKZ5sZu9Xwb"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_bow_bool.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4_1T0bX9p-E",
        "outputId": "c444c033-03d2-4c45-b910-fff665b6a68b"
      },
      "source": [
        "vectorizer = BagOfWords(boolean=True, max_size = max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_bow_bool, vectorized_train, vectorized_valid, criterion,\n",
        "          optimizer, save_filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/9 Train Loss: 0.010083 Valid Loss: 0.006647 Valid Acc: 0.831\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.005940 Valid Loss: 0.005311 Valid Acc: 0.859\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.004731 Valid Loss: 0.005377 Valid Acc: 0.859\n",
            "Época: 3/9 Train Loss: 0.004157 Valid Loss: 0.005544 Valid Acc: 0.854\n",
            "Época: 4/9 Train Loss: 0.003747 Valid Loss: 0.004819 Valid Acc: 0.880\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.003460 Valid Loss: 0.005128 Valid Acc: 0.877\n",
            "Época: 6/9 Train Loss: 0.003036 Valid Loss: 0.004947 Valid Acc: 0.874\n",
            "Época: 7/9 Train Loss: 0.002731 Valid Loss: 0.005468 Valid Acc: 0.871\n",
            "Época: 8/9 Train Loss: 0.002243 Valid Loss: 0.005766 Valid Acc: 0.869\n",
            "Época: 9/9 Train Loss: 0.002208 Valid Loss: 0.005378 Valid Acc: 0.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2AwVpypAGd4",
        "outputId": "60e1b9ed-c100-4982-f54b-4605c0db9ea5"
      },
      "source": [
        "predict(mlp_bow_bool, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 87.428 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QzuMHMT9u02"
      },
      "source": [
        "### BoW frequência"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDp44vxp-FJE"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.1\n",
        "save_filename = 'bow_freq'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eqq3FnW-FJF",
        "outputId": "ba8da0c8-e995-4a32-8dcd-f7e0b5524a16"
      },
      "source": [
        "mlp_bow_freq = MLP(max_size)\n",
        "mlp_bow_freq.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly6LhNC9-FJF"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_bow_freq.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNayZFPs-FJG",
        "outputId": "c27ecd10-466c-4c86-dbc3-59bb576f82e2"
      },
      "source": [
        "vectorizer = BagOfWords(max_size=max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_bow_freq, vectorized_train, vectorized_valid, criterion,\n",
        "          optimizer, save_filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/9 Train Loss: 0.010010 Valid Loss: 0.010389 Valid Acc: 0.648\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.008421 Valid Loss: 0.007636 Valid Acc: 0.774\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.007375 Valid Loss: 0.007630 Valid Acc: 0.748\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.006855 Valid Loss: 0.006338 Valid Acc: 0.835\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.006291 Valid Loss: 0.005637 Valid Acc: 0.854\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.005811 Valid Loss: 0.005923 Valid Acc: 0.840\n",
            "Época: 6/9 Train Loss: 0.005661 Valid Loss: 0.006150 Valid Acc: 0.821\n",
            "Época: 7/9 Train Loss: 0.005237 Valid Loss: 0.005180 Valid Acc: 0.867\n",
            "best model\n",
            "Época: 8/9 Train Loss: 0.005247 Valid Loss: 0.005864 Valid Acc: 0.834\n",
            "Época: 9/9 Train Loss: 0.004928 Valid Loss: 0.005831 Valid Acc: 0.859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKC_uhYtAje3",
        "outputId": "43a0300e-ef30-4610-ae7c-03a01392d17c"
      },
      "source": [
        "predict(mlp_bow_freq, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 86.384 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfyrRMue-JZh"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2hZ6AHP-MzH"
      },
      "source": [
        "max_size = 3000\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "learningRate = 0.01\n",
        "save_filename = 'tfidf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaV-SXEF-MzI",
        "outputId": "eaa54dcc-8081-43af-b8fb-261695e1ed93"
      },
      "source": [
        "mlp_tfidf = MLP(max_size)\n",
        "mlp_tfidf.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (dense): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T49fixlm-MzI"
      },
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(mlp_tfidf.parameters(), lr=learningRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_oR8jxQ-MzJ",
        "outputId": "605397d1-ec06-4891-910a-3be65c5abc93"
      },
      "source": [
        "vectorizer = TfIdf(max_size=max_size)\n",
        "\n",
        "vectorized_train, vectorized_valid, vectorized_test, _ = create_nn_input(tokenized_x_train, y_train,\n",
        "                                                                         tokenized_x_valid, y_valid,\n",
        "                                                                         tokenized_x_test, y_test,\n",
        "                                                                         vectorizer, batch_size, True)\n",
        "\n",
        "_ = train(mlp_tfidf, vectorized_train, vectorized_valid, criterion,\n",
        "          optimizer, save_filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/9 Train Loss: 0.010833 Valid Loss: 0.010912 Valid Acc: 0.596\n",
            "best model\n",
            "Época: 1/9 Train Loss: 0.010750 Valid Loss: 0.010779 Valid Acc: 0.681\n",
            "best model\n",
            "Época: 2/9 Train Loss: 0.010412 Valid Loss: 0.010074 Valid Acc: 0.781\n",
            "best model\n",
            "Época: 3/9 Train Loss: 0.008645 Valid Loss: 0.007109 Valid Acc: 0.847\n",
            "best model\n",
            "Época: 4/9 Train Loss: 0.005563 Valid Loss: 0.005180 Valid Acc: 0.875\n",
            "best model\n",
            "Época: 5/9 Train Loss: 0.004282 Valid Loss: 0.005118 Valid Acc: 0.874\n",
            "best model\n",
            "Época: 6/9 Train Loss: 0.003744 Valid Loss: 0.004959 Valid Acc: 0.877\n",
            "best model\n",
            "Época: 7/9 Train Loss: 0.003407 Valid Loss: 0.005075 Valid Acc: 0.880\n",
            "Época: 8/9 Train Loss: 0.003204 Valid Loss: 0.005311 Valid Acc: 0.878\n",
            "Época: 9/9 Train Loss: 0.002964 Valid Loss: 0.005546 Valid Acc: 0.873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1BnY0cV3w4P",
        "outputId": "c0d386c4-b87d-4477-ffe0-2555dda74f55"
      },
      "source": [
        "predict(mlp_tfidf, save_filename, vectorized_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 87.912 %\n",
            "****************************************\n"
          ]
        }
      ]
    }
  ]
}