{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 2 - Exercício - Pedro Gengo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Aula_2_Exerc%C3%ADcio_Pedro_Gengo_BoW_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "# Aula 2: Análise de Sentimentos usando Bag of Words e TF-IDF\n",
        "Nome: Pedro Gabriel Gengo Lourenço\n",
        "\n",
        "## Enunciado\n",
        "\n",
        "- Treinar um classificador binário na tarefa de análise de sentimentos usando dataset IMDB.\n",
        "\n",
        "- Experimentar e reportar a acurácia usando 3 diferentes tipos de features como entrada:\n",
        "    1) Bag-of-words booleano\n",
        "    2) Bag-of-words com contagem das palavras (histograma das palavras)\n",
        "    3) TF-IDF\n",
        "\n",
        "\n",
        "As funções de tokenização e conversão de tokens para features devem ser implementas usando apenas o numpy ou outros pacotes nativos do python. Não é permitido usar as funções prontas (ex: do scikit-learn) para se obter as features de entrada.\n",
        "\n",
        "\n",
        "O scikit-learn pode ser usado apenas para treinar e avaliar o classificador (ex: SVM).\n",
        "\n",
        "Neste notebook iremos treinar um modelo para fazer análise de sentimento usando o dataset IMDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "# Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115c517a-fcf0-473f-eaa2-05e262db9bab"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/examples/imdb_sample.tgz\n",
        "!tar -xzf imdb_sample.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘imdb_sample.tgz’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "Carregamos o dataset .csv usando o pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "dfbfecc1-cf8e-4a00-c338-c31965bd55c2"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('imdb_sample/texts.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8dfjdJ-AV79"
      },
      "source": [
        "Iremos agora dividir o dataset em conjuntos de treino e teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCoftmPmAfXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70688de-ca7b-4aa2-d695-2cb3d441c9ac"
      },
      "source": [
        "treino = df[df['is_valid'] == False]\n",
        "valid = df[df['is_valid'] == True]\n",
        "\n",
        "print('treino.shape:', treino.shape)\n",
        "print('valid.shape:', valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "treino.shape: (800, 3)\n",
            "valid.shape: (200, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHus6FH7DftH"
      },
      "source": [
        "E iremos dividir estes dois conjuntos em entrada (X) e saída desejada (Y, ground-truth) do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2HyoywGDcW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c12741b-2f81-4528-e7a4-7f464c1dec2e"
      },
      "source": [
        "X_treino = treino['text']\n",
        "Y_treino = treino['label']\n",
        "X_valid = valid['text']\n",
        "Y_valid = valid['label']\n",
        "\n",
        "print('X_treino.head():', X_treino.head())\n",
        "print('Y_treino.head():', Y_treino.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_treino.head(): 0    Un-bleeping-believable! Meg Ryan doesn't even ...\n",
            "1    This is a extremely well-made film. The acting...\n",
            "2    Every once in a long while a movie will come a...\n",
            "3    Name just says it all. I watched this movie wi...\n",
            "4    This movie succeeds at being one of the most u...\n",
            "Name: text, dtype: object\n",
            "Y_treino.head(): 0    negative\n",
            "1    positive\n",
            "2    negative\n",
            "3    positive\n",
            "4    negative\n",
            "Name: label, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2yNXutfEXQ7"
      },
      "source": [
        "Ainda falta converter as strings \"positive\" e \"negative\" do ground-truth para valores booleanos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46RdLFLkEW-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642fe62e-293a-4fe6-c405-4f3f9687dd41"
      },
      "source": [
        "mapeamento = {'positive': True, 'negative': False}\n",
        "Y_treino_bool = Y_treino.map(mapeamento)\n",
        "Y_valid_bool = Y_valid.map(mapeamento)\n",
        "print(Y_treino_bool.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    False\n",
            "1     True\n",
            "2    False\n",
            "3     True\n",
            "4    False\n",
            "Name: label, dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQJ1E_cKkXpT"
      },
      "source": [
        "## Processamento dos textos\n",
        "\n",
        "Nessa etapa, como descrito no enunciado do exercício, iremos realizar três tipos de processamento:\n",
        "\n",
        "1. BoW booleano\n",
        "2. BoW com base na frequência\n",
        "3. TF-IDF\n",
        "\n",
        "É muito importante ressaltar a importância de aplicar a \"aprender\" transformação apenas no treino, ou seja, utilizar apenas o vocabulário do treino, e utilizar o que foi encontrado para o teste/validação.\n",
        "\n",
        "Para isso, usarei a estrutura de fit/transform bastante conhecida da biblioteca sklearn. Começarei, então, criando uma classe abstrata que será herdada na criação das outras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzlNA3zKmWBE"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        " \n",
        "class Transformer(ABC):\n",
        " \n",
        "  @abstractmethod\n",
        "  def fit(self):\n",
        "      pass\n",
        "    \n",
        "  @abstractmethod\n",
        "  def transform(self):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdDJMVUnSK0"
      },
      "source": [
        "## BoW (booleano e com frequência)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjnyDSRsnUW7"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class BagOfWords(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros.\n",
        "\n",
        "  Attrs:\n",
        "    boolean(bool): Flag que define se o vetor gerado sera com base na\n",
        "      frequencia (contagem) ou com base na ocorrencia ou nao (bool) de\n",
        "      uma palavra do vocabulario.\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "    use_unknown(bool): Flag que define o uso ou nao de um elemento para\n",
        "      palavras que nao existem no vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, boolean=False, max_size=None, stopwords = [], use_unknown=False):\n",
        "    self.max_size = max_size\n",
        "    self.boolean = boolean\n",
        "    self.stopwords = stopwords\n",
        "    self.use_unknown = use_unknown\n",
        "\n",
        "  def _create_vocab(self, tokenized_texts):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(text)\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    if self.use_unknown:\n",
        "      vocab['unknown'] = len(vocab)\n",
        "    return vocab\n",
        "  \n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    vocab = self._create_vocab(texts)\n",
        "    self.vocabulary = vocab\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      bow_texts(np.array): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "    if self.use_unknown:\n",
        "      unknown = self.vocabulary.get('unknown')\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = np.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      if self.use_unknown:\n",
        "        index = [self.vocabulary.get(key, unknown) for key in counter.keys()]\n",
        "      else:\n",
        "        index = [self.vocabulary[key] for key in counter.keys() if key in self.vocabulary.keys()]\n",
        "\n",
        "      if self.boolean:\n",
        "        bow_text[index] = 1\n",
        "      else:\n",
        "        values = [value for key, value in counter.items() if self.use_unknown or key in self.vocabulary.keys()]\n",
        "        bow_text[index] = values\n",
        "\n",
        "      transformed_texts.append(bow_text)\n",
        "\n",
        "    return np.vstack(transformed_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTNcy1YhyylV"
      },
      "source": [
        "## Vocabulario deve ser: {'a': 0, 'texttinho': 1, 'testando': 2, 'b': 3, 'c': 4, 'unknown': 5}\n",
        "texts_test = [['text', 'texttinho', 'texttinho', 'testando'], ['a', 'b', 'a', 'a', 'c']]\n",
        "\n",
        "## Testando BoW com frequencia\n",
        "bow = BagOfWords(boolean=False, stopwords=['text'], use_unknown=True)\n",
        "bow.fit(texts_test)\n",
        "assert np.all(bow.transform(texts_test) == np.array([[0., 2., 1., 0., 0., 1.], [3., 0., 0., 1., 1., 0.]]))\n",
        "\n",
        "## Testando BoW booleano\n",
        "bow = BagOfWords(boolean=True, stopwords=['text'], use_unknown=False)\n",
        "bow.fit(texts_test)\n",
        "assert np.all(bow.transform(texts_test) == np.array([[0., 1., 1., 0., 0.], [1., 0., 0., 1., 1.]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyp2AzOx1UQD"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "$$\\text{TF-IDF}(t, d, C) = tf(t, d) * idf(t, C)$$\n",
        "\n",
        "Abrindo as funções definidas na equação principal:\n",
        "- $tf(t, d) = \\text{numero de vezes que o termo t aparece no documento d}$\n",
        "- $idf(t, C) = \\log{\\frac{C}{n_t}}$ \n",
        "\n",
        "Onde: \n",
        "\n",
        "- $\\text{t: token ou termo;}$\n",
        "- $\\text{d: documento(frase, enunciado, etc);}$\n",
        "- $\\text{C: Corpus (conjunto de documentos).}$\n",
        "- $n_t\\text{: numero de documentos onde o token t aparece.}$\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Zv_fUG3mkb"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class TfIdf(Transformer):\n",
        "  '''\n",
        "  Essa classe realiza a transformacao de uma lista de palavras\n",
        "  para uma lista de inteiros utilizando TFIDF.\n",
        "\n",
        "  Attrs:\n",
        "    max_size(int): Define o tamanho maximo do vocabulario. Caso usado com\n",
        "      use_unknown = True, o vocabulario tera o tamanho de max_size + 1.\n",
        "    stopwords(list): Define a lista de palavras que serao desconsideras na\n",
        "      geracao do vocabulario.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_size=None, stopwords = []):\n",
        "    self.max_size = max_size\n",
        "    self.stopwords = stopwords\n",
        "\n",
        "  def _count_tokens_in_doc(self, tokenized_texts):\n",
        "    '''\n",
        "    Realiza a contagem de em quantos documentos uma mesma\n",
        "    palavra aparece, desconsiderando as stopwords.\n",
        "\n",
        "    Args:\n",
        "      tokenized_texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    '''\n",
        "    counter = Counter()\n",
        "    for text in tokenized_texts:\n",
        "      counter.update(set(text))\n",
        "    for stop_word in self.stopwords:\n",
        "      if stop_word in counter.keys():\n",
        "        del counter[stop_word]\n",
        "    return counter\n",
        "  \n",
        "  def _create_idf(self, counter):\n",
        "    '''\n",
        "    Cria o vetor de idf para cada um dos tokens do conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      idf(np.array): Array contendo o valor de idf para cada um dos tokens\n",
        "        do conjunto de treino.\n",
        "    '''\n",
        "    idf = [self.len_corpus/count for token, count in counter.most_common(self.max_size)]\n",
        "    return np.log(idf)\n",
        "\n",
        "  def _create_vocab(self, counter):\n",
        "    '''\n",
        "    Cria o vocabulario que sera utilizado na transformacao do vetores\n",
        "    de palavras para vetores de inteiros.\n",
        "\n",
        "    Args:\n",
        "      counter(collections.Counter): Objeto da classe Counter com todos\n",
        "        os elementos do conjunto de treino.\n",
        "    \n",
        "    Return:\n",
        "      vocab(dict): Dicionario onde as chaves sao as palavras do vocabulario\n",
        "        e os valores representam o indice da palavra no vetor a ser gerado.\n",
        "    '''\n",
        "    vocab = {element[0]: index for index, element in enumerate(counter.most_common(self.max_size))}\n",
        "    return vocab\n",
        "\n",
        "  def fit(self, texts):\n",
        "    '''\n",
        "    Metodo que cria os argumentos que serao utilizados nas\n",
        "    transformacoes posteriores. Esse metodo so deve ser utilizado \n",
        "    sobre o conjunto de treino.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    '''\n",
        "    self.len_corpus = len(texts)\n",
        "    counter = self._count_tokens_in_doc(texts)\n",
        "\n",
        "    self.vocabulary = self._create_vocab(counter)\n",
        "    self.idf = self._create_idf(counter)\n",
        "\n",
        "  def transform(self, texts):\n",
        "    '''\n",
        "    Realiza a transformacao de uma lista de tokens para uma\n",
        "    lista de inteiros com base no vocabulario criado na etapa\n",
        "    de fit.\n",
        "\n",
        "    Args:\n",
        "      texts(list): Lista de textos ja tokenizados, ou seja,\n",
        "        uma lista onde cada elemento e um token.\n",
        "    \n",
        "    Return:\n",
        "      tfidf_texts(np.array): Array contendo os vetores de tokens\n",
        "        transformados para vetores de inteiros de tamanho fixo.\n",
        "    '''\n",
        "    transformed_texts = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "      bow_text = np.zeros(len(self.vocabulary))\n",
        "      counter = Counter(text)\n",
        "\n",
        "      index = []\n",
        "      values = []\n",
        "      for key, value in counter.items():\n",
        "        if key in self.vocabulary.keys():\n",
        "          index.append(self.vocabulary[key])\n",
        "          values.append(value)\n",
        "\n",
        "      bow_text[index] = values\n",
        "\n",
        "      transformed_texts.append(bow_text * self.idf)\n",
        "\n",
        "    return np.vstack(transformed_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKFMFu77hEM"
      },
      "source": [
        "## Vocabulario: {'t1': 0, 't2': 1, 't3': 2, 't4': 3}\n",
        "\n",
        "## Validando o TFIDF\n",
        "texts = [['t1', 't2', 't3', 't2', 't1'], ['t2', 't1'], ['t4', 't1']]\n",
        "tfidf = TfIdf()\n",
        "tfidf.fit(texts)\n",
        "assert np.all(tfidf.transform(texts) - np.array([[0., 2 * np.log(3/2), np.log(3), 0.], [0., np.log(3/2), 0., 0.] , [0., 0., 0., np.log(3)]]) < 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1abQX3O9x0X"
      },
      "source": [
        "## Treinando um classificador binário\n",
        "\n",
        "Antes de treinar o classificador, precisamos garantir que nossos textos de entradas estejam vetorizados, ou seja, tenhamos aplicado algum método de conversão de token para features, que são as classes que definimos acima.\n",
        "Contudo, para que utilizemos as classes, necessitamos tokenizar nossos textos antes. Usei a estratégia de realizar a tokenização fora das classes pois assim, posso reutilizar os textos já tokenizados para os três experimentos que irei efetuar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOdOREM_g1I"
      },
      "source": [
        "### Tokenização\n",
        "\n",
        "Irei aplicar uma tokenização simples, onde irei remover a pontuação do texto e irei dividí-lo por palavras, ou seja, meus tokens serão as palavras que compõe a avaliação do filme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUKtOiSXo-z1"
      },
      "source": [
        "from re import findall\n",
        "\n",
        "def tokenizer(texts):\n",
        "  tokenized_texts = []\n",
        "  for text in texts:\n",
        "    tokens = findall(r'\\w+|[^?\\-!.,:;\"\\'/><\\s]', text)\n",
        "    tokenized = [token.lower() for token in tokens]\n",
        "    tokenized_texts.append(tokenized)\n",
        "  return tokenized_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZbGyXLWpr8J"
      },
      "source": [
        "tokenized_X_treino = tokenizer(X_treino.values)\n",
        "tokenized_X_valid = tokenizer(X_valid.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT-AT_EF_QIM"
      },
      "source": [
        "### Acurácia\n",
        "\n",
        "Os resultados dos experimentados serão dados em função da acurácia, que representa o quanto acertamos do total. Para isso, necessitei escrever a função que irá calcular ela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3HMZM__cz8"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return np.sum(y_true == y_pred) / y_true.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWDG3fYhEe-P"
      },
      "source": [
        "### Experimentos\n",
        "\n",
        "A função abaixo foi definida para facilitar a execução dos experimentos utilizando os diferentes tipos de conversão de tokens para features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLdf0wJOBNOa"
      },
      "source": [
        "def run(tokenized_train, tokenized_valid, target_train, target_valid, vectorizer, model):\n",
        "  print(f'Aplicando {vectorizer.__class__.__name__}')\n",
        "  vectorizer.fit(tokenized_train)\n",
        "  vectorized_texts = vectorizer.transform(tokenized_train)\n",
        "  print(f'O shape dos dados de treinos vetorizados é: {vectorized_texts.shape}')\n",
        "  print(f'Treinando {model.__class__.__name__}')\n",
        "  model.fit(vectorized_texts, target_train)\n",
        "\n",
        "  print(f'Utilizando os dados de validação')\n",
        "  vectorized_valid = vectorizer.transform(tokenized_valid)\n",
        "  y_predicted = model.predict(vectorized_valid)\n",
        "\n",
        "  acc = round(accuracy(target_valid, y_predicted), 4) * 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {acc}% utilizando {vectorizer.__class__.__name__}')\n",
        "  print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfD2e6-rEtZH"
      },
      "source": [
        "#### Experimento 1: BoW booleano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqXX1dCmEY_0"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abJUyv18Dek4",
        "outputId": "1404f6f9-7dc1-49c7-e259-85f58fc7eaa8"
      },
      "source": [
        "bow_bool = BagOfWords(boolean=True, max_size=3000)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, bow_bool, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando BagOfWords\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 82.5% utilizando BagOfWords\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzAP7kGME1HZ"
      },
      "source": [
        "#### Experimento 2: BoW frequencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe7Gqw1PDd-C",
        "outputId": "ab1007db-359b-4c26-c899-476f1875f720"
      },
      "source": [
        "bow_freq = BagOfWords(max_size=3000)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, bow_freq, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando BagOfWords\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 77.0% utilizando BagOfWords\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02NJitxEx7G"
      },
      "source": [
        "#### Experimento 3: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM2IrMmXC74Y",
        "outputId": "e78a6b0e-9abd-4d67-c948-605be985e09c"
      },
      "source": [
        "tfidf = TfIdf(max_size=3000)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, tfidf, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando TfIdf\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 83.5% utilizando TfIdf\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg8ojhgIGHbd"
      },
      "source": [
        "### Extra: Removendo stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2VlJyvKGLn_",
        "outputId": "87ab4e71-9f76-4d7a-f4a9-3410c3b86b5a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sw_english = list(stopwords.words('english'))\n",
        "sw_english[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4LCNwBlGqTG"
      },
      "source": [
        "#### Experimento 1: BoW booleano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2LarZBHGQu5",
        "outputId": "1ff5955c-8245-4b8e-9f38-f0f374843a38"
      },
      "source": [
        "bow_bool = BagOfWords(boolean=True, max_size=3000, stopwords=sw_english)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, bow_bool, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando BagOfWords\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 81.0% utilizando BagOfWords\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNJAA-w5GyAx"
      },
      "source": [
        "#### Experimento 2: BoW frequencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLlIcmlYGyAy",
        "outputId": "4a0d4f0c-4d52-4920-e189-0d8dc443a5f1"
      },
      "source": [
        "bow_freq = BagOfWords(max_size=3000, stopwords=sw_english)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, bow_freq, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando BagOfWords\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 83.5% utilizando BagOfWords\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-PxWO6IGyAy"
      },
      "source": [
        "#### Experimento 3: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbuzYANOGyAy",
        "outputId": "92543a35-03f7-4866-9809-7838fc47d7f5"
      },
      "source": [
        "tfidf = TfIdf(max_size=3000, stopwords=sw_english)\n",
        "model = SVC(C=10.)\n",
        "run(tokenized_X_treino, tokenized_X_valid, Y_treino_bool, Y_valid_bool, tfidf, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aplicando TfIdf\n",
            "O shape dos dados de treinos vetorizados é: (800, 3000)\n",
            "Treinando SVC\n",
            "Utilizando os dados de validação\n",
            "****************************************\n",
            "Acurácia de 82.5% utilizando TfIdf\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_GPqLapG1ik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}