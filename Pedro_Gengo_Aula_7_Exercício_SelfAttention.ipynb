{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pedro Gengo - Aula 7 - Exercício",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/DLforNLP/blob/main/Pedro_Gengo_Aula_7_Exerc%C3%ADcio_SelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Pedro Gabriel Gengo Lourenço"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE_e0z4Y7Rnp",
        "outputId": "2706d8d5-05b2-46e4-e8c0-701d8b5b4084"
      },
      "source": [
        "!pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.12.0.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.18.51-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Requirement already satisfied: jsonschema<4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.51\n",
            "  Downloading botocore-1.21.51-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 31.9 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.51->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.17.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 45.8 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.12.0-py2.py3-none-any.whl size=478334 sha256=082d846d4643274567f9140954eadbda2c14806e791202e341375177d73be817\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/2f/0a/f16d84ff3a68368a6b7d39d199c37454d2cefaf3b99e6eb435\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ec918079c7a1a5ad6cf37844131beff0e1543dbcc9f7c431fa3039785e3e6e36\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=2e5de016033f7398b9842c11ed99feb399f527d7be73f14f73819df595e50d7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, urllib3, strict-rfc3339, rfc3987, jmespath, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 PyJWT-2.1.0 boto3-1.18.51 botocore-1.21.51 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.7 jmespath-0.10.0 jsonref-0.2 monotonic-1.6 neptune-client-0.12.0 rfc3987-1.3.8 s3transfer-0.5.0 simplejson-3.17.5 smmap-4.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 urllib3-1.25.11 webcolors-1.11.1 websocket-client-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "Treinar e medir a acurácia de um modelo de classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "O modelo deverá ter uma camada de auto-atenção simplificada igual à apresentada no slide 96.\n",
        "\n",
        "Deverão ser entregues duas implementações da camada de auto-atenção, como apresentado no slide 100:\n",
        "1. Usando laços (ineficiente, mas bom para o aprendizado)\n",
        "2. Matricial\n",
        "\n",
        "Devemos usar embeddings pretreinados (glove) como entrada para a camada de auto-atenção. Lembrar de congelá-los pois, caso contrário,  pode ocorrer overfit.\n",
        "\n",
        "Ao corrigir o exercicio, iremos também nos atentar na eficiencia/velocidade das implementações.\n",
        "\n",
        "Dicas:\n",
        "- A dificuldade deste exercício será implementar a auto-atenção de forma matricial usando minibatches. Para lidar com exemplos de tamanho variável, deve-se truncá-los e aplicar padding.\n",
        "\n",
        "- Evitar usar qualquer laço na implementação matricial, pois isso a deixará muito ineficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojyVOubollcH"
      },
      "source": [
        "## Definindo os parametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-YHxi_AllQZ"
      },
      "source": [
        "params = {\n",
        "    'vocabulary_size': 400000,\n",
        "    'padding_idx': 400001,\n",
        "    'max_length': 200,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "import neptune.new as neptune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "40496b7a-4ad5-4f76-d174-1c7b31f366c2"
      },
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8f49c4cc30>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf6bdc2-4569-4394-c105-58827b0c6ce4"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-30 11:22:18--  http://files.fast.ai/data/aclImdb.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 104.26.2.19, 172.67.69.159, 104.26.3.19, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/aclImdb.tgz [following]\n",
            "--2021-09-30 11:22:18--  https://files.fast.ai/data/aclImdb.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145982645 (139M) [application/x-gtar-compressed]\n",
            "Saving to: ‘aclImdb.tgz’\n",
            "\n",
            "aclImdb.tgz         100%[===================>] 139.22M  9.46MB/s    in 16s     \n",
            "\n",
            "2021-09-30 11:22:34 (8.87 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ecd255-1f55-4d77-d881-944ecc9e2b8b"
      },
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False This, and Immoral Tales, both left a bad taste in my mouth. It seems to me that Borowczyk is disgust\n",
            "False Phew--I don't what to say. This is a film that could be really good a with a bunch of stoned viewers\n",
            "True I'm giving ten out of ten it's one of the best movies ever. Absolutely smashed, stunned and dazed by\n",
            "3 últimas amostras treino:\n",
            "False I'm not looking for quality; I'm just trying to get through the 74 famous video nasties that were ba\n",
            "True Best animated movie ever made. This film explores not only the vast world of modern animation with a\n",
            "True Young Mr.Lincoln is a poetic,beautiful film that captures the myth of one of the most revered figure\n",
            "3 primeiras amostras validação:\n",
            "True I remember seeing this movie a long time ago, back then even though it didn't have any special effec\n",
            "True I love Claire Danes, and Kate Beckinsale looks amazingly immature in her role. The movie is flawed o\n",
            "True This film, once sensational for its forward-thinking politics and depictions of free love and sexual\n",
            "3 últimas amostras validação:\n",
            "True A Frank Capra WONDERS OF LIFE film.<br /><br />Keeping the blood pumping through our veins is the re\n",
            "True David Lynch's (1999) film of John Roach / Mary Sweeney's story is set in Iowa and Wisconsin some tim\n",
            "True I have to say I quite enjoyed Soldier. Russell was very good as this trained psychopath rediscoverin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c_-4MRtsOL_"
      },
      "source": [
        "# Carregando os embeddings do Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5h_a9nvs5FJ",
        "outputId": "500623cb-f0c9-4897-e088-0b8ea3333e68"
      },
      "source": [
        "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip -d glove_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-30 11:22:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-30 11:22:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-30 11:22:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 41s  \n",
            "\n",
            "2021-09-30 11:25:33 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove_dir/glove.6B.50d.txt  \n",
            "  inflating: glove_dir/glove.6B.100d.txt  \n",
            "  inflating: glove_dir/glove.6B.200d.txt  \n",
            "  inflating: glove_dir/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9NSvROYvnWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7cb515-3878-4e64-e811-bb472729cfa7"
      },
      "source": [
        "from torchtext.vocab import GloVe\n",
        "glove_vectors = GloVe(name='6B', dim=300, cache='./glove_dir')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 399999/400000 [00:52<00:00, 7669.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz4RNqJ3wNzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191558af-c3c3-4a0d-810f-f6be5e7de517"
      },
      "source": [
        "print(glove_vectors.vectors.shape)\n",
        "print('Primeiras 20 palavras e seus índices:', list(glove_vectors.stoi.items())[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([400000, 300])\n",
            "Primeiras 20 palavras e seus índices: [('the', 0), (',', 1), ('.', 2), ('of', 3), ('to', 4), ('and', 5), ('in', 6), ('a', 7), ('\"', 8), (\"'s\", 9), ('for', 10), ('-', 11), ('that', 12), ('on', 13), ('is', 14), ('was', 15), ('said', 16), ('with', 17), ('he', 18), ('as', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ySaZBHxTLFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45895113-1cdb-49f1-d977-7906753dac2a"
      },
      "source": [
        "vocab = glove_vectors.stoi\n",
        "vocab['<UNK>'] = params['vocabulary_size'] # The last row is for the unknown token.\n",
        "\n",
        "# We create a random vector for the unknown token\n",
        "unk_vector = torch.FloatTensor(1, glove_vectors.vectors.shape[1]).uniform_(-0.5, 0.5)\n",
        "\n",
        "# We create a vector of zeros for the pad token\n",
        "pad_vector = torch.zeros(1, glove_vectors.vectors.shape[1])\n",
        "\n",
        "# And add them to the embeddings matrix.\n",
        "embeddings = torch.cat((glove_vectors.vectors, unk_vector, pad_vector), dim=0)\n",
        "\n",
        "print(f'Total de palavras: {len(vocab)}')\n",
        "print(f'embeddings.shape: {embeddings.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras: 400001\n",
            "embeddings.shape: torch.Size([400002, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlaPgP0Z_D4"
      },
      "source": [
        "# Definindo o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIpp1C_qZ-QX"
      },
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return [token.lower() for token in re.compile('\\w+').findall(text)]\n",
        "\n",
        "\n",
        "def to_token_ids(text, vocab, max_length, padding_idx):\n",
        "    tokens = tokenize(text)[:max_length]  # Truncating.\n",
        "    token_ids = []\n",
        "    for token in tokens:\n",
        "        # We use the id of the \"<UNK>\" token if we don't find it in the vocabulary.\n",
        "        token_id = vocab.get(token, vocab['<UNK>'])\n",
        "        token_ids.append(token_id)\n",
        "\n",
        "    # Adding PAD tokens, if necessary.\n",
        "    token_ids += [padding_idx] * max(0, max_length - len(token_ids))\n",
        "    return token_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_6pddDHEM_r"
      },
      "source": [
        "# Definindo a camada de atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NupsGJ5PzZJR"
      },
      "source": [
        "## Com loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLReRSuDEPLL"
      },
      "source": [
        "class SelfAttentionLayerLoop(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, padding_idx):\n",
        "        super(SelfAttentionLayerLoop, self).__init__()\n",
        "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings, padding_idx=padding_idx)\n",
        "        self.padding_idx = padding_idx\n",
        "            \n",
        "    def forward(self, batch_token_ids):\n",
        "        batch_emb_tokens = self.embeddings(batch_token_ids)\n",
        "        batch_mean_embeddings = []\n",
        "        for emb_tokens, ids in zip(batch_emb_tokens, batch_token_ids): # Iterando cada elemento do batch\n",
        "          Q_means = []\n",
        "\n",
        "          for emb_token_Q in emb_tokens[torch.nonzero(ids != self.padding_idx)]: # Pego apenas os exemplos que nao sao padding como Q\n",
        "            scores = []\n",
        "            for i, emb_token_K in enumerate(emb_tokens):\n",
        "              if ids[i] == self.padding_idx:\n",
        "                score_not_norm = -float(\"Inf\")\n",
        "              else:\n",
        "                score_not_norm = torch.matmul(emb_token_Q, emb_token_K)\n",
        "              scores.append(score_not_norm)\n",
        "            scores = torch.tensor(scores)\n",
        "            scores_norm = torch.nn.functional.softmax(scores, dim=0)\n",
        "\n",
        "            emb_sum = torch.zeros_like(emb_tokens[0]) # Inicializando vetor que sera a soma dos vetores contextualizados dos tokens\n",
        "            for emb_token_V, score_norm in zip(emb_tokens, scores_norm):\n",
        "              emb_sum += emb_token_V * score_norm\n",
        "\n",
        "            Q_means.append(emb_sum)\n",
        "          batch_mean_embeddings.append(torch.stack(Q_means).sum(dim=0) / len(Q_means))\n",
        "        return torch.vstack(batch_mean_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAbyurShDA8e"
      },
      "source": [
        "## Sem Loop (multiplicação de matriz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_ba6VrPDFDs"
      },
      "source": [
        "class SelfAttentionLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, padding_idx):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings, padding_idx=padding_idx)\n",
        "        self.padding_idx = padding_idx\n",
        "            \n",
        "    def forward(self, batch_token_ids):\n",
        "        batch_emb_tokens = self.embeddings(batch_token_ids)\n",
        "        pad_ids = torch.nonzero(batch_token_ids == self.padding_idx) # Posicoes, em cada batch, onde estao os PAD\n",
        "        scores = torch.bmm(batch_emb_tokens, batch_emb_tokens.transpose(2,1))\n",
        "        scores[pad_ids[:, 0], :, pad_ids[:, 1]] = -float(\"Inf\") # Colocando Inf nos scores das Keys\n",
        "        scores[pad_ids[:, 0],  pad_ids[:, 1], :] = -float(\"Inf\") # Colocando Inf nos scores da Queries\n",
        "        probs = torch.nn.functional.softmax(scores, dim=2)\n",
        "        lengths = torch.logical_not(torch.isnan(probs)).sum(dim=1)[:, 0] # Calculando o tamanho de cada sentenca do batch\n",
        "        probs[torch.isnan(probs)] = 0. # Zerando os embeddings Nan, devido a Queries do token PAD\n",
        "        new_embs = torch.matmul(probs, batch_emb_tokens).sum(dim=1)\n",
        "        emb_means = new_embs / lengths.reshape(batch_token_ids.shape[0], -1) # Reshape para ficar no formato correto\n",
        "        return emb_means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSbNNLzjya7z"
      },
      "source": [
        "## Testando a implementação com embeddings \"falsos\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K_LJm2lygau",
        "outputId": "f4ba0bbd-bb1b-41d7-d390-04b4b20413ca"
      },
      "source": [
        "fake_vocab = {\n",
        "    'a': 0,\n",
        "    'b': 1,\n",
        "    'c': 2,\n",
        "    '<UNK>': 3 \n",
        "}\n",
        "\n",
        "fake_embeddings = torch.arange(0, 2 * len(fake_vocab)).reshape(len(fake_vocab), 2).float()\n",
        "pad_vector = torch.zeros(1, 2)\n",
        "fake_embeddings = torch.cat((fake_embeddings, pad_vector), dim=0)\n",
        "\n",
        "fake_examples = [\n",
        "    'a', # Testing PAD\n",
        "    'a b',\n",
        "    'a c b', # Testing truncation\n",
        "    'a z', # Testing <UNK>\n",
        "    ]\n",
        "\n",
        "print(f'Total de palavras: {len(fake_vocab)}')\n",
        "print(f'embeddings.shape: {fake_embeddings.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palavras: 4\n",
            "embeddings.shape: torch.Size([5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5lhQ2YY4rz6",
        "outputId": "4e35be66-d828-424c-e2ef-a6b90a347c0e"
      },
      "source": [
        "fake_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.],\n",
              "        [6., 7.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IHiV6nRzg4q"
      },
      "source": [
        "self_attention_layer = SelfAttentionLayer(\n",
        "    embeddings=fake_embeddings,\n",
        "    padding_idx=4)\n",
        "self_attention_layer_loop = SelfAttentionLayerLoop(\n",
        "    embeddings=fake_embeddings,\n",
        "    padding_idx=4)\n",
        "\n",
        "batch_token_ids = []\n",
        "for example in fake_examples:\n",
        "    token_ids = to_token_ids(\n",
        "        text=example,\n",
        "        vocab=fake_vocab,\n",
        "        max_length=2,\n",
        "        padding_idx=4)\n",
        "    batch_token_ids.append(token_ids)\n",
        "\n",
        "batch_token_ids = torch.LongTensor(batch_token_ids)\n",
        "my_output = self_attention_layer(batch_token_ids)\n",
        "my_output_loop = self_attention_layer_loop(batch_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJsMGZ911l67"
      },
      "source": [
        "target_output = torch.FloatTensor([\n",
        "    [0.00000000, 1.00000000],\n",
        "    [1.88075161, 2.88075161],\n",
        "    [3.96402740, 4.96402740],\n",
        "    [5.99258232, 6.99258232]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR_sjMsm1kK1"
      },
      "source": [
        "assert torch.allclose(my_output, target_output, atol=1e-6)\n",
        "assert torch.allclose(my_output_loop, target_output, atol=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS6p45y4It01"
      },
      "source": [
        "## Testando a implementação com 8 exemplos do dataset do IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm_WeJi-EciW"
      },
      "source": [
        "examples = [\n",
        "    \"THE TEMP (1993) didn't do much theatrical business, but here's the direct-to-video rip-off you didn't want, anyway! Ellen Bradford (Mel Harris) is the new woman at Millennium Investments, a high scale brokerage firm, who starts getting helpful hints from wide-eyed secretary Deidre (Sheila Kelley). Deidre turns out to be an ambitious daddy's girl who will stop at nothing to move up the corporate ladder, including screwing a top broker she can't stand and murdering anyone who gets on her bad side. She digs up skeletons in Ellen's closet, tries to cause problems with her husband (Barry Bostwick), kills while making it look like she is responsible, kidnaps her daughter and tries to get her to embezzle money from the company.<br /><br />Harris and Kelley deliver competent performances, the supporting cast is alright and it's reasonably well put-together, but that doesn't fully compensate for a script that travels down a well-worn path and offers few surprises.\",\n",
        "    \"Sondra Locke stinks in this film, but then she was an awful 'actress' anyway. Unfortunately, she drags everyone else (including then =real life boyfriend Clint Eastwood down the drain with her. But what was Clint Eastwood thinking when he agreed to star in this one? One read of the script should have told him that this one was going to be a real snorer. It's an exceptionally weak story, basically no story or plot at all. Add in bored, poor acting, even from the normally good Eastwood. There's absolutely no action except a couple arguments and as far as I was concerned, this film ranks up at the top of the heap of natural sleep enhancers. Wow! Could a film BE any more boring? I think watching paint dry or the grass grow might be more fun. A real stinker. Don't bother with this one.\",\n",
        "    \"Judy Davis shows us here why she is one of Australia's most respected and loved actors - her portrayal of a lonely, directionless nomad is first-rate. A teenaged Claudia Karvan also gives us a glimpse of what would make her one of this country's most popular actors in years to come, with future roles in THE BIG STEAL, THE HEARTBREAK KID, DATING THE ENEMY, RISK and the acclaimed TV series THE SECRET LIFE OF US. (Incidentally, Karvan, as a child, was a young girl whose toy Panda was stolen outside a chemist's shop in the 1983 drama GOING DOWN with Tracey Mann.) If this films comes your way, make sure you see it!! Rating: 79/100. See also: HOTEL SORRENTO, RADIANCE, VACANT POSSESSION, LANTANA.\",\n",
        "    'New York playwright Michael Caine (as Sidney Bruhl) is 46-years-old and fading fast; as the film opens, Mr. Caine\\'s latest play flops on Broadway. TV reviewers poke fun at Caine, and he gets drunk. Passing out on the Long Island Railroad lands Caine in Montauk, instead of his residence in East Hampton. Finally arriving home, Caine is comforted by tightly-attired wife Dyan Cannon (as Myra), an unfortunately high-strung heart patient. There, Caine and Ms. Cannon discuss a new play called \"Deathtrap\", written by hunky young Christopher Reeve (as Clifford \"Cliff\" Anderson), one of Caine\\'s former students. The couple believe Mr. Reeve\\'s \"Deathtrap\" is the hit needed to revive Caine\\'s career.<br /><br />\"The Trap Is Set\\x85 For A Wickedly Funny Who\\'ll-Do-It.\" <br /><br />Directed by Sidney Lumet, Ira Levin\\'s long-running Broadway hit doesn\\'t stray too far from its stage origin. The cast is enjoyable and the story\\'s twists are still engrossing. One thing that did not work (for me) was the curtain call ending; surely, it played better on stage. \"Deathtrap\" is a fun film to watch again; the performances are dead on - but, in hindsight, the greeting Reeve gives Caine at the East Hampton train station should have been simplified to a smiling \"Hello.\" The location isn\\'t really East Hampton, but the windmill and pond look similar. And, the much ballyhooed love scene is shockingly tepid. But, the play was so good, \"even a gifted director couldn\\'t ruin it.\" And, Mr. Lumet doesn\\'t disappoint.<br /><br />******** Deathtrap (3/19/82) Sidney Lumet ~ Michael Caine, Christopher Reeve, Dyan Cannon, Irene Worth',\n",
        "    'Students often ask me why I choose this version of Othello. Shakespeare\\'s text is strongly truncated and the film contains material which earned it an \"R\" rating.<br /><br />I have several reasons for using this production: First, I had not seen a depiction of the Moor that actually made me sympathetic to Othello until I saw Fishburne play him. I saw James Earl Jones and Christopher Plummer play Othello and Iago on Broadway, and it was wonderful. Plummer\\'s energy was especially noticeable. But in spite of Jone\\'s incredible presence both physically and vocally, the character he played just seemed too passive to illicit from me a complete emotional purgation in the Aristotelian sense. Jones, in fact, affirmed what I felt when in an interview he noted that he had played Othello as passive--seeing Iago as basically doing him over. Unfortunately this sapped my grief for the character destruction. Thus, I felt sympathy for Jone\\'s Moor but not the horror over his corruption by an evil man. In contrast, Fishburne\\'s Othello is a strong and vigorous figure familiar with taking action. Thus, Iago\\'s temptation to actively deal with what is presented to Othello as his wife\\'s unfaithfulness is a perversion of the general\\'s positive quality to be active not passive.1 The horror of the story is that this good quality in Othello becomes perverted. Fishburne\\'s depiction is therefore classically tragic.<br /><br />Second, Fishburne is the first black actor to play Othello in a film. Both Orsen Wells and Anthony Hopkins did fine film versions, but they were white men in black face.2 Why is this important? Why should a Black actor be the Black man on the stage?3 Certainly in Shakespeare\\'s day they used black face just as they used boys to make girls. Perhaps then, the reason is the same. Female actors bring a special quality to female roles on the Shakespearian stage because they understand best what Shakespeare\\'s genius was trying to present. A gifted black actor should play the moor because his experience in a white dominated culture is vital to understanding what Shakespeare\\'s genius recognized: the pain of being marginalized because of race. An important theme in Othello is isolation caused by racism. Although it is a mistake to insert American racism into a Shakespearian play, there can be little doubt that racism is still working among the characters. Many, including Desdimona\\'s father, think that a union between a Venetian white Christian woman and a North African black Christian man is UNNATURAL.<br /><br />Third, Shakespeare was never G rated. He never has been. His stage productions were always typified by violence and strong language. But Shakespeare\\'s genius uses these elements not as sensationialism but for artistic honesty.',\n",
        "    'Roeg has done some great movies, but this a turkey. It has a feel of a play written by an untalented high-school student for his class assignment. The set decoration is appealing in a somewhat surrealistic way, but the actual story is insufferable hokum.',\n",
        "    \"<br /><br />What is left of Planet Earth is populated by a few poor and starving rag-tag survivors. They must eat bugs and insects, or whatever, after a poison war, or something, has nearly wiped out all human civilization. In these dark times, one of the few people on Earth still able to live in comfort, we will call him the All Knowing Big Boss, has a great quest to prevent some secret spore seeds from being released into the air. It seems that the All Knowing Big Boss is the last person on Earth that knows that these spores even exist. The spores are located far away from any living soul, and they are highly protected by many layers of deadly defense systems. <br /><br />The All Knowing Big Boss wants the secret spores to remain in their secret protected containers. So, he makes a plan to send in a macho action team to remove the spore containers from all of the protective systems and secret location. Sending people to the location of secret spores makes them no longer a secret. Sending people to disable all of the protective systems makes it possible for the spores to be easily released into the air. How about letting sleeping dogs lie?! <br /><br />The one pleasant feature of ENCRYPT is the radiant and elegant Vivian Wu. As the unremarkable macho action team members drop off with mechanically paced predictable timing, engaging Vivian Wu's charm makes acceptable the plot idea of her old employer wanting her so much. She is an object of love, an object of desire -- a very believable concept!<br /><br />Fans of Vivian Wu may want to check out an outstanding B-movie she is in from a couple years back called DINNER RUSH. DINNER RUSH is highly recommended. ENCRYPT is not.\",\n",
        "    \"So the other night I decided to watch Tales from the Hollywood Hills: Natica Jackson. Or Power, Passion, Murder as it is called in Holland. When I bought the film I noticed that Michelle Pfeiffer was starring in it and I thought that had to say something about the quality. Unfortunately, it didn't.<br /><br />1) The plot of the film is really confusing. There are two story lines running simultaneously during the film. Only they have nothing in common. Throughout the entire movie I was waiting for the moment these two story lines would come together so the plot would be clear to me. But it still hasn't.<br /><br />2) The title of the film says the film will be about Natica Jackson. Well it is, sometimes. Like said the film covers two different stories and the part about Natica Jackson is the shortest. So another title for this movie would not be a wrong choice.<br /><br />To conclude my story, I really recommend that you leave this movie where it belongs, on the shelf in the store on a place nobody can see it. By doing this you won't waste 90 minutes of your life, as I did.\"         \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glpuB87bI1_X"
      },
      "source": [
        "self_attention_layer = SelfAttentionLayer(\n",
        "    embeddings=embeddings,\n",
        "    padding_idx=params['padding_idx'])\n",
        "\n",
        "self_attention_layer_loop = SelfAttentionLayerLoop(\n",
        "    embeddings=embeddings,\n",
        "    padding_idx=params['padding_idx'])\n",
        "\n",
        "batch_token_ids = []\n",
        "for example in examples:\n",
        "    token_ids = to_token_ids(\n",
        "        text=example,\n",
        "        vocab=vocab,\n",
        "        max_length=params['max_length'],\n",
        "        padding_idx=params['padding_idx'])\n",
        "    batch_token_ids.append(token_ids)\n",
        "\n",
        "batch_token_ids = torch.LongTensor(batch_token_ids)\n",
        "my_output = self_attention_layer(batch_token_ids)\n",
        "my_output_loop = self_attention_layer_loop(batch_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0VUv1A_r5oO"
      },
      "source": [
        "Fazemos o download do tensor esperado e o comparamos com nossa saída"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THva0R6h6A_e",
        "outputId": "7ee39a55-c5af-4a5a-f24e-ab5a2f1024b5"
      },
      "source": [
        "!gsutil cp gs://neuralresearcher_data/unicamp/ia376e_2021s2/aula7/target_tensor.pt target_tensor.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://neuralresearcher_data/unicamp/ia376e_2021s2/aula7/target_tensor.pt...\n",
            "- [1 files][ 10.1 KiB/ 10.1 KiB]                                                \n",
            "Operation completed over 1 objects/10.1 KiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2C1ZvxoAkCN"
      },
      "source": [
        "target_output = torch.load('target_tensor.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNPTGnCLGBl"
      },
      "source": [
        "assert torch.allclose(my_output, target_output, atol=1e-6)\n",
        "assert torch.allclose(my_output_loop, target_output, atol=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh7NNucM38wm"
      },
      "source": [
        "# Classificador\n",
        "\n",
        "Diferentemente dos outros exercícios, aqui iremos usar a camada de _self attention_ implementada acima com o intuito realizar uma classificação binária das avaliações de filmes entre positiva e negativa. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfJUUfCF4AJf"
      },
      "source": [
        "### Criação da classe Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meHfqut36gAX"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, x, y, tokenizer, vocab, max_length, padding_idx):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab = vocab\n",
        "    self.max_length = max_length\n",
        "    self.padding_idx = padding_idx\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.tokenizer(self.x[idx], self.vocab, self.max_length, self.padding_idx)\n",
        "    return torch.tensor(x).long(), torch.tensor(self.y[idx]).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirnjDaI4ssk"
      },
      "source": [
        "## Loops de treino, validação e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4dsfZpsQ0Y_",
        "outputId": "861ab60b-8aee-463f-b402-9c395344b0d1"
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN1sSxVzQ5N7"
      },
      "source": [
        "def train(model, train, valid, criterion, optimizer, filename_save, n_epochs=10, run=None, params=None):\n",
        "  \n",
        "  best_valid_loss = 10e9\n",
        "  best_epoch = 0\n",
        "  train_losses, valid_losses = [], []\n",
        "  if run:\n",
        "    run['parameters'] = params\n",
        "  for i in range(n_epochs):\n",
        "    accumulated_loss = 0\n",
        "    model.train()\n",
        "    for x_train, y_train in train:\n",
        "      x_train = x_train.to(device)\n",
        "      y_train = y_train.to(device).reshape(-1, 1)\n",
        "      outputs = model(x_train)\n",
        "      batch_loss = criterion(outputs, y_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      accumulated_loss += batch_loss.item()\n",
        "\n",
        "    train_loss = accumulated_loss / len(train.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Laço de Validação, um a cada época.\n",
        "    accumulated_loss = 0\n",
        "    accumulated_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x_valid, y_valid in valid:\n",
        "            x_valid = x_valid.to(device)\n",
        "            y_valid = y_valid.to(device).reshape(-1, 1)\n",
        "\n",
        "            # predict da rede\n",
        "            outputs = model(x_valid)\n",
        "\n",
        "            # calcula a perda\n",
        "            batch_loss = criterion(outputs, y_valid)\n",
        "            preds = outputs > 0.5\n",
        "            # preds = outputs.argmax(dim=1)\n",
        "\n",
        "            # calcula a acurácia\n",
        "            batch_accuracy = (preds == y_valid).sum()\n",
        "            accumulated_loss += batch_loss\n",
        "            accumulated_accuracy += batch_accuracy\n",
        "\n",
        "    valid_loss = accumulated_loss / len(valid.dataset)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    valid_acc = accumulated_accuracy / len(valid.dataset)\n",
        "\n",
        "    print(f'Época: {i:d}/{n_epochs - 1:d} Train Loss: {train_loss:.6f} Valid Loss: {valid_loss:.6f} Valid Acc: {valid_acc:.3f}')\n",
        "\n",
        "    if run:\n",
        "      run[f\"{filename_save}_valid/loss\"].log(valid_loss)\n",
        "      run[f\"{filename_save}_valid/acc\"].log(valid_acc)\n",
        "      run[f\"{filename_save}_train/loss\"].log(train_loss)\n",
        "\n",
        "\n",
        "    # Salvando o melhor modelo de acordo com a loss de validação\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), filename_save + '.pt')\n",
        "        best_valid_loss = valid_loss\n",
        "        best_epoch = i\n",
        "        print('best model')\n",
        "\n",
        "  return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I0eLbvERClx"
      },
      "source": [
        "def predict(model, state_dict, test, run=None):\n",
        "  accumulated_accuracy = 0\n",
        "  model.load_state_dict(torch.load(state_dict + '.pt'))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for x_test, y_test in test:\n",
        "          x_test = x_test.to(device)\n",
        "          y_test = y_test.to(device).reshape(-1,1)\n",
        "\n",
        "          # predict da rede\n",
        "          outputs = model(x_test)\n",
        "  \n",
        "          # calcula a perda\n",
        "          batch_loss = criterion(outputs, y_test)\n",
        "          preds = outputs > 0.5\n",
        "          # preds = outputs.argmax(dim=1)\n",
        "\n",
        "          # calcula a acurácia\n",
        "          batch_accuracy = (preds == y_test).sum()\n",
        "          accumulated_accuracy += batch_accuracy\n",
        "\n",
        "  test_acc = accumulated_accuracy / len(test.dataset)\n",
        "  test_acc *= 100\n",
        "  print('*' * 40)\n",
        "  print(f'Acurácia de {test_acc:.3f} %')\n",
        "  print('*' * 40)\n",
        "\n",
        "  if run:\n",
        "    run['results'] = test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk6YEgox4C8r"
      },
      "source": [
        "## Definição da rede\n",
        "\n",
        "Usarei uma rede onde temos uma camada de atenção seguida por duas camadas lineares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Igy1xNN-XA"
      },
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embeddings, padding_idx, size_lin1):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.self_attention = SelfAttentionLayer(embeddings, padding_idx)\n",
        "    self.lin1 = torch.nn.Linear(embeddings.shape[1], size_lin1)\n",
        "    self.lin2 = torch.nn.Linear(size_lin1, 1)\n",
        "    self.output = torch.nn.Sigmoid()\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.lin1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.lin2(x)\n",
        "    x = self.output(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSyY1VQFQE5U"
      },
      "source": [
        "## Experimento teste\n",
        "\n",
        "Utilizando poucos dados, irei rodar um experimento para validar a execução do fluxo de treino e validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJAXBUBm_FWK"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 300\n",
        "batch_size = 50\n",
        "hidden_size = 150\n",
        "filename = \"self_attention\"\n",
        "\n",
        "hprams = {\"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"hidden_size\": hidden_size\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuWqZpmY_Je-"
      },
      "source": [
        "dataset_train = Dataset(x_train[:50], y_train[:50], to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_valid = Dataset(x_valid[:50], y_valid[:50], to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_test = Dataset(x_test, y_test, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=hprams[\"batch_size\"], shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=hprams[\"batch_size\"], shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=hprams[\"batch_size\"], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDZ9ebSa_LPz",
        "outputId": "2386ed46-1080-4076-a1a9-60d217873f32"
      },
      "source": [
        "cls = Classifier(embeddings, params['padding_idx'], hprams[\"hidden_size\"])\n",
        "cls.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (self_attention): SelfAttentionLayer(\n",
              "    (embeddings): Embedding(400002, 300, padding_idx=400001)\n",
              "  )\n",
              "  (lin1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
              "  (output): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpohmcmb_Lwh"
      },
      "source": [
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(cls.parameters(), lr=hprams[\"learning_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhuhAjF7_O3b",
        "outputId": "7de6a53d-a636-4f7b-da93-a812fc763a3b"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(cls, dataloader_train, dataloader_valid, criterion,\n",
        "          optimizer, filename, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/299 Train Loss: 0.013848 Valid Loss: 0.015148 Valid Acc: 0.400\n",
            "best model\n",
            "Época: 1/299 Train Loss: 0.013766 Valid Loss: 0.013774 Valid Acc: 0.520\n",
            "best model\n",
            "Época: 2/299 Train Loss: 0.013324 Valid Loss: 0.013287 Valid Acc: 0.700\n",
            "best model\n",
            "Época: 3/299 Train Loss: 0.013136 Valid Loss: 0.013294 Valid Acc: 0.600\n",
            "Época: 4/299 Train Loss: 0.012693 Valid Loss: 0.013620 Valid Acc: 0.600\n",
            "Época: 5/299 Train Loss: 0.012276 Valid Loss: 0.013682 Valid Acc: 0.580\n",
            "Época: 6/299 Train Loss: 0.011854 Valid Loss: 0.013137 Valid Acc: 0.560\n",
            "best model\n",
            "Época: 7/299 Train Loss: 0.011270 Valid Loss: 0.012699 Valid Acc: 0.620\n",
            "best model\n",
            "Época: 8/299 Train Loss: 0.010783 Valid Loss: 0.012704 Valid Acc: 0.640\n",
            "Época: 9/299 Train Loss: 0.010191 Valid Loss: 0.013087 Valid Acc: 0.560\n",
            "Época: 10/299 Train Loss: 0.009659 Valid Loss: 0.013147 Valid Acc: 0.600\n",
            "Época: 11/299 Train Loss: 0.009132 Valid Loss: 0.012909 Valid Acc: 0.620\n",
            "Época: 12/299 Train Loss: 0.008622 Valid Loss: 0.013098 Valid Acc: 0.620\n",
            "Época: 13/299 Train Loss: 0.008152 Valid Loss: 0.013694 Valid Acc: 0.640\n",
            "Época: 14/299 Train Loss: 0.007676 Valid Loss: 0.014015 Valid Acc: 0.660\n",
            "Época: 15/299 Train Loss: 0.007226 Valid Loss: 0.013977 Valid Acc: 0.660\n",
            "Época: 16/299 Train Loss: 0.006774 Valid Loss: 0.014294 Valid Acc: 0.680\n",
            "Época: 17/299 Train Loss: 0.006310 Valid Loss: 0.014892 Valid Acc: 0.640\n",
            "Época: 18/299 Train Loss: 0.005866 Valid Loss: 0.014928 Valid Acc: 0.660\n",
            "Época: 19/299 Train Loss: 0.005384 Valid Loss: 0.014908 Valid Acc: 0.660\n",
            "Época: 20/299 Train Loss: 0.004960 Valid Loss: 0.015477 Valid Acc: 0.660\n",
            "Época: 21/299 Train Loss: 0.004509 Valid Loss: 0.015748 Valid Acc: 0.680\n",
            "Época: 22/299 Train Loss: 0.004107 Valid Loss: 0.015555 Valid Acc: 0.680\n",
            "Época: 23/299 Train Loss: 0.003753 Valid Loss: 0.016240 Valid Acc: 0.700\n",
            "Época: 24/299 Train Loss: 0.003402 Valid Loss: 0.016722 Valid Acc: 0.700\n",
            "Época: 25/299 Train Loss: 0.003110 Valid Loss: 0.016548 Valid Acc: 0.700\n",
            "Época: 26/299 Train Loss: 0.002848 Valid Loss: 0.017470 Valid Acc: 0.700\n",
            "Época: 27/299 Train Loss: 0.002578 Valid Loss: 0.018043 Valid Acc: 0.680\n",
            "Época: 28/299 Train Loss: 0.002338 Valid Loss: 0.017981 Valid Acc: 0.700\n",
            "Época: 29/299 Train Loss: 0.002109 Valid Loss: 0.018987 Valid Acc: 0.680\n",
            "Época: 30/299 Train Loss: 0.001870 Valid Loss: 0.019733 Valid Acc: 0.680\n",
            "Época: 31/299 Train Loss: 0.001657 Valid Loss: 0.019808 Valid Acc: 0.700\n",
            "Época: 32/299 Train Loss: 0.001455 Valid Loss: 0.020684 Valid Acc: 0.660\n",
            "Época: 33/299 Train Loss: 0.001265 Valid Loss: 0.021743 Valid Acc: 0.660\n",
            "Época: 34/299 Train Loss: 0.001106 Valid Loss: 0.022068 Valid Acc: 0.660\n",
            "Época: 35/299 Train Loss: 0.000959 Valid Loss: 0.022619 Valid Acc: 0.680\n",
            "Época: 36/299 Train Loss: 0.000842 Valid Loss: 0.023714 Valid Acc: 0.660\n",
            "Época: 37/299 Train Loss: 0.000737 Valid Loss: 0.024538 Valid Acc: 0.660\n",
            "Época: 38/299 Train Loss: 0.000654 Valid Loss: 0.024896 Valid Acc: 0.660\n",
            "Época: 39/299 Train Loss: 0.000580 Valid Loss: 0.025460 Valid Acc: 0.660\n",
            "Época: 40/299 Train Loss: 0.000520 Valid Loss: 0.026364 Valid Acc: 0.660\n",
            "Época: 41/299 Train Loss: 0.000465 Valid Loss: 0.027140 Valid Acc: 0.660\n",
            "Época: 42/299 Train Loss: 0.000420 Valid Loss: 0.027521 Valid Acc: 0.660\n",
            "Época: 43/299 Train Loss: 0.000378 Valid Loss: 0.027841 Valid Acc: 0.660\n",
            "Época: 44/299 Train Loss: 0.000342 Valid Loss: 0.028390 Valid Acc: 0.660\n",
            "Época: 45/299 Train Loss: 0.000309 Valid Loss: 0.029078 Valid Acc: 0.660\n",
            "Época: 46/299 Train Loss: 0.000280 Valid Loss: 0.029627 Valid Acc: 0.680\n",
            "Época: 47/299 Train Loss: 0.000254 Valid Loss: 0.029905 Valid Acc: 0.680\n",
            "Época: 48/299 Train Loss: 0.000231 Valid Loss: 0.030100 Valid Acc: 0.680\n",
            "Época: 49/299 Train Loss: 0.000211 Valid Loss: 0.030430 Valid Acc: 0.680\n",
            "Época: 50/299 Train Loss: 0.000193 Valid Loss: 0.030907 Valid Acc: 0.680\n",
            "Época: 51/299 Train Loss: 0.000177 Valid Loss: 0.031393 Valid Acc: 0.680\n",
            "Época: 52/299 Train Loss: 0.000164 Valid Loss: 0.031746 Valid Acc: 0.680\n",
            "Época: 53/299 Train Loss: 0.000152 Valid Loss: 0.031976 Valid Acc: 0.680\n",
            "Época: 54/299 Train Loss: 0.000141 Valid Loss: 0.032167 Valid Acc: 0.680\n",
            "Época: 55/299 Train Loss: 0.000132 Valid Loss: 0.032332 Valid Acc: 0.660\n",
            "Época: 56/299 Train Loss: 0.000124 Valid Loss: 0.032678 Valid Acc: 0.660\n",
            "Época: 57/299 Train Loss: 0.000116 Valid Loss: 0.033111 Valid Acc: 0.680\n",
            "Época: 58/299 Train Loss: 0.000109 Valid Loss: 0.033419 Valid Acc: 0.680\n",
            "Época: 59/299 Train Loss: 0.000104 Valid Loss: 0.033813 Valid Acc: 0.680\n",
            "Época: 60/299 Train Loss: 0.000098 Valid Loss: 0.033973 Valid Acc: 0.680\n",
            "Época: 61/299 Train Loss: 0.000093 Valid Loss: 0.034093 Valid Acc: 0.660\n",
            "Época: 62/299 Train Loss: 0.000089 Valid Loss: 0.034226 Valid Acc: 0.660\n",
            "Época: 63/299 Train Loss: 0.000085 Valid Loss: 0.034412 Valid Acc: 0.660\n",
            "Época: 64/299 Train Loss: 0.000081 Valid Loss: 0.034931 Valid Acc: 0.660\n",
            "Época: 65/299 Train Loss: 0.000078 Valid Loss: 0.035181 Valid Acc: 0.660\n",
            "Época: 66/299 Train Loss: 0.000075 Valid Loss: 0.035419 Valid Acc: 0.660\n",
            "Época: 67/299 Train Loss: 0.000072 Valid Loss: 0.035607 Valid Acc: 0.660\n",
            "Época: 68/299 Train Loss: 0.000069 Valid Loss: 0.035758 Valid Acc: 0.660\n",
            "Época: 69/299 Train Loss: 0.000067 Valid Loss: 0.035871 Valid Acc: 0.660\n",
            "Época: 70/299 Train Loss: 0.000065 Valid Loss: 0.035987 Valid Acc: 0.660\n",
            "Época: 71/299 Train Loss: 0.000062 Valid Loss: 0.036143 Valid Acc: 0.660\n",
            "Época: 72/299 Train Loss: 0.000061 Valid Loss: 0.036306 Valid Acc: 0.660\n",
            "Época: 73/299 Train Loss: 0.000059 Valid Loss: 0.036509 Valid Acc: 0.660\n",
            "Época: 74/299 Train Loss: 0.000057 Valid Loss: 0.036692 Valid Acc: 0.660\n",
            "Época: 75/299 Train Loss: 0.000055 Valid Loss: 0.070482 Valid Acc: 0.660\n",
            "Época: 76/299 Train Loss: 0.000054 Valid Loss: 0.070625 Valid Acc: 0.660\n",
            "Época: 77/299 Train Loss: 0.000053 Valid Loss: 0.070748 Valid Acc: 0.660\n",
            "Época: 78/299 Train Loss: 0.000051 Valid Loss: 0.070861 Valid Acc: 0.660\n",
            "Época: 79/299 Train Loss: 0.000050 Valid Loss: 0.070943 Valid Acc: 0.660\n",
            "Época: 80/299 Train Loss: 0.000049 Valid Loss: 0.071071 Valid Acc: 0.660\n",
            "Época: 81/299 Train Loss: 0.000048 Valid Loss: 0.071215 Valid Acc: 0.660\n",
            "Época: 82/299 Train Loss: 0.000047 Valid Loss: 0.071330 Valid Acc: 0.660\n",
            "Época: 83/299 Train Loss: 0.000046 Valid Loss: 0.071488 Valid Acc: 0.660\n",
            "Época: 84/299 Train Loss: 0.000045 Valid Loss: 0.071597 Valid Acc: 0.660\n",
            "Época: 85/299 Train Loss: 0.000044 Valid Loss: 0.071739 Valid Acc: 0.660\n",
            "Época: 86/299 Train Loss: 0.000043 Valid Loss: 0.071818 Valid Acc: 0.660\n",
            "Época: 87/299 Train Loss: 0.000042 Valid Loss: 0.071941 Valid Acc: 0.660\n",
            "Época: 88/299 Train Loss: 0.000041 Valid Loss: 0.072006 Valid Acc: 0.660\n",
            "Época: 89/299 Train Loss: 0.000041 Valid Loss: 0.072074 Valid Acc: 0.660\n",
            "Época: 90/299 Train Loss: 0.000040 Valid Loss: 0.072148 Valid Acc: 0.660\n",
            "Época: 91/299 Train Loss: 0.000039 Valid Loss: 0.072291 Valid Acc: 0.660\n",
            "Época: 92/299 Train Loss: 0.000039 Valid Loss: 0.072377 Valid Acc: 0.660\n",
            "Época: 93/299 Train Loss: 0.000038 Valid Loss: 0.072462 Valid Acc: 0.660\n",
            "Época: 94/299 Train Loss: 0.000037 Valid Loss: 0.072615 Valid Acc: 0.660\n",
            "Época: 95/299 Train Loss: 0.000037 Valid Loss: 0.072686 Valid Acc: 0.660\n",
            "Época: 96/299 Train Loss: 0.000036 Valid Loss: 0.072748 Valid Acc: 0.660\n",
            "Época: 97/299 Train Loss: 0.000036 Valid Loss: 0.072805 Valid Acc: 0.660\n",
            "Época: 98/299 Train Loss: 0.000035 Valid Loss: 0.072859 Valid Acc: 0.660\n",
            "Época: 99/299 Train Loss: 0.000035 Valid Loss: 0.072914 Valid Acc: 0.660\n",
            "Época: 100/299 Train Loss: 0.000034 Valid Loss: 0.072973 Valid Acc: 0.660\n",
            "Época: 101/299 Train Loss: 0.000034 Valid Loss: 0.073126 Valid Acc: 0.660\n",
            "Época: 102/299 Train Loss: 0.000033 Valid Loss: 0.073192 Valid Acc: 0.660\n",
            "Época: 103/299 Train Loss: 0.000033 Valid Loss: 0.073259 Valid Acc: 0.660\n",
            "Época: 104/299 Train Loss: 0.000032 Valid Loss: 0.073323 Valid Acc: 0.660\n",
            "Época: 105/299 Train Loss: 0.000032 Valid Loss: 0.073382 Valid Acc: 0.660\n",
            "Época: 106/299 Train Loss: 0.000031 Valid Loss: 0.073437 Valid Acc: 0.660\n",
            "Época: 107/299 Train Loss: 0.000031 Valid Loss: 0.073487 Valid Acc: 0.660\n",
            "Época: 108/299 Train Loss: 0.000030 Valid Loss: 0.073535 Valid Acc: 0.660\n",
            "Época: 109/299 Train Loss: 0.000030 Valid Loss: 0.073698 Valid Acc: 0.660\n",
            "Época: 110/299 Train Loss: 0.000030 Valid Loss: 0.073748 Valid Acc: 0.660\n",
            "Época: 111/299 Train Loss: 0.000029 Valid Loss: 0.073800 Valid Acc: 0.660\n",
            "Época: 112/299 Train Loss: 0.000029 Valid Loss: 0.073855 Valid Acc: 0.660\n",
            "Época: 113/299 Train Loss: 0.000029 Valid Loss: 0.073911 Valid Acc: 0.660\n",
            "Época: 114/299 Train Loss: 0.000028 Valid Loss: 0.073966 Valid Acc: 0.660\n",
            "Época: 115/299 Train Loss: 0.000028 Valid Loss: 0.074020 Valid Acc: 0.660\n",
            "Época: 116/299 Train Loss: 0.000027 Valid Loss: 0.074070 Valid Acc: 0.660\n",
            "Época: 117/299 Train Loss: 0.000027 Valid Loss: 0.074118 Valid Acc: 0.660\n",
            "Época: 118/299 Train Loss: 0.000027 Valid Loss: 0.074163 Valid Acc: 0.660\n",
            "Época: 119/299 Train Loss: 0.000026 Valid Loss: 0.074208 Valid Acc: 0.660\n",
            "Época: 120/299 Train Loss: 0.000026 Valid Loss: 0.074253 Valid Acc: 0.660\n",
            "Época: 121/299 Train Loss: 0.000026 Valid Loss: 0.074299 Valid Acc: 0.660\n",
            "Época: 122/299 Train Loss: 0.000026 Valid Loss: 0.074509 Valid Acc: 0.660\n",
            "Época: 123/299 Train Loss: 0.000025 Valid Loss: 0.074558 Valid Acc: 0.660\n",
            "Época: 124/299 Train Loss: 0.000025 Valid Loss: 0.074608 Valid Acc: 0.660\n",
            "Época: 125/299 Train Loss: 0.000025 Valid Loss: 0.074657 Valid Acc: 0.660\n",
            "Época: 126/299 Train Loss: 0.000024 Valid Loss: 0.074705 Valid Acc: 0.660\n",
            "Época: 127/299 Train Loss: 0.000024 Valid Loss: 0.074751 Valid Acc: 0.660\n",
            "Época: 128/299 Train Loss: 0.000024 Valid Loss: 0.074796 Valid Acc: 0.660\n",
            "Época: 129/299 Train Loss: 0.000024 Valid Loss: 0.074839 Valid Acc: 0.660\n",
            "Época: 130/299 Train Loss: 0.000023 Valid Loss: 0.074882 Valid Acc: 0.660\n",
            "Época: 131/299 Train Loss: 0.000023 Valid Loss: 0.074926 Valid Acc: 0.660\n",
            "Época: 132/299 Train Loss: 0.000023 Valid Loss: 0.074970 Valid Acc: 0.660\n",
            "Época: 133/299 Train Loss: 0.000023 Valid Loss: 0.075014 Valid Acc: 0.660\n",
            "Época: 134/299 Train Loss: 0.000022 Valid Loss: 0.075059 Valid Acc: 0.660\n",
            "Época: 135/299 Train Loss: 0.000022 Valid Loss: 0.075104 Valid Acc: 0.660\n",
            "Época: 136/299 Train Loss: 0.000022 Valid Loss: 0.075149 Valid Acc: 0.660\n",
            "Época: 137/299 Train Loss: 0.000022 Valid Loss: 0.075193 Valid Acc: 0.660\n",
            "Época: 138/299 Train Loss: 0.000021 Valid Loss: 0.075236 Valid Acc: 0.660\n",
            "Época: 139/299 Train Loss: 0.000021 Valid Loss: 0.075279 Valid Acc: 0.660\n",
            "Época: 140/299 Train Loss: 0.000021 Valid Loss: 0.075322 Valid Acc: 0.660\n",
            "Época: 141/299 Train Loss: 0.000021 Valid Loss: 0.075364 Valid Acc: 0.660\n",
            "Época: 142/299 Train Loss: 0.000021 Valid Loss: 0.075405 Valid Acc: 0.660\n",
            "Época: 143/299 Train Loss: 0.000020 Valid Loss: 0.075447 Valid Acc: 0.660\n",
            "Época: 144/299 Train Loss: 0.000020 Valid Loss: 0.075488 Valid Acc: 0.660\n",
            "Época: 145/299 Train Loss: 0.000020 Valid Loss: 0.075807 Valid Acc: 0.660\n",
            "Época: 146/299 Train Loss: 0.000020 Valid Loss: 0.075848 Valid Acc: 0.660\n",
            "Época: 147/299 Train Loss: 0.000020 Valid Loss: 0.075890 Valid Acc: 0.660\n",
            "Época: 148/299 Train Loss: 0.000019 Valid Loss: 0.075931 Valid Acc: 0.660\n",
            "Época: 149/299 Train Loss: 0.000019 Valid Loss: 0.075973 Valid Acc: 0.660\n",
            "Época: 150/299 Train Loss: 0.000019 Valid Loss: 0.076014 Valid Acc: 0.660\n",
            "Época: 151/299 Train Loss: 0.000019 Valid Loss: 0.076054 Valid Acc: 0.660\n",
            "Época: 152/299 Train Loss: 0.000019 Valid Loss: 0.076095 Valid Acc: 0.660\n",
            "Época: 153/299 Train Loss: 0.000018 Valid Loss: 0.076134 Valid Acc: 0.660\n",
            "Época: 154/299 Train Loss: 0.000018 Valid Loss: 0.076173 Valid Acc: 0.660\n",
            "Época: 155/299 Train Loss: 0.000018 Valid Loss: 0.076213 Valid Acc: 0.660\n",
            "Época: 156/299 Train Loss: 0.000018 Valid Loss: 0.076252 Valid Acc: 0.660\n",
            "Época: 157/299 Train Loss: 0.000018 Valid Loss: 0.076291 Valid Acc: 0.660\n",
            "Época: 158/299 Train Loss: 0.000018 Valid Loss: 0.076330 Valid Acc: 0.660\n",
            "Época: 159/299 Train Loss: 0.000017 Valid Loss: 0.076370 Valid Acc: 0.660\n",
            "Época: 160/299 Train Loss: 0.000017 Valid Loss: 0.076409 Valid Acc: 0.660\n",
            "Época: 161/299 Train Loss: 0.000017 Valid Loss: 0.076448 Valid Acc: 0.660\n",
            "Época: 162/299 Train Loss: 0.000017 Valid Loss: 0.076486 Valid Acc: 0.660\n",
            "Época: 163/299 Train Loss: 0.000017 Valid Loss: 0.076524 Valid Acc: 0.660\n",
            "Época: 164/299 Train Loss: 0.000017 Valid Loss: 0.076561 Valid Acc: 0.660\n",
            "Época: 165/299 Train Loss: 0.000016 Valid Loss: 0.076599 Valid Acc: 0.660\n",
            "Época: 166/299 Train Loss: 0.000016 Valid Loss: 0.076635 Valid Acc: 0.660\n",
            "Época: 167/299 Train Loss: 0.000016 Valid Loss: 0.076672 Valid Acc: 0.660\n",
            "Época: 168/299 Train Loss: 0.000016 Valid Loss: 0.076709 Valid Acc: 0.660\n",
            "Época: 169/299 Train Loss: 0.000016 Valid Loss: 0.076746 Valid Acc: 0.660\n",
            "Época: 170/299 Train Loss: 0.000016 Valid Loss: 0.076783 Valid Acc: 0.660\n",
            "Época: 171/299 Train Loss: 0.000016 Valid Loss: 0.076820 Valid Acc: 0.660\n",
            "Época: 172/299 Train Loss: 0.000015 Valid Loss: 0.076857 Valid Acc: 0.660\n",
            "Época: 173/299 Train Loss: 0.000015 Valid Loss: 0.076894 Valid Acc: 0.660\n",
            "Época: 174/299 Train Loss: 0.000015 Valid Loss: 0.076930 Valid Acc: 0.660\n",
            "Época: 175/299 Train Loss: 0.000015 Valid Loss: 0.076966 Valid Acc: 0.660\n",
            "Época: 176/299 Train Loss: 0.000015 Valid Loss: 0.077002 Valid Acc: 0.660\n",
            "Época: 177/299 Train Loss: 0.000015 Valid Loss: 0.077037 Valid Acc: 0.660\n",
            "Época: 178/299 Train Loss: 0.000015 Valid Loss: 0.077072 Valid Acc: 0.660\n",
            "Época: 179/299 Train Loss: 0.000015 Valid Loss: 0.077107 Valid Acc: 0.660\n",
            "Época: 180/299 Train Loss: 0.000014 Valid Loss: 0.077142 Valid Acc: 0.660\n",
            "Época: 181/299 Train Loss: 0.000014 Valid Loss: 0.077176 Valid Acc: 0.660\n",
            "Época: 182/299 Train Loss: 0.000014 Valid Loss: 0.077211 Valid Acc: 0.660\n",
            "Época: 183/299 Train Loss: 0.000014 Valid Loss: 0.077245 Valid Acc: 0.660\n",
            "Época: 184/299 Train Loss: 0.000014 Valid Loss: 0.077280 Valid Acc: 0.660\n",
            "Época: 185/299 Train Loss: 0.000014 Valid Loss: 0.077314 Valid Acc: 0.660\n",
            "Época: 186/299 Train Loss: 0.000014 Valid Loss: 0.077348 Valid Acc: 0.660\n",
            "Época: 187/299 Train Loss: 0.000014 Valid Loss: 0.077382 Valid Acc: 0.660\n",
            "Época: 188/299 Train Loss: 0.000013 Valid Loss: 0.077416 Valid Acc: 0.660\n",
            "Época: 189/299 Train Loss: 0.000013 Valid Loss: 0.077450 Valid Acc: 0.660\n",
            "Época: 190/299 Train Loss: 0.000013 Valid Loss: 0.077484 Valid Acc: 0.660\n",
            "Época: 191/299 Train Loss: 0.000013 Valid Loss: 0.077517 Valid Acc: 0.660\n",
            "Época: 192/299 Train Loss: 0.000013 Valid Loss: 0.077550 Valid Acc: 0.660\n",
            "Época: 193/299 Train Loss: 0.000013 Valid Loss: 0.077583 Valid Acc: 0.660\n",
            "Época: 194/299 Train Loss: 0.000013 Valid Loss: 0.077615 Valid Acc: 0.660\n",
            "Época: 195/299 Train Loss: 0.000013 Valid Loss: 0.077648 Valid Acc: 0.660\n",
            "Época: 196/299 Train Loss: 0.000013 Valid Loss: 0.077680 Valid Acc: 0.660\n",
            "Época: 197/299 Train Loss: 0.000013 Valid Loss: 0.077712 Valid Acc: 0.660\n",
            "Época: 198/299 Train Loss: 0.000012 Valid Loss: 0.077744 Valid Acc: 0.660\n",
            "Época: 199/299 Train Loss: 0.000012 Valid Loss: 0.077776 Valid Acc: 0.660\n",
            "Época: 200/299 Train Loss: 0.000012 Valid Loss: 0.077809 Valid Acc: 0.660\n",
            "Época: 201/299 Train Loss: 0.000012 Valid Loss: 0.077841 Valid Acc: 0.660\n",
            "Época: 202/299 Train Loss: 0.000012 Valid Loss: 0.077873 Valid Acc: 0.660\n",
            "Época: 203/299 Train Loss: 0.000012 Valid Loss: 0.077905 Valid Acc: 0.660\n",
            "Época: 204/299 Train Loss: 0.000012 Valid Loss: 0.077937 Valid Acc: 0.660\n",
            "Época: 205/299 Train Loss: 0.000012 Valid Loss: 0.077968 Valid Acc: 0.660\n",
            "Época: 206/299 Train Loss: 0.000012 Valid Loss: 0.111623 Valid Acc: 0.660\n",
            "Época: 207/299 Train Loss: 0.000012 Valid Loss: 0.111654 Valid Acc: 0.660\n",
            "Época: 208/299 Train Loss: 0.000012 Valid Loss: 0.111685 Valid Acc: 0.660\n",
            "Época: 209/299 Train Loss: 0.000011 Valid Loss: 0.111715 Valid Acc: 0.660\n",
            "Época: 210/299 Train Loss: 0.000011 Valid Loss: 0.111746 Valid Acc: 0.660\n",
            "Época: 211/299 Train Loss: 0.000011 Valid Loss: 0.111776 Valid Acc: 0.660\n",
            "Época: 212/299 Train Loss: 0.000011 Valid Loss: 0.111806 Valid Acc: 0.660\n",
            "Época: 213/299 Train Loss: 0.000011 Valid Loss: 0.111836 Valid Acc: 0.660\n",
            "Época: 214/299 Train Loss: 0.000011 Valid Loss: 0.111866 Valid Acc: 0.660\n",
            "Época: 215/299 Train Loss: 0.000011 Valid Loss: 0.111896 Valid Acc: 0.660\n",
            "Época: 216/299 Train Loss: 0.000011 Valid Loss: 0.111927 Valid Acc: 0.660\n",
            "Época: 217/299 Train Loss: 0.000011 Valid Loss: 0.111958 Valid Acc: 0.660\n",
            "Época: 218/299 Train Loss: 0.000011 Valid Loss: 0.111988 Valid Acc: 0.660\n",
            "Época: 219/299 Train Loss: 0.000011 Valid Loss: 0.112018 Valid Acc: 0.660\n",
            "Época: 220/299 Train Loss: 0.000011 Valid Loss: 0.112047 Valid Acc: 0.660\n",
            "Época: 221/299 Train Loss: 0.000010 Valid Loss: 0.112076 Valid Acc: 0.660\n",
            "Época: 222/299 Train Loss: 0.000010 Valid Loss: 0.112105 Valid Acc: 0.660\n",
            "Época: 223/299 Train Loss: 0.000010 Valid Loss: 0.112134 Valid Acc: 0.660\n",
            "Época: 224/299 Train Loss: 0.000010 Valid Loss: 0.112163 Valid Acc: 0.660\n",
            "Época: 225/299 Train Loss: 0.000010 Valid Loss: 0.112192 Valid Acc: 0.660\n",
            "Época: 226/299 Train Loss: 0.000010 Valid Loss: 0.112222 Valid Acc: 0.660\n",
            "Época: 227/299 Train Loss: 0.000010 Valid Loss: 0.112250 Valid Acc: 0.660\n",
            "Época: 228/299 Train Loss: 0.000010 Valid Loss: 0.112279 Valid Acc: 0.660\n",
            "Época: 229/299 Train Loss: 0.000010 Valid Loss: 0.112307 Valid Acc: 0.660\n",
            "Época: 230/299 Train Loss: 0.000010 Valid Loss: 0.112336 Valid Acc: 0.660\n",
            "Época: 231/299 Train Loss: 0.000010 Valid Loss: 0.112363 Valid Acc: 0.660\n",
            "Época: 232/299 Train Loss: 0.000010 Valid Loss: 0.112391 Valid Acc: 0.660\n",
            "Época: 233/299 Train Loss: 0.000010 Valid Loss: 0.112419 Valid Acc: 0.660\n",
            "Época: 234/299 Train Loss: 0.000010 Valid Loss: 0.112448 Valid Acc: 0.660\n",
            "Época: 235/299 Train Loss: 0.000010 Valid Loss: 0.112476 Valid Acc: 0.660\n",
            "Época: 236/299 Train Loss: 0.000009 Valid Loss: 0.112505 Valid Acc: 0.660\n",
            "Época: 237/299 Train Loss: 0.000009 Valid Loss: 0.112533 Valid Acc: 0.660\n",
            "Época: 238/299 Train Loss: 0.000009 Valid Loss: 0.112561 Valid Acc: 0.660\n",
            "Época: 239/299 Train Loss: 0.000009 Valid Loss: 0.112588 Valid Acc: 0.660\n",
            "Época: 240/299 Train Loss: 0.000009 Valid Loss: 0.112615 Valid Acc: 0.660\n",
            "Época: 241/299 Train Loss: 0.000009 Valid Loss: 0.112642 Valid Acc: 0.660\n",
            "Época: 242/299 Train Loss: 0.000009 Valid Loss: 0.112668 Valid Acc: 0.660\n",
            "Época: 243/299 Train Loss: 0.000009 Valid Loss: 0.112696 Valid Acc: 0.660\n",
            "Época: 244/299 Train Loss: 0.000009 Valid Loss: 0.112723 Valid Acc: 0.660\n",
            "Época: 245/299 Train Loss: 0.000009 Valid Loss: 0.112751 Valid Acc: 0.660\n",
            "Época: 246/299 Train Loss: 0.000009 Valid Loss: 0.112777 Valid Acc: 0.660\n",
            "Época: 247/299 Train Loss: 0.000009 Valid Loss: 0.112804 Valid Acc: 0.660\n",
            "Época: 248/299 Train Loss: 0.000009 Valid Loss: 0.112831 Valid Acc: 0.660\n",
            "Época: 249/299 Train Loss: 0.000009 Valid Loss: 0.112857 Valid Acc: 0.660\n",
            "Época: 250/299 Train Loss: 0.000009 Valid Loss: 0.112884 Valid Acc: 0.660\n",
            "Época: 251/299 Train Loss: 0.000009 Valid Loss: 0.112910 Valid Acc: 0.660\n",
            "Época: 252/299 Train Loss: 0.000009 Valid Loss: 0.112937 Valid Acc: 0.660\n",
            "Época: 253/299 Train Loss: 0.000008 Valid Loss: 0.112963 Valid Acc: 0.660\n",
            "Época: 254/299 Train Loss: 0.000008 Valid Loss: 0.112989 Valid Acc: 0.660\n",
            "Época: 255/299 Train Loss: 0.000008 Valid Loss: 0.113015 Valid Acc: 0.660\n",
            "Época: 256/299 Train Loss: 0.000008 Valid Loss: 0.113041 Valid Acc: 0.660\n",
            "Época: 257/299 Train Loss: 0.000008 Valid Loss: 0.113066 Valid Acc: 0.660\n",
            "Época: 258/299 Train Loss: 0.000008 Valid Loss: 0.113092 Valid Acc: 0.660\n",
            "Época: 259/299 Train Loss: 0.000008 Valid Loss: 0.113118 Valid Acc: 0.660\n",
            "Época: 260/299 Train Loss: 0.000008 Valid Loss: 0.113144 Valid Acc: 0.660\n",
            "Época: 261/299 Train Loss: 0.000008 Valid Loss: 0.113169 Valid Acc: 0.660\n",
            "Época: 262/299 Train Loss: 0.000008 Valid Loss: 0.113195 Valid Acc: 0.660\n",
            "Época: 263/299 Train Loss: 0.000008 Valid Loss: 0.113219 Valid Acc: 0.660\n",
            "Época: 264/299 Train Loss: 0.000008 Valid Loss: 0.113245 Valid Acc: 0.660\n",
            "Época: 265/299 Train Loss: 0.000008 Valid Loss: 0.113270 Valid Acc: 0.660\n",
            "Época: 266/299 Train Loss: 0.000008 Valid Loss: 0.113296 Valid Acc: 0.660\n",
            "Época: 267/299 Train Loss: 0.000008 Valid Loss: 0.113321 Valid Acc: 0.660\n",
            "Época: 268/299 Train Loss: 0.000008 Valid Loss: 0.113346 Valid Acc: 0.660\n",
            "Época: 269/299 Train Loss: 0.000008 Valid Loss: 0.113371 Valid Acc: 0.660\n",
            "Época: 270/299 Train Loss: 0.000008 Valid Loss: 0.113395 Valid Acc: 0.660\n",
            "Época: 271/299 Train Loss: 0.000008 Valid Loss: 0.113420 Valid Acc: 0.660\n",
            "Época: 272/299 Train Loss: 0.000008 Valid Loss: 0.113444 Valid Acc: 0.660\n",
            "Época: 273/299 Train Loss: 0.000007 Valid Loss: 0.113468 Valid Acc: 0.660\n",
            "Época: 274/299 Train Loss: 0.000007 Valid Loss: 0.113493 Valid Acc: 0.660\n",
            "Época: 275/299 Train Loss: 0.000007 Valid Loss: 0.113517 Valid Acc: 0.660\n",
            "Época: 276/299 Train Loss: 0.000007 Valid Loss: 0.113542 Valid Acc: 0.660\n",
            "Época: 277/299 Train Loss: 0.000007 Valid Loss: 0.113566 Valid Acc: 0.660\n",
            "Época: 278/299 Train Loss: 0.000007 Valid Loss: 0.113590 Valid Acc: 0.660\n",
            "Época: 279/299 Train Loss: 0.000007 Valid Loss: 0.113615 Valid Acc: 0.660\n",
            "Época: 280/299 Train Loss: 0.000007 Valid Loss: 0.113639 Valid Acc: 0.660\n",
            "Época: 281/299 Train Loss: 0.000007 Valid Loss: 0.113664 Valid Acc: 0.660\n",
            "Época: 282/299 Train Loss: 0.000007 Valid Loss: 0.113688 Valid Acc: 0.660\n",
            "Época: 283/299 Train Loss: 0.000007 Valid Loss: 0.113711 Valid Acc: 0.660\n",
            "Época: 284/299 Train Loss: 0.000007 Valid Loss: 0.113734 Valid Acc: 0.660\n",
            "Época: 285/299 Train Loss: 0.000007 Valid Loss: 0.113757 Valid Acc: 0.660\n",
            "Época: 286/299 Train Loss: 0.000007 Valid Loss: 0.113780 Valid Acc: 0.660\n",
            "Época: 287/299 Train Loss: 0.000007 Valid Loss: 0.113803 Valid Acc: 0.660\n",
            "Época: 288/299 Train Loss: 0.000007 Valid Loss: 0.113827 Valid Acc: 0.660\n",
            "Época: 289/299 Train Loss: 0.000007 Valid Loss: 0.113851 Valid Acc: 0.660\n",
            "Época: 290/299 Train Loss: 0.000007 Valid Loss: 0.113875 Valid Acc: 0.660\n",
            "Época: 291/299 Train Loss: 0.000007 Valid Loss: 0.113897 Valid Acc: 0.660\n",
            "Época: 292/299 Train Loss: 0.000007 Valid Loss: 0.113921 Valid Acc: 0.660\n",
            "Época: 293/299 Train Loss: 0.000007 Valid Loss: 0.113945 Valid Acc: 0.660\n",
            "Época: 294/299 Train Loss: 0.000007 Valid Loss: 0.113968 Valid Acc: 0.660\n",
            "Época: 295/299 Train Loss: 0.000007 Valid Loss: 0.113992 Valid Acc: 0.660\n",
            "Época: 296/299 Train Loss: 0.000007 Valid Loss: 0.114015 Valid Acc: 0.660\n",
            "Época: 297/299 Train Loss: 0.000007 Valid Loss: 0.114036 Valid Acc: 0.660\n",
            "Época: 298/299 Train Loss: 0.000006 Valid Loss: 0.114059 Valid Acc: 0.660\n",
            "Época: 299/299 Train Loss: 0.000006 Valid Loss: 0.114080 Valid Acc: 0.660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BglT68Vpdwj-",
        "outputId": "f5185bd7-4d36-4ca6-8d0a-1296f929f3fd"
      },
      "source": [
        "accumulated_accuracy = 0\n",
        "cls.eval()\n",
        "with torch.no_grad():\n",
        "    for x_test, y_test in dataloader_train:\n",
        "        x_test = x_test.to(device)\n",
        "        y_test = y_test.to(device).reshape(-1,1)\n",
        "\n",
        "        # predict da rede\n",
        "        outputs = cls(x_test)\n",
        "\n",
        "        # calcula a perda\n",
        "        batch_loss = criterion(outputs, y_test)\n",
        "        preds = outputs > 0.5\n",
        "        # preds = outputs.argmax(dim=1)\n",
        "\n",
        "        # calcula a acurácia\n",
        "        batch_accuracy = (preds == y_test).sum()\n",
        "        accumulated_accuracy += batch_accuracy\n",
        "\n",
        "test_acc = accumulated_accuracy / len(dataloader_train.dataset)\n",
        "test_acc *= 100\n",
        "print('*' * 40)\n",
        "print(f'Acurácia de {test_acc:.3f} %')\n",
        "print('*' * 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 100.000 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrlY7L-W7etN"
      },
      "source": [
        "## Experimento final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb2XDD7z7h_4",
        "outputId": "1381f7c7-9ae4-4b23-e225-e9d1fadd3978"
      },
      "source": [
        "run = neptune.init(\n",
        "    project=\"pedro.gengo/IA-376\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZjYyNDA1MS1hZDJlLTRiZDctYjIxNy0xMTNhY2FmNzZhYmIifQ==\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/pedro.gengo/IA-376/e/IA-21\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWNK9LjrQbi3"
      },
      "source": [
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "hidden_size = 150\n",
        "filename = \"self_attention\"\n",
        "\n",
        "hprams = {\"learning_rate\": learning_rate,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"hidden_size\": hidden_size\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0h1668xPl-k"
      },
      "source": [
        "dataset_train = Dataset(x_train, y_train, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_valid = Dataset(x_valid, y_valid, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "dataset_test = Dataset(x_test, y_test, to_token_ids, vocab=vocab, max_length=params['max_length'], padding_idx=params['padding_idx'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=hprams[\"batch_size\"], shuffle=True)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=hprams[\"batch_size\"], shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=hprams[\"batch_size\"], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH1ZBV4dQlm-",
        "outputId": "b4ae9d8d-115e-44af-ceb5-56ec25ee869c"
      },
      "source": [
        "cls = Classifier(embeddings, params['padding_idx'], hprams[\"hidden_size\"])\n",
        "cls.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (self_attention): SelfAttentionLayer(\n",
              "    (embeddings): Embedding(400002, 300, padding_idx=400001)\n",
              "  )\n",
              "  (lin1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (lin2): Linear(in_features=150, out_features=1, bias=True)\n",
              "  (output): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFBBrB_ZREp4"
      },
      "source": [
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(cls.parameters(), lr=hprams[\"learning_rate\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Ie61iKRgOG",
        "outputId": "186e191b-8b0a-4934-ab70-fadc877b49a4"
      },
      "source": [
        "_, train_losses_bow, valid_losses_bow = train(cls, dataloader_train, dataloader_valid, criterion,\n",
        "          optimizer, filename, n_epochs=n_epochs, run=run, params=hprams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época: 0/19 Train Loss: 0.011082 Valid Loss: 0.009220 Valid Acc: 0.790\n",
            "best model\n",
            "Época: 1/19 Train Loss: 0.008975 Valid Loss: 0.008554 Valid Acc: 0.810\n",
            "best model\n",
            "Época: 2/19 Train Loss: 0.008586 Valid Loss: 0.008187 Valid Acc: 0.818\n",
            "best model\n",
            "Época: 3/19 Train Loss: 0.008402 Valid Loss: 0.008229 Valid Acc: 0.811\n",
            "Época: 4/19 Train Loss: 0.008354 Valid Loss: 0.008257 Valid Acc: 0.813\n",
            "Época: 5/19 Train Loss: 0.008304 Valid Loss: 0.008048 Valid Acc: 0.823\n",
            "best model\n",
            "Época: 6/19 Train Loss: 0.008229 Valid Loss: 0.008162 Valid Acc: 0.815\n",
            "Época: 7/19 Train Loss: 0.008191 Valid Loss: 0.008480 Valid Acc: 0.804\n",
            "Época: 8/19 Train Loss: 0.008168 Valid Loss: 0.007996 Valid Acc: 0.826\n",
            "best model\n",
            "Época: 9/19 Train Loss: 0.008125 Valid Loss: 0.008005 Valid Acc: 0.824\n",
            "Época: 10/19 Train Loss: 0.008130 Valid Loss: 0.008352 Valid Acc: 0.807\n",
            "Época: 11/19 Train Loss: 0.008063 Valid Loss: 0.008019 Valid Acc: 0.819\n",
            "Época: 12/19 Train Loss: 0.008023 Valid Loss: 0.007981 Valid Acc: 0.824\n",
            "best model\n",
            "Época: 13/19 Train Loss: 0.008019 Valid Loss: 0.008019 Valid Acc: 0.818\n",
            "Época: 14/19 Train Loss: 0.007980 Valid Loss: 0.007966 Valid Acc: 0.822\n",
            "best model\n",
            "Época: 15/19 Train Loss: 0.007960 Valid Loss: 0.007939 Valid Acc: 0.825\n",
            "best model\n",
            "Época: 16/19 Train Loss: 0.007937 Valid Loss: 0.007930 Valid Acc: 0.822\n",
            "best model\n",
            "Época: 17/19 Train Loss: 0.007911 Valid Loss: 0.007946 Valid Acc: 0.823\n",
            "Época: 18/19 Train Loss: 0.007869 Valid Loss: 0.008377 Valid Acc: 0.807\n",
            "Época: 19/19 Train Loss: 0.007883 Valid Loss: 0.007920 Valid Acc: 0.825\n",
            "best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWgeAfa08ZeX",
        "outputId": "49a8142e-55f3-497e-f2fd-9fbaad880b9e"
      },
      "source": [
        "predict(cls, filename, dataloader_test, run=run)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "Acurácia de 81.440 %\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyroR53P7-Gw",
        "outputId": "101448b7-2880-4b08-bbe7-ea173ca719b9"
      },
      "source": [
        "run.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Waiting for the remaining 8 operations to synchronize with Neptune. Do not kill this process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 8 operations synced, thanks for waiting!\n"
          ]
        }
      ]
    }
  ]
}